# arxiv-daily latest papers around wearable device
Automated deployment @ 2023-06-09 16:02:27 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`]({repo_url}/blob/main/database/topic.yml).
> You can also view historical data through the [storage]({repo_url}/blob/main/database/storage).

## 3D Vision

### Visual Localization
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Asymmetric periodic boundary conditions for molecular dynamics and coarse-grained simulations of nucleic acids**|Radek Erban et.al.|[2306.05396v1](http://arxiv.org/abs/2306.05396v1)|null|Periodic boundary conditions are commonly applied in molecular dynamics
simulations in the microcanonical (NVE), canonical (NVT) and
isothermal-isobaric (NpT) ensembles. In their simplest application, a
biological system of interest is placed in the middle of a solvation box, which
is chosen 'sufficiently large' to minimize any numerical artefacts associated
with the periodic boundary conditions. This practical approach brings
limitations to the size of biological systems that can be simulated. Here, we
study simulations of effectively infinitely-long nucleic acids, which are
solvated in the directions perpendicular to the polymer chain, while periodic
boundary conditions are also applied along the polymer chain. We study the
effects of these asymmetric periodic boundary conditions (APBC) on the
simulated results, including the mechanical properties of biopolymers and the
properties of the surrounding solvent. To get some further insights into the
advantages of using the APBC, a coarse-grained worm-like chain model is first
studied, illustrating how the persistence length can be extracted from local
properties of the polymer chain, which are less affected by the APBC than some
global averages. This is followed by all-atom molecular dynamics simulations of
DNA in ionic solutions, where we use the APBC to investigate sequence-dependent
properties of DNA molecules and properties of the surrounding solvent.|
|**2023-06-08**|**Optimizing helical disc dynamo**|J. Priede et.al.|[2306.05379v1](http://arxiv.org/abs/2306.05379v1)|null|We present an optimized design of our recently realized helical disc dynamo.
Like the original set-up, the optimized dynamo consists of a flat multi-arm
spiral coil and a co-axially placed disc which is connected to the former by
sliding liquid metal contacts. In contrast to the original set-up, the disc and
the coil in the optimized design have different sizes. This allows the disc to
capture more of the high-density magnetic flux generated in the inner part of
the coil and to avoid the reverse flux in the outer part of the coil. By
optimizing the coil and dics radii, the critical magnetic Reynolds number can
be reduced from ${\mathit Rm}\approx34.6$ when the disc and coil have equal
inner and outer radii with the ratio $r_{i}/r_{o}\approx0.36$ to ${\mathit
Rm}\approx11.6.$ This lowest possible disc dynamo threshold is attained when
the disc and coil have relatively narrow widths. Using a slightly suboptimal
but more practical set-up with the inner and outer radii of the disc and coil
equal to to $(0.3,0.9)$ and $(0.74,1),$ respectively, self-excitation is
expected at ${\mathit Rm}\approx14.6.$|
|**2023-06-08**|**The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues**|Adaeze Adigwe et.al.|[2306.05360v1](http://arxiv.org/abs/2306.05360v1)|null|This paper presents the ADAIO team's system entry in the Building Educational
Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in
Educational Dialogues. The task aims to assess the performance of
state-of-the-art generative models as AI teachers in producing suitable
responses within a student-teacher dialogue. Our system comprises evaluating
various baseline models using OpenAI GPT-3 and designing diverse prompts to
prompt the OpenAI models for teacher response generation. After the challenge,
our system achieved second place by employing a few-shot prompt-based approach
with the OpenAI text-davinci-003 model. The results highlight the few-shot
learning capabilities of large-language models, particularly OpenAI's GPT-3, in
the role of AI teachers.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Safe Collaborative Filtering**|Riku Togashi et.al.|[2306.05292v1](http://arxiv.org/abs/2306.05292v1)|[link](https://github.com/riktor/safer2-recommender)|Excellent tail performance is crucial for modern machine learning tasks, such
as algorithmic fairness, class imbalance, and risk-sensitive decision making,
as it ensures the effective handling of challenging samples within a dataset.
Tail performance is also a vital determinant of success for personalised
recommender systems to reduce the risk of losing users with low satisfaction.
This study introduces a "safe" collaborative filtering method that prioritises
recommendation quality for less-satisfied users rather than focusing on the
average performance. Our approach minimises the conditional value at risk
(CVaR), which represents the average risk over the tails of users' loss. To
overcome computational challenges for web-scale recommender systems, we develop
a robust yet practical algorithm that extends the most scalable method,
implicit alternating least squares (iALS). Empirical evaluation on real-world
datasets demonstrates the excellent tail performance of our approach while
maintaining competitive computational efficiency.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit**|Jiongnan Liu et.al.|[2306.05212v1](http://arxiv.org/abs/2306.05212v1)|[link](https://github.com/ruc-gsai/yulan-ir)|Although Large Language Models (LLMs) have demonstrated extraordinary
capabilities in many domains, they still have a tendency to hallucinate and
generate fictitious responses to user requests. This problem can be alleviated
by augmenting LLMs with information retrieval (IR) systems (also known as
retrieval-augmented LLMs). Applying this strategy, LLMs can generate more
factual texts in response to user input according to the relevant content
retrieved by IR systems from external corpora as references. In addition, by
incorporating external knowledge, retrieval-augmented LLMs can answer in-domain
questions that cannot be answered by solely relying on the world knowledge
stored in parameters. To support research in this area and facilitate the
development of retrieval-augmented LLM systems, we develop RETA-LLM, a
{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline
to help researchers and users build their customized in-domain LLM-based
systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM
provides more plug-and-play modules to support better interaction between IR
systems and LLMs, including {request rewriting, document retrieval, passage
extraction, answer generation, and fact checking} modules. Our toolkit is
publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.|
|**2023-06-08**|**Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose**|Wiktor Mucha et.al.|[2306.05147v1](http://arxiv.org/abs/2306.05147v1)|null|Egocentric action recognition is essential for healthcare and assistive
technology that relies on egocentric cameras because it allows for the
automatic and continuous monitoring of activities of daily living (ADLs)
without requiring any conscious effort from the user. This study explores the
feasibility of using 2D hand and object pose information for egocentric action
recognition. While current literature focuses on 3D hand pose information, our
work shows that using 2D skeleton data is a promising approach for hand-based
action classification, might offer privacy enhancement, and could be less
computationally demanding. The study uses a state-of-the-art transformer-based
method to classify sequences and achieves validation results of 94%,
outperforming other existing solutions. The accuracy of the test subset drops
to 76%, indicating the need for further generalization improvement. This
research highlights the potential of 2D hand and object pose information for
action recognition tasks and offers a promising alternative to 3D-based
methods.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Constraints on the intergalactic magnetic field using Fermi-LAT and H.E.S.S. blazar observations**|H. E. S. S. et.al.|[2306.05132v1](http://arxiv.org/abs/2306.05132v1)|null|Magnetic fields in galaxies and galaxy clusters are believed to be the result
of the amplification of intergalactic seed fields during the formation of
large-scale structures in the universe. However, the origin, strength, and
morphology of this intergalactic magnetic field (IGMF) remain unknown. Lower
limits on (or indirect detection of) the IGMF can be obtained from observations
of high-energy gamma rays from distant blazars. Gamma rays interact with the
extragalactic background light to produce electron-positron pairs, which can
subsequently initiate electromagnetic cascades. The $\gamma$-ray signature of
the cascade depends on the IGMF since it deflects the pairs. Here we report on
a new search for this cascade emission using a combined data set from the Fermi
Large Area Telescope and the High Energy Stereoscopic System. Using
state-of-the-art Monte Carlo predictions for the cascade signal, our results
place a lower limit on the IGMF of $B > 7.1\times10^{-16}$ G for a coherence
length of 1 Mpc even when blazar duty cycles as short as 10 yr are assumed.
This improves on previous lower limits by a factor of 2. For longer duty cycles
of $10^4$ ($10^7$) yr, IGMF strengths below $1.8\times10^{-14}$ G
($3.9\times10^{-14}$ G) are excluded, which rules out specific models for IGMF
generation in the early universe.|
|**2023-06-08**|**Controllable Multi-Objective Re-ranking with Policy Hypernetworks**|Sirui Chen et.al.|[2306.05118v1](http://arxiv.org/abs/2306.05118v1)|[link](https://github.com/lyingcs/controllable-multi-objective-reranking)|Multi-stage ranking pipelines have become widely used strategies in modern
recommender systems, where the final stage aims to return a ranked list of
items that balances a number of requirements such as user preference,
diversity, novelty etc. Linear scalarization is arguably the most widely used
technique to merge multiple requirements into one optimization objective, by
summing up the requirements with certain preference weights. Existing
final-stage ranking methods often adopt a static model where the preference
weights are determined during offline training and kept unchanged during online
serving. Whenever a modification of the preference weights is needed, the model
has to be re-trained, which is time and resources inefficient. Meanwhile, the
most appropriate weights may vary greatly for different groups of targeting
users or at different time periods (e.g., during holiday promotions). In this
paper, we propose a framework called controllable multi-objective re-ranking
(CMR) which incorporates a hypernetwork to generate parameters for a re-ranking
model according to different preference weights. In this way, CMR is enabled to
adapt the preference weights according to the environment changes in an online
manner, without retraining the models. Moreover, we classify practical
business-oriented tasks into four main categories and seamlessly incorporate
them in a new proposed re-ranking model based on an Actor-Evaluator framework,
which serves as a reliable real-world testbed for CMR. Offline experiments
based on the dataset collected from Taobao App showed that CMR improved several
popular re-ranking models by using them as underlying models. Online A/B tests
also demonstrated the effectiveness and trustworthiness of CMR.|
|**2023-06-08**|**Neuromorphic Sampling of Signals in Shift-Invariant Spaces**|Abijith Jagannath Kamath et.al.|[2306.05103v1](http://arxiv.org/abs/2306.05103v1)|null|Neuromorphic sampling is a paradigm shift in analog-to-digital conversion
where the acquisition strategy is opportunistic and measurements are recorded
only when there is a significant change in the signal. Neuromorphic sampling
has given rise to a new class of event-based sensors called dynamic vision
sensors or neuromorphic cameras. The neuromorphic sampling mechanism utilizes
low power and provides high-dynamic range sensing with low latency and high
temporal resolution. The measurements are sparse and have low redundancy making
it convenient for downstream tasks. In this paper, we present a
sampling-theoretic perspective to neuromorphic sensing of continuous-time
signals. We establish a close connection between neuromorphic sampling and
time-based sampling - where signals are encoded temporally. We analyse
neuromorphic sampling of signals in shift-invariant spaces, in particular,
bandlimited signals and polynomial splines. We present an iterative technique
for perfect reconstruction subject to the events satisfying a density
criterion. We also provide necessary and sufficient conditions for perfect
reconstruction. Owing to practical limitations in meeting the sufficient
conditions for perfect reconstruction, we extend the analysis to approximate
reconstruction from sparse events. In the latter setting, we pose signal
reconstruction as a continuous-domain linear inverse problem whose solution can
be obtained by solving an equivalent finite-dimensional convex optimization
program using a variable-splitting approach. We demonstrate the performance of
the proposed algorithm and validate our claims via experiments on synthetic
signals.|
|**2023-06-08**|**On the Robustness of Topics API to a Re-Identification Attack**|Nikhil Jha et.al.|[2306.05094v1](http://arxiv.org/abs/2306.05094v1)|null|Web tracking through third-party cookies is considered a threat to users'
privacy and is supposed to be abandoned in the near future. Recently, Google
proposed the Topics API framework as a privacy-friendly alternative for
behavioural advertising. Using this approach, the browser builds a user profile
based on navigation history, which advertisers can access. The Topics API has
the possibility of becoming the new standard for behavioural advertising, thus
it is necessary to fully understand its operation and find possible
limitations.
  This paper evaluates the robustness of the Topics API to a re-identification
attack where an attacker reconstructs the user profile by accumulating user's
exposed topics over time to later re-identify the same user on a different
website. Using real traffic traces and realistic population models, we find
that the Topics API mitigates but cannot prevent re-identification to take
place, as there is a sizeable chance that a user's profile is unique within a
website's audience. Consequently, the probability of correct re-identification
can reach 15-17%, considering a pool of 1,000 users. We offer the code and data
we use in this work to stimulate further studies and the tuning of the Topic
API parameters.|
|**2023-06-08**|**Deriving interaction vertices in higher derivative theories**|Sudarshan Ananth et.al.|[2306.05074v1](http://arxiv.org/abs/2306.05074v1)|null|We derive cubic interaction vertices for a class of higher-derivative
theories involving three arbitrary integer spin fields. This derivation uses
the requirement of closure of the Poincar\`e algebra in four-dimensional flat
spacetime. We find two varieties of permitted structures at the cubic level and
eliminate one variety, which is proportional to the equations of motion, using
suitable field redefinitions. We then consider soft theorems for field theories
with higher-derivative interactions and construct amplitudes in these theories
using the inverse-soft approach.|
|**2023-06-08**|**Real-Time Rendering of Glinty Appearances using Distributed Binomial Laws on Anisotropic Grids**|Deliot et.al.|[2306.05051v1](http://arxiv.org/abs/2306.05051v1)|null|In this work, we render in real-time glittery materials caused by discrete
flakes on the surface. To achieve this, one has to count the number of flakes
reflecting the light towards the camera within every texel covered by a given
pixel footprint. To do so, we derive a counting method for arbitrary footprints
that, unlike previous work, outputs the correct statistics. We combine this
counting method with an anisotropic parameterization of the texture space that
reduces the number of texels falling under a pixel footprint. This allows our
method to run with both stable performance and 1.5X to 5X faster than the
state-of-the-art.|
|**2023-06-08**|**Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges**|Karan Jakhar et.al.|[2306.05014v1](http://arxiv.org/abs/2306.05014v1)|[link](https://github.com/jakharkaran/eqsdiscovery_2d-fhit_rbc)|There is growing interest in discovering interpretable, closed-form equations
for subgrid-scale (SGS) closures/parameterizations of complex processes in
Earth system. Here, we apply a common equation-discovery technique with
expansive libraries to learn closures from filtered direct numerical
simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC).
Across common filters, we robustly discover closures of the same form for
momentum and heat fluxes. These closures depend on nonlinear combinations of
gradients of filtered variables (velocity, temperature), with constants that
are independent of the fluid/flow properties and only depend on filter
type/size. We show that these closures are the nonlinear gradient model (NGM),
which is derivable analytically using Taylor-series expansions. In fact, we
suggest that with common (physics-free) equation-discovery algorithms,
regardless of the system/physics, discovered closures are always consistent
with the Taylor-series. Like previous studies, we find that large-eddy
simulations with NGM closures are unstable, despite significant similarities
between the true and NGM-predicted fluxes (pattern correlations $> 0.95$). We
identify two shortcomings as reasons for these instabilities: in 2D, NGM
produces zero kinetic energy transfer between resolved and subgrid scales,
lacking both diffusion and backscattering. In RBC, backscattering of potential
energy is poorly predicted. Moreover, we show that SGS fluxes diagnosed from
data, presumed the "truth" for discovery, depend on filtering procedures and
are not unique. Accordingly, to learn accurate, stable closures from
high-fidelity data in future work, we propose several ideas around using
physics-informed libraries, loss functions, and metrics. These findings are
relevant beyond turbulence to closure modeling of any multi-scale system.|
|**2023-06-08**|**Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce**|Juan Gong et.al.|[2306.05011v1](http://arxiv.org/abs/2306.05011v1)|null|Ranking model plays an essential role in e-commerce search and
recommendation. An effective ranking model should give a personalized ranking
list for each user according to the user preference. Existing algorithms
usually extract a user representation vector from the user behavior sequence,
then feed the vector into a feed-forward network (FFN) together with other
features for feature interactions, and finally produce a personalized ranking
score. Despite tremendous progress in the past, there is still room for
improvement. Firstly, the personalized patterns of feature interactions for
different users are not explicitly modeled. Secondly, most of existing
algorithms have poor personalized ranking results for long-tail users with few
historical behaviors due to the data sparsity. To overcome the two challenges,
we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive
learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework
to capture personalized feature interactions for different users. To model the
user preference, the user behavior sequence is simultaneously fed into expert
networks and the gate network. Within the gate network, one gate unit and one
activation unit are designed to adaptively learn the fine-grained activation
vector for experts using an attention mechanism. Secondly, a random masking
strategy is applied to the user behavior sequence to simulate long-tail users,
and an auxiliary contrastive loss is imposed to the output of the gate network
to improve the model generalization for these users. This is validated by a
higher performance gain on the long-tail user test set. Experiment results on a
JD real production dataset and a public dataset demonstrate the effectiveness
of AW-MoE, which significantly outperforms state-of-art methods. Notably,
AW-MoE has been successfully deployed in the JD e-commerce search engine, ...|
|**2023-06-08**|**A Model for Confined Solar Eruptions Including External Reconnection**|Jun Chen et.al.|[2306.04993v1](http://arxiv.org/abs/2306.04993v1)|null|The violent disruption of the coronal magnetic field is often observed to be
restricted to the low corona, appearing as a confined eruption. The possible
causes of the confinement remain elusive. Here, we model the eruption of a
magnetic flux rope in a quadrupolar active region, with the parameters set such
that magnetic X-lines exist both below and above the rope. This facilitates the
onset of magnetic reconnection in either place but with partly opposing effects
on the eruption. The lower reconnection initially adds poloidal flux to the
rope, increasing the upward hoop force and supporting the rise of the rope.
However, when the flux of the magnetic side lobes enters the lower
reconnection, the flux rope is found to separate from the reconnection site and
the flux accumulation ceases. At the same time, the upper reconnection begins
to reduce the poloidal flux of the rope, decreasing its hoop force; eventually
this cuts the rope completely. The relative weight of the two reconnection
processes is varied in the model, and it is found that their combined effect
and the tension force of the overlying field confine the eruption if the flux
ratio of the outer to the inner polarities exceeds a threshold, which is about
1.3 for our Cartesian box and chosen parameters. We hence propose that external
reconnection between an erupting flux rope and overlying flux can play a vital
role in confining eruptions.|
|**2023-06-08**|**StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views**|Jianfei Guo et.al.|[2306.04988v1](http://arxiv.org/abs/2306.04988v1)|null|We present a novel multi-view implicit surface reconstruction technique,
termed StreetSurf, that is readily applicable to street view images in
widely-used autonomous driving datasets, such as Waymo-perception sequences,
without necessarily requiring LiDAR data. As neural rendering research expands
rapidly, its integration into street views has started to draw interests.
Existing approaches on street views either mainly focus on novel view synthesis
with little exploration of the scene geometry, or rely heavily on dense LiDAR
data when investigating reconstruction. Neither of them investigates multi-view
implicit surface reconstruction, especially under settings without LiDAR data.
Our method extends prior object-centric neural surface reconstruction
techniques to address the unique challenges posed by the unbounded street views
that are captured with non-object-centric, long and narrow camera trajectories.
We delimit the unbounded space into three parts, close-range, distant-view and
sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids
along with road-surface initialization scheme for finer and disentangled
representation. To further address the geometric errors arising from
textureless regions and insufficient viewing angles, we adopt geometric priors
that are estimated using general purpose monocular models. Coupled with our
implementation of efficient and fine-grained multi-stage ray marching strategy,
we achieve state of the art reconstruction quality in both geometry and
appearance within only one to two hours of training time with a single RTX3090
GPU for each street view sequence. Furthermore, we demonstrate that the
reconstructed implicit surfaces have rich potential for various downstream
tasks, including ray tracing and LiDAR simulation.|
|**2023-06-08**|**Ultraviolet Photodetectors based on GaN and AlGaN/AlN Nanowire Ensembles: Effects of Planarization with Hydrogen Silsesquioxane and Nanowire Architecture**|E. Akar et.al.|[2306.04986v1](http://arxiv.org/abs/2306.04986v1)|null|The interest in nanowire photodetectors stems from their potential to improve
the performance of a variety of devices, including solar cells, cameras,
sensors, and communication systems. Implementing devices based on nanowire
ensembles requires a planarization process which must be conceived to preserve
the advantages of the nanowire geometry. This is particularly challenging in
the ultraviolet (UV) range, where spin coating with hydrogen silsesquioxane
(HSQ) appears as an interesting approach in terms of transmittance and
refractive index. Here, we report a comprehensive study on UV photodetectors
based on GaN or AlGaN/AlN nanowire ensembles encapsulated in HSQ. We show that
this material is efficient for passivating the nanowire surface, it introduces
a compressive strain in the nanowires and preserves their radiative efficiency.
We discuss the final performance of planarized UV photodetectors based on three
kinds of nanowire ensembles: (i) non-intentionally-doped (nid) GaN nanowires,
(ii) Ge-doped GaN nanowires, and (iii) nid GaN nanowires terminated with an
AlGaN/AlN superlattice. The incorporation of the superlattice allows tuning the
spectral response with bias, which can enhance the carrier collection from the
AlGaN/AlN superlattice or from the GaN stem. In all the cases, the performance
of the planarized devices remains determined by the nanowire nature, since
their characteristics in terms of linearity and spectral selectivity are closer
to those demonstrated in single nanowires than those of planar devices. Thus,
the visible rejection is several orders of magnitude and there is no indication
of persistent photocurrent, which makes all the samples suitable for
UV-selective photodetection applications.|
|**2023-06-08**|**Motion Planning for Aerial Pick-and-Place based on Geometric Feasibility Constraints**|Huazi Cao et.al.|[2306.04970v1](http://arxiv.org/abs/2306.04970v1)|null|This paper studies the motion planning problem of the pick-and-place of an
aerial manipulator that consists of a quadcopter flying base and a Delta arm.
We propose a novel partially decoupled motion planning framework to solve this
problem. Compared to the state-of-the-art approaches, the proposed one has two
novel features. First, it does not suffer from increased computation in
high-dimensional configuration spaces. That is because it calculates the
trajectories of the quadcopter base and the end-effector separately in the
Cartesian space based on proposed geometric feasibility constraints. The
geometric feasibility constraints can ensure the resulting trajectories satisfy
the aerial manipulator's geometry. Second, collision avoidance for the Delta
arm is achieved through an iterative approach based on a pinhole mapping
method, so that the feasible trajectory can be found in an efficient manner.
The proposed approach is verified by three experiments on a real aerial
manipulation platform. The experimental results show the effectiveness of the
proposed method for the aerial pick-and-place task.|
|**2023-06-08**|**Knowledge Detection by Relevant Question and Image Attributes in Visual Question Answering**|Param Ahir et.al.|[2306.04938v1](http://arxiv.org/abs/2306.04938v1)|null|Visual question answering (VQA) is a Multidisciplinary research problem that
pursued through practices of natural language processing and computer vision.
Visual question answering automatically answers natural language questions
according to the content of an image. Some testing questions require external
knowledge to derive a solution. Such knowledge-based VQA uses various methods
to retrieve features of image and text, and combine them to generate the
answer. To generate knowledgebased answers either question dependent or image
dependent knowledge retrieval methods are used. If knowledge about all the
objects in the image is derived, then not all knowledge is relevant to the
question. On other side only question related knowledge may lead to incorrect
answers and over trained model that answers question that is irrelevant to
image. Our proposed method takes image attributes and question features as
input for knowledge derivation module and retrieves only question relevant
knowledge about image objects which can provide accurate answers.|
|**2023-06-08**|**Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization**|Jungwuk Park et.al.|[2306.04911v1](http://arxiv.org/abs/2306.04911v1)|null|In domain generalization (DG), the target domain is unknown when the model is
being trained, and the trained model should successfully work on an arbitrary
(and possibly unseen) target domain during inference. This is a difficult
problem, and despite active studies in recent years, it remains a great
challenge. In this paper, we take a simple yet effective approach to tackle
this issue. We propose test-time style shifting, which shifts the style of the
test sample (that has a large style gap with the source domains) to the nearest
source domain that the model is already familiar with, before making the
prediction. This strategy enables the model to handle any target domains with
arbitrary style statistics, without additional model update at test-time.
Additionally, we propose style balancing, which provides a great platform for
maximizing the advantage of test-time style shifting by handling the
DG-specific imbalance issues. The proposed ideas are easy to implement and
successfully work in conjunction with various other DG schemes. Experimental
results on different datasets show the effectiveness of our methods.|
|**2023-06-08**|**ViG-UNet: Vision Graph Neural Networks for Medical Image Segmentation**|Juntao Jiang et.al.|[2306.04905v1](http://arxiv.org/abs/2306.04905v1)|null|Deep neural networks have been widely used in medical image analysis and
medical image segmentation is one of the most important tasks. U-shaped neural
networks with encoder-decoder are prevailing and have succeeded greatly in
various segmentation tasks. While CNNs treat an image as a grid of pixels in
Euclidean space and Transformers recognize an image as a sequence of patches,
graph-based representation is more generalized and can construct connections
for each part of an image. In this paper, we propose a novel ViG-UNet, a graph
neural network-based U-shaped architecture with the encoder, the decoder, the
bottleneck, and skip connections. The downsampling and upsampling modules are
also carefully designed. The experimental results on ISIC 2016, ISIC 2017 and
Kvasir-SEG datasets demonstrate that our proposed architecture outperforms most
existing classic and state-of-the-art U-shaped networks.|

### Point Cloud Matching
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Scalar curvature rigidity of degenerate warped product spaces**|Jinmin Wang et.al.|[2306.05413v1](http://arxiv.org/abs/2306.05413v1)|null|In this paper we prove the scalar curvature extremality and rigidity for a
class of warped product spaces that are possibly degenerate at the two ends.
The leaves of these warped product spaces can be any closed Riemannian manifold
with nonnegative curvature operator and nonvanishing Euler characteristic, flat
tori, round spheres and their direct products. In particular, we obtain the
scalar curvature extremality and rigidity for certain degenerate toric bands
and also for round spheres with two antipodal points removed. This answers the
corresponding questions of Gromov in all dimensions.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Picard and Brauer groups of $K(n)$-local spectra via profinite Galois descent**|Itamar Mor et.al.|[2306.05393v1](http://arxiv.org/abs/2306.05393v1)|null|Using the pro\'etale site, we construct models for the continuous actions of
the Morava stabiliser group on Morava E-theory, its $\infty$-category of
$K(n)$-local modules, and its Picard spectrum. For the two sheaves of spectra,
we evaluate the resulting descent spectral sequences: these can be thought of
as homotopy fixed point spectral sequences for the profinite Galois extension
$L_{K(n)} \mathbb S \to E_n$. We show that the descent spectral sequence for
the Morava E-theory sheaf is the $K(n)$-local $E_n$-Adams spectral sequence.
The spectral sequence for the sheaf of Picard spectra is closely related to one
recently defined by Heard; our formalism allows us to compare many
differentials with those in the $K(n)$-local $E_n$-Adams spectral sequence, and
isolate the exotic Picard elements in the $0$-stem. In particular, we show how
this recovers the computation due to Hopkins, Mahowald and Sadofsky of the
group $\mathrm{Pic}_1$ at all primes. We also use these methods to bound the
Brauer group of $K(n)$-local spectra, and compute this bound at height one.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**The correlations between galaxy properties in different environments of the cosmic web**|Anindita Nandi et.al.|[2306.05354v1](http://arxiv.org/abs/2306.05354v1)|null|We study the correlations between galaxy properties in different environments
of the cosmic web using a volume limited sample from the SDSS. We determine the
geometric environment at the location of each galaxy using the eigenvalues of
the tidal tensor. The correlations are then separately analyzed in different
cosmic web environments. We use the Pearson correlation coefficient and the
normalized mutual information for measuring the correlations. Using a
two-tailed t-test, we find that the correlations between the galaxy properties
are sensitive to the geometric environments. The stellar mass can be an
important link between the galaxy properties and the environment. We repeat the
analysis after matching the stellar mass distributions in different
environments and find that the conclusions remain unchanged for most of the
relations. Our study suggests that the galaxy properties and their
interrelationships are susceptible to the geometric environments of the cosmic
web.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**FCNC charmed-hadron decays with invisible singlet particles in light of recent data**|Geng Li et.al.|[2306.05333v1](http://arxiv.org/abs/2306.05333v1)|null|The flavor-changing neutral current (FCNC) decays of charmed hadrons with
missing energy ($\slashed E$) can serve as potentially promising hunting
grounds for hints of new physics, as the standard-model backgrounds are very
suppressed. A few of such processes have been searched for in recent
experiments, particularly $D^0\to\slashed E$ by Belle and $D^0\to\pi^0\slashed
E$ and $\Lambda_c^+\to p\slashed E$ by BESIII, resulting in upper bounds on
their branching fractions. We consider them to illuminate the possible
contributions of the quark transition $c\to u\slashed E$ with a couple of
invisible spinless bosons carrying away the missing energy, assuming that they
are not charge conjugates of each other and hence can have unequal masses. We
find that these data are complementary in that they constrain different sets of
the underlying operators and do not cover the same ranges of the bosons'
masses, but there are regions not yet accessible. From the allowed parameter
space, we show that other $D$-meson decays, such as $D\to\rho\slashed E$, and
the charmed-baryon ones $\Xi_c\to(\Sigma,\Lambda)\slashed E$ can have sizable
branching fractions and therefore may offer further probes of the new-physics
interactions. We point out the importance of $D^0\to\gamma\slashed E$ which are
not yet searched for but could access parts of the parameter space beyond the
reach of the other modes. In addition, we look at a scenario where the
invisibles are instead fermionic, namely sterile neutrinos, and a scalar
leptoquark mediates $c\to u\slashed E$. We discuss the implications of the
aforesaid bounds for this model. The predictions we make for the various
charmed-hadron decays in the different scenarios may be testable in the near
future by BESIII and Belle II.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of
the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our
solver relies on a relaxation method in which convergence is sought as the
steady-state solution of a parabolic equation, whose time-discretization is
governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings
indicate that the RKL-based Poisson solver, which is both fully parallel and
rapidly convergent, has the potential to serve as a practical alternative to
conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and
\textit{successive over-relaxation} (SOR) methods. Additionally, it can
mitigate some of the drawbacks of these traditional techniques. We incorporate
our algorithm into a multigrid solver to provide a simple and efficient gravity
solver that can be used to obtain the gravitational potentials in
self-gravitational hydrodynamics. We test our implementation against a broad
range of standard self-gravitating astrophysical problems designed to examine
different aspects of the code. We demonstrate that the results match
excellently with the analytical predictions (when available), and the findings
of similar previous studies.|
|**2023-06-08**|**Federated Learning under Covariate Shifts with Generalization Guarantees**|Ali Ramezani-Kebrya et.al.|[2306.05325v1](http://arxiv.org/abs/2306.05325v1)|null|This paper addresses intra-client and inter-client covariate shifts in
federated learning (FL) with a focus on the overall generalization performance.
To handle covariate shifts, we formulate a new global model training paradigm
and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)
along with improving density ratio matching methods without requiring perfect
knowledge of the supremum over true ratios. We also propose the
communication-efficient variant FITW-ERM with the same level of privacy
guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM
achieves smaller generalization error than classical ERM under certain
settings. Experimental results demonstrate the superiority of FTW-ERM over
existing FL baselines in challenging imbalanced federated settings in terms of
data distribution shifts across clients.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**A physically motivated analytical expression for the temperature dependence of the zero-field splitting of the nitrogen-vacancy center in diamond**|M. C. Cambria et.al.|[2306.05318v1](http://arxiv.org/abs/2306.05318v1)|null|The temperature dependence of the zero-field splitting (ZFS) between the
$|m_{s}=0\rangle$ and $|m_{s}=\pm 1\rangle$ levels of the nitrogen-vacancy (NV)
center's electronic ground-state spin triplet can be used as a robust nanoscale
thermometer in a broad range of environments. However, despite numerous
measurements of this dependence in different temperature ranges, to our
knowledge no analytical expression has been put forward that captures the
scaling of the ZFS of the NV center across all relevant temperatures. Here we
present a simple, analytical, and physically motivated expression for the
temperature dependence of the NV center's ZFS that matches all experimental
observations, in which the ZFS shifts in proportion to the occupation numbers
of two representative phonon modes. In contrast to prior models our expression
does not diverge outside the regions of fitting. We show that our model
quantitatively matches experimental measurements of the ZFS from 15 to 500 K in
single NV centers in ultra-pure bulk diamond, and we compare our model and
measurements to prior models and experimental data.|
|**2023-06-08**|**Hybridizable discontinuous Galerkin methods for the Monge-Ampere equation**|Ngoc Cuong Nguyen et.al.|[2306.05296v1](http://arxiv.org/abs/2306.05296v1)|null|We introduce two hybridizable discontinuous Galerkin (HDG) methods for
numerically solving the Monge-Ampere equation. The first HDG method is devised
to solve the nonlinear elliptic Monge-Ampere equation by using Newton's method.
The second HDG method is devised to solve a sequence of the Poisson equation
until convergence to a fixed-point solution of the Monge-Ampere equation is
reached. Numerical examples are presented to demonstrate the convergence and
accuracy of the HDG methods. Furthermore, the HDG methods are applied to
r-adaptive mesh generation by redistributing a given scalar density function
via the optimal transport theory. This r-adaptivity methodology leads to the
Monge-Ampere equation with a nonlinear Neumann boundary condition arising from
the optimal transport of the density function to conform the resulting
high-order mesh to the boundary. Hence, we extend the HDG methods to treat the
nonlinear Neumann boundary condition. Numerical experiments are presented to
illustrate the generation of r-adaptive high-order meshes on planar and curved
domains.|
|**2023-06-08**|**Positive Geometries for Scattering Amplitudes in Momentum Space**|Robert Moerman et.al.|[2306.05287v1](http://arxiv.org/abs/2306.05287v1)|null|Positive geometries provide a purely geometric point of departure for
studying scattering amplitudes in quantum field theory. A positive geometry is
a specific semi-algebraic set equipped with a unique rational top form - the
canonical form. There are known examples where the canonical form of some
positive geometry, defined in some kinematic space, encodes a scattering
amplitude in some theory. Remarkably, the boundaries of the positive geometry
are in bijection with the physical singularities of the scattering amplitude.
The Amplituhedron, discovered by Arkani-Hamed and Trnka, is a prototypical
positive geometry. It lives in momentum twistor space and describes tree-level
(and the integrands of planar loop-level) scattering amplitudes in maximally
supersymmetric Yang-Mills theory.
  In this dissertation, we study three positive geometries defined in on-shell
momentum space: the Arkani-Hamed-Bai-He-Yan (ABHY) associahedron, the Momentum
Amplituhedron, and the orthogonal Momentum Amplituhedron. Each describes
tree-level scattering amplitudes for different theories in different spacetime
dimensions. The three positive geometries share a series of interrelations in
terms of their boundary posets and canonical forms. We review these
relationships in detail, highlighting the author's contributions. We study
their boundary posets, classifying all boundaries and hence all physical
singularities at the tree level. We develop new combinatorial results to derive
rank-generating functions which enumerate boundaries according to their
dimension. These generating functions allow us to prove that the Euler
characteristics of the three positive geometries are one. In addition, we
discuss methods for manipulating canonical forms using ideas from computational
algebraic geometry.|
|**2023-06-08**|**Federated Linear Contextual Bandits with User-level Differential Privacy**|Ruiquan Huang et.al.|[2306.05275v1](http://arxiv.org/abs/2306.05275v1)|null|This paper studies federated linear contextual bandits under the notion of
user-level differential privacy (DP). We first introduce a unified federated
bandits framework that can accommodate various definitions of DP in the
sequential decision-making setting. We then formally introduce user-level
central DP (CDP) and local DP (LDP) in the federated bandits framework, and
investigate the fundamental trade-offs between the learning regrets and the
corresponding DP guarantees in a federated linear contextual bandits model. For
CDP, we propose a federated algorithm termed as \robin and show that it is
near-optimal in terms of the number of clients $M$ and the privacy budget
$\varepsilon$ by deriving nearly-matching upper and lower regret bounds when
user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating
that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret
blow-up factor at least {$\min\{1/\varepsilon,M\}$ or
$\min\{1/\sqrt{\varepsilon},\sqrt{M}\}$} under different conditions.|
|**2023-06-08**|**How to Incorporate Systematic Effects into Parameter Determination**|David van Dyk et.al.|[2306.05271v1](http://arxiv.org/abs/2306.05271v1)|null|We describe two different approaches for incorporating systematics into
analyses for parameter determination in the physical sciences. We refer to
these as the Pragmatic and the Full methods, with the latter coming in two
variants: Full Likelihood and Fully Bayesian. By the use of a simple and
readily understood example, we point out the advantage of using the Full
Likelihood and Fully Bayesian approaches; a more realistic example from
Astrophysics is also presented. This could be relevant for data analyses in a
wide range of scientific fields, for situations where systematic effects need
to be incorporated in the analysis procedure. This note is an extension of part
of the talk by van Dyk at the PHYSTAT-Systematics meeting.|
|**2023-06-08**|**Linking Frequentist and Bayesian Change-Point Methods**|David Ardia et.al.|[2306.05265v1](http://arxiv.org/abs/2306.05265v1)|null|We show that the two-stage minimum description length (MDL) criterion widely
used to estimate linear change-point (CP) models corresponds to the marginal
likelihood of a Bayesian model with a specific class of prior distributions.
This allows results from the frequentist and Bayesian paradigms to be bridged
together. Thanks to this link, one can rely on the consistency of the number
and locations of the estimated CPs and the computational efficiency of
frequentist methods, and obtain a probability of observing a CP at a given
time, compute model posterior probabilities, and select or combine CP methods
via Bayesian posteriors. Furthermore, we adapt several CP methods to take
advantage of the MDL probabilistic representation. Based on simulated data, we
show that the adapted CP methods can improve structural break detection
compared to state-of-the-art approaches. Finally, we empirically illustrate the
usefulness of combining CP detection methods when dealing with long time series
and forecasting.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Quantum computing algorithms for inverse problems on graphs and an NP-complete inverse problem**|Joonas Ilmavirta et.al.|[2306.05253v1](http://arxiv.org/abs/2306.05253v1)|null|We consider an inverse problem for a finite graph $(X,E)$ where we are given
a subset of vertices $B\subset X$ and the distances $d_{(X,E)}(b_1,b_2)$ of all
vertices $b_1,b_2\in B$. The distance of points $x_1,x_2\in X$ is defined as
the minimal number of edges needed to connect two vertices, so all edges have
length 1. The inverse problem is a discrete version of the boundary rigidity
problem in Riemannian geometry or the inverse travel time problem in
geophysics. We will show that this problem has unique solution under certain
conditions and develop quantum computing methods to solve it. We prove the
following uniqueness result: when $(X,E)$ is a tree and $B$ is the set of
leaves of the tree, the graph $(X,E)$ can be uniquely determined in the class
of all graphs having a fixed number of vertices. We present a quantum computing
algorithm which produces a graph $(X,E)$, or one of those, which has a given
number of vertices and the required distances between vertices in $B$. To this
end we develop an algorithm that takes in a qubit representation of a graph and
combine it with Grover's search algorithm. The algorithm can be implemented
using only $O(|X|^2)$ qubits, the same order as the number of elements in the
adjacency matrix of $(X,E)$. It also has a quadratic improvement in
computational cost compared to standard classical algorithms. Finally, we
consider applications in theory of computation, and show that a slight
modification of the above inverse problem is NP-complete: all NP-problems can
be reduced to a discrete inverse problem we consider.|
|**2023-06-08**|**Matching Latent Encoding for Audio-Text based Keyword Spotting**|Kumari Nishu et.al.|[2306.05245v1](http://arxiv.org/abs/2306.05245v1)|null|Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown
high-quality results, but the key challenge of how to semantically align two
embeddings for multi-word keywords of different sequence lengths remains
largely unsolved. In this paper, we propose an audio-text-based end-to-end
model architecture for flexible keyword spotting (KWS), which builds upon
learned audio and text embeddings. Our architecture uses a novel dynamic
programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally
partition the audio sequence into the same length as the word-based text
sequence using the monotonic alignment of spoken content. Our proposed model
consists of an encoder block to get audio and text embeddings, a projector
block to project individual embeddings to a common latent space, and an
audio-text aligner containing a novel DSP algorithm, which aligns the audio and
text embeddings to determine if the spoken content is the same as the text.
Experimental results show that our DSP is more effective than other
partitioning schemes, and the proposed architecture outperformed the
state-of-the-art results on the public dataset in terms of Area Under the ROC
Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth**|Zelin Liu et.al.|[2306.05238v1](http://arxiv.org/abs/2306.05238v1)|null|Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Investigating the OH-H2 relation in diffuse Galactic clouds**|Katherine Rawlins et.al.|[2306.05213v1](http://arxiv.org/abs/2306.05213v1)|null|We investigate the correlation between OH and H2 column densities in diffuse
Galactic clouds, in order to identify potential molecular tracers of
interstellar H2. For this, we analyse near-UV spectra extracted from the
ESO/VLT archives towards seventeen sightlines (five of them new) with known
N(H2), along with nine sightlines with no H2 information. N(OH) shows only
marginal correlation with N(H2) (10$^{20}$ to 2 x 10$^{21}$ cm$^{-2}$), at the
95 per cent confidence level. We use orthogonal distance regression analysis to
obtain N(OH)/N(H2) = (1.32+/-0.15) x 10$^{-7}$, which is ~ 33 per cent higher
than the previous estimates based on near-UV data. We also obtain N(CH)/N(H2) =
(3.83+/-0.23) x 10$^{-8}$ and a significant correlation between N(OH) and
N(CH), with N(OH) = (2.61+/-0.19) x N(CH), both of which are consistent with
previous results. Comparison with predictions of numerical models indicate that
OH absorption arises from diffuse gas (nH ~ 50 cm$^{-3}$) illuminated by
radiation fields ~ 0.5-5 G0, while CH is associated with higher density of 500
cm$^{-3}$. We posit that the apparent dichotomy in the properties of the
diffuse clouds giving rise to OH and CH absorption could be due to either (a)
the presence of multiple spectroscopically unresolved clouds along the
line-of-sight, or, (b) density gradients along the line-of-sight within a
single cloud.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**EUV brightenings in the quiet-Sun: Signatures in spectral and imaging data from the Interface Region Imaging Spectrograph**|C. J. Nelson et.al.|[2306.05190v1](http://arxiv.org/abs/2306.05190v1)|null|Localised transient EUV brightenings, sometimes named `campfires', occur
throughout the quiet-Sun. However, there are still many open questions about
such events, in particular regarding their temperature range and dynamics. In
this article, we aim to determine whether any transition region response can be
detected for small-scale EUV brightenings and, if so, to identify whether the
measured spectra correspond to any previously reported bursts in the transition
region, such as Explosive Events (EEs). EUV brightenings were detected in a
~29.4 minute dataset sampled by Solar Orbiter's Extreme Ultraviolet Imager on 8
March 2022 using an automated detection algorithm. Any potential transition
region response was inferred through analysis of imaging and spectral data
sampled through coordinated observations conducted by the Interface Region
Imaging Spectrograph (IRIS). EUV brightenings display a range of responses in
IRIS slit-jaw imager (SJI) data. Some events have clear signatures in the Mg II
and Si IV SJI filters, whilst others have no discernible counterpart. Both
extended and more complex EUV brightenings are found to, sometimes, have
responses in IRIS SJI data. Examples of EUI intensities peaking before, during,
and after their IRIS counterparts were found in lightcurves constructed
co-spatial to EUV brightenings. Importantly, therefore, it is likely that not
all EUV brightenings are driven in the same way, with some seemingly being
magnetic reconnection driven and others not. A single EUV brightening occurred
co-spatial to the IRIS slit, with its spectra matching the properties of EEs.
EUV brightenings is a term used to describe a range of small-scale event in the
solar corona. The physics responsible for all EUV brightenings is likely not
the same and, therefore, more research is required to assess their importance
towards global questions in the field, such as coronal heating.|
|**2023-06-08**|**On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs**|Constantin Christof et.al.|[2306.05185v1](http://arxiv.org/abs/2306.05185v1)|null|We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|

### Point Cloud Registration
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Scalar curvature rigidity of degenerate warped product spaces**|Jinmin Wang et.al.|[2306.05413v1](http://arxiv.org/abs/2306.05413v1)|null|In this paper we prove the scalar curvature extremality and rigidity for a
class of warped product spaces that are possibly degenerate at the two ends.
The leaves of these warped product spaces can be any closed Riemannian manifold
with nonnegative curvature operator and nonvanishing Euler characteristic, flat
tori, round spheres and their direct products. In particular, we obtain the
scalar curvature extremality and rigidity for certain degenerate toric bands
and also for round spheres with two antipodal points removed. This answers the
corresponding questions of Gromov in all dimensions.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Picard and Brauer groups of $K(n)$-local spectra via profinite Galois descent**|Itamar Mor et.al.|[2306.05393v1](http://arxiv.org/abs/2306.05393v1)|null|Using the pro\'etale site, we construct models for the continuous actions of
the Morava stabiliser group on Morava E-theory, its $\infty$-category of
$K(n)$-local modules, and its Picard spectrum. For the two sheaves of spectra,
we evaluate the resulting descent spectral sequences: these can be thought of
as homotopy fixed point spectral sequences for the profinite Galois extension
$L_{K(n)} \mathbb S \to E_n$. We show that the descent spectral sequence for
the Morava E-theory sheaf is the $K(n)$-local $E_n$-Adams spectral sequence.
The spectral sequence for the sheaf of Picard spectra is closely related to one
recently defined by Heard; our formalism allows us to compare many
differentials with those in the $K(n)$-local $E_n$-Adams spectral sequence, and
isolate the exotic Picard elements in the $0$-stem. In particular, we show how
this recovers the computation due to Hopkins, Mahowald and Sadofsky of the
group $\mathrm{Pic}_1$ at all primes. We also use these methods to bound the
Brauer group of $K(n)$-local spectra, and compute this bound at height one.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**FCNC charmed-hadron decays with invisible singlet particles in light of recent data**|Geng Li et.al.|[2306.05333v1](http://arxiv.org/abs/2306.05333v1)|null|The flavor-changing neutral current (FCNC) decays of charmed hadrons with
missing energy ($\slashed E$) can serve as potentially promising hunting
grounds for hints of new physics, as the standard-model backgrounds are very
suppressed. A few of such processes have been searched for in recent
experiments, particularly $D^0\to\slashed E$ by Belle and $D^0\to\pi^0\slashed
E$ and $\Lambda_c^+\to p\slashed E$ by BESIII, resulting in upper bounds on
their branching fractions. We consider them to illuminate the possible
contributions of the quark transition $c\to u\slashed E$ with a couple of
invisible spinless bosons carrying away the missing energy, assuming that they
are not charge conjugates of each other and hence can have unequal masses. We
find that these data are complementary in that they constrain different sets of
the underlying operators and do not cover the same ranges of the bosons'
masses, but there are regions not yet accessible. From the allowed parameter
space, we show that other $D$-meson decays, such as $D\to\rho\slashed E$, and
the charmed-baryon ones $\Xi_c\to(\Sigma,\Lambda)\slashed E$ can have sizable
branching fractions and therefore may offer further probes of the new-physics
interactions. We point out the importance of $D^0\to\gamma\slashed E$ which are
not yet searched for but could access parts of the parameter space beyond the
reach of the other modes. In addition, we look at a scenario where the
invisibles are instead fermionic, namely sterile neutrinos, and a scalar
leptoquark mediates $c\to u\slashed E$. We discuss the implications of the
aforesaid bounds for this model. The predictions we make for the various
charmed-hadron decays in the different scenarios may be testable in the near
future by BESIII and Belle II.|
|**2023-06-08**|**Hybridizable discontinuous Galerkin methods for the Monge-Ampere equation**|Ngoc Cuong Nguyen et.al.|[2306.05296v1](http://arxiv.org/abs/2306.05296v1)|null|We introduce two hybridizable discontinuous Galerkin (HDG) methods for
numerically solving the Monge-Ampere equation. The first HDG method is devised
to solve the nonlinear elliptic Monge-Ampere equation by using Newton's method.
The second HDG method is devised to solve a sequence of the Poisson equation
until convergence to a fixed-point solution of the Monge-Ampere equation is
reached. Numerical examples are presented to demonstrate the convergence and
accuracy of the HDG methods. Furthermore, the HDG methods are applied to
r-adaptive mesh generation by redistributing a given scalar density function
via the optimal transport theory. This r-adaptivity methodology leads to the
Monge-Ampere equation with a nonlinear Neumann boundary condition arising from
the optimal transport of the density function to conform the resulting
high-order mesh to the boundary. Hence, we extend the HDG methods to treat the
nonlinear Neumann boundary condition. Numerical experiments are presented to
illustrate the generation of r-adaptive high-order meshes on planar and curved
domains.|
|**2023-06-08**|**Positive Geometries for Scattering Amplitudes in Momentum Space**|Robert Moerman et.al.|[2306.05287v1](http://arxiv.org/abs/2306.05287v1)|null|Positive geometries provide a purely geometric point of departure for
studying scattering amplitudes in quantum field theory. A positive geometry is
a specific semi-algebraic set equipped with a unique rational top form - the
canonical form. There are known examples where the canonical form of some
positive geometry, defined in some kinematic space, encodes a scattering
amplitude in some theory. Remarkably, the boundaries of the positive geometry
are in bijection with the physical singularities of the scattering amplitude.
The Amplituhedron, discovered by Arkani-Hamed and Trnka, is a prototypical
positive geometry. It lives in momentum twistor space and describes tree-level
(and the integrands of planar loop-level) scattering amplitudes in maximally
supersymmetric Yang-Mills theory.
  In this dissertation, we study three positive geometries defined in on-shell
momentum space: the Arkani-Hamed-Bai-He-Yan (ABHY) associahedron, the Momentum
Amplituhedron, and the orthogonal Momentum Amplituhedron. Each describes
tree-level scattering amplitudes for different theories in different spacetime
dimensions. The three positive geometries share a series of interrelations in
terms of their boundary posets and canonical forms. We review these
relationships in detail, highlighting the author's contributions. We study
their boundary posets, classifying all boundaries and hence all physical
singularities at the tree level. We develop new combinatorial results to derive
rank-generating functions which enumerate boundaries according to their
dimension. These generating functions allow us to prove that the Euler
characteristics of the three positive geometries are one. In addition, we
discuss methods for manipulating canonical forms using ideas from computational
algebraic geometry.|
|**2023-06-08**|**How to Incorporate Systematic Effects into Parameter Determination**|David van Dyk et.al.|[2306.05271v1](http://arxiv.org/abs/2306.05271v1)|null|We describe two different approaches for incorporating systematics into
analyses for parameter determination in the physical sciences. We refer to
these as the Pragmatic and the Full methods, with the latter coming in two
variants: Full Likelihood and Fully Bayesian. By the use of a simple and
readily understood example, we point out the advantage of using the Full
Likelihood and Fully Bayesian approaches; a more realistic example from
Astrophysics is also presented. This could be relevant for data analyses in a
wide range of scientific fields, for situations where systematic effects need
to be incorporated in the analysis procedure. This note is an extension of part
of the talk by van Dyk at the PHYSTAT-Systematics meeting.|
|**2023-06-08**|**Linking Frequentist and Bayesian Change-Point Methods**|David Ardia et.al.|[2306.05265v1](http://arxiv.org/abs/2306.05265v1)|null|We show that the two-stage minimum description length (MDL) criterion widely
used to estimate linear change-point (CP) models corresponds to the marginal
likelihood of a Bayesian model with a specific class of prior distributions.
This allows results from the frequentist and Bayesian paradigms to be bridged
together. Thanks to this link, one can rely on the consistency of the number
and locations of the estimated CPs and the computational efficiency of
frequentist methods, and obtain a probability of observing a CP at a given
time, compute model posterior probabilities, and select or combine CP methods
via Bayesian posteriors. Furthermore, we adapt several CP methods to take
advantage of the MDL probabilistic representation. Based on simulated data, we
show that the adapted CP methods can improve structural break detection
compared to state-of-the-art approaches. Finally, we empirically illustrate the
usefulness of combining CP detection methods when dealing with long time series
and forecasting.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Quantum computing algorithms for inverse problems on graphs and an NP-complete inverse problem**|Joonas Ilmavirta et.al.|[2306.05253v1](http://arxiv.org/abs/2306.05253v1)|null|We consider an inverse problem for a finite graph $(X,E)$ where we are given
a subset of vertices $B\subset X$ and the distances $d_{(X,E)}(b_1,b_2)$ of all
vertices $b_1,b_2\in B$. The distance of points $x_1,x_2\in X$ is defined as
the minimal number of edges needed to connect two vertices, so all edges have
length 1. The inverse problem is a discrete version of the boundary rigidity
problem in Riemannian geometry or the inverse travel time problem in
geophysics. We will show that this problem has unique solution under certain
conditions and develop quantum computing methods to solve it. We prove the
following uniqueness result: when $(X,E)$ is a tree and $B$ is the set of
leaves of the tree, the graph $(X,E)$ can be uniquely determined in the class
of all graphs having a fixed number of vertices. We present a quantum computing
algorithm which produces a graph $(X,E)$, or one of those, which has a given
number of vertices and the required distances between vertices in $B$. To this
end we develop an algorithm that takes in a qubit representation of a graph and
combine it with Grover's search algorithm. The algorithm can be implemented
using only $O(|X|^2)$ qubits, the same order as the number of elements in the
adjacency matrix of $(X,E)$. It also has a quadratic improvement in
computational cost compared to standard classical algorithms. Finally, we
consider applications in theory of computation, and show that a slight
modification of the above inverse problem is NP-complete: all NP-problems can
be reduced to a discrete inverse problem we consider.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Investigating the OH-H2 relation in diffuse Galactic clouds**|Katherine Rawlins et.al.|[2306.05213v1](http://arxiv.org/abs/2306.05213v1)|null|We investigate the correlation between OH and H2 column densities in diffuse
Galactic clouds, in order to identify potential molecular tracers of
interstellar H2. For this, we analyse near-UV spectra extracted from the
ESO/VLT archives towards seventeen sightlines (five of them new) with known
N(H2), along with nine sightlines with no H2 information. N(OH) shows only
marginal correlation with N(H2) (10$^{20}$ to 2 x 10$^{21}$ cm$^{-2}$), at the
95 per cent confidence level. We use orthogonal distance regression analysis to
obtain N(OH)/N(H2) = (1.32+/-0.15) x 10$^{-7}$, which is ~ 33 per cent higher
than the previous estimates based on near-UV data. We also obtain N(CH)/N(H2) =
(3.83+/-0.23) x 10$^{-8}$ and a significant correlation between N(OH) and
N(CH), with N(OH) = (2.61+/-0.19) x N(CH), both of which are consistent with
previous results. Comparison with predictions of numerical models indicate that
OH absorption arises from diffuse gas (nH ~ 50 cm$^{-3}$) illuminated by
radiation fields ~ 0.5-5 G0, while CH is associated with higher density of 500
cm$^{-3}$. We posit that the apparent dichotomy in the properties of the
diffuse clouds giving rise to OH and CH absorption could be due to either (a)
the presence of multiple spectroscopically unresolved clouds along the
line-of-sight, or, (b) density gradients along the line-of-sight within a
single cloud.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs**|Constantin Christof et.al.|[2306.05185v1](http://arxiv.org/abs/2306.05185v1)|null|We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean**|Spyros Kondylatos et.al.|[2306.05144v1](http://arxiv.org/abs/2306.05144v1)|[link](https://github.com/orion-ai-lab/mesogeos)|We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.|
|**2023-06-08**|**Partition functions of non-Lagrangian theories from the holomorphic anomaly**|Francesco Fucito et.al.|[2306.05141v1](http://arxiv.org/abs/2306.05141v1)|null|The computation of the partition function in certain quantum field theories,
such as those of the Argyres-Douglas or Minahan-Nemeschansky type, is
problematic due to the lack of a Lagrangian description. In this paper, we use
the holomorphic anomaly equation to derive the gravitational corrections to the
prepotential of such theories at rank one by deforming them from the conformal
point. In the conformal limit, we find a general formula for the partition
function as a sum of hypergeometric functions. We show explicit results for the
round sphere and the Nekrasov-Shatashvili phases of the $\Omega$ background.
The first case is relevant for the derivation of extremal correlators in flat
space, whereas the second one has interesting applications for the study of
anharmonic oscillators.|
|**2023-06-08**|**Octree-based hierarchical sampling optimization for the volumetric super-resolution of scientific data**|Xinjie Wang et.al.|[2306.05133v1](http://arxiv.org/abs/2306.05133v1)|null|When introducing physics-constrained deep learning solutions to the
volumetric super-resolution of scientific data, the training is challenging to
converge and always time-consuming. We propose a new hierarchical sampling
method based on octree to solve these difficulties. In our approach, scientific
data is preprocessed before training, and a hierarchical octree-based data
structure is built to guide sampling on the latent context grid. Each leaf node
in the octree corresponds to an indivisible subblock of the volumetric data.
The dimensions of the subblocks are different, making the number of sample
points in each randomly cropped training data block to be adaptive. We
reconstruct the octree at intervals according to loss distribution to perform
the multi-stage training. With the Rayleigh-B\'enard convection problem, we
deploy our method to state-of-the-art models. We constructed adequate
experiments to evaluate the training performance and model accuracy of our
method. Experiments indicate that our sampling optimization improves the
convergence performance of physics-constrained deep learning super-resolution
solutions. Furthermore, the sample points and training time are significantly
reduced with no drop in model accuracy. We also test our method in training
tasks of other deep neural networks, and the results show our sampling
optimization has extensive effectiveness and applicability. The code is
publicly available at https://github.com/xinjiewang/octree-based_sampling.|
|**2023-06-08**|**The Uniform Even Subgraph and Its Connection to Phase Transitions of Graphical Representations of the Ising Model**|Ulrik Thinggaard Hansen et.al.|[2306.05130v1](http://arxiv.org/abs/2306.05130v1)|null|The uniform even subgraph is intimately related to the Ising model, the
random-cluster model, the random current model and the loop $\mathrm{O}$(1)
model. In this paper, we first prove that the uniform even subgraph of
$\mathbb{Z}^d$ percolates for $d \geq 2$ using its characterisation as the Haar
measure on the group of even graphs. We then tighten the result by showing that
the loop $\mathrm{O}$(1) model on $\mathbb{Z}^d$ percolates for $d \geq 2$ on
some interval $(1-\varepsilon,1]$. Finally, our main theorem is that the loop
$\mathrm{O}$(1) model and random current models corresponding to a
supercritical Ising model are always at least critical, in the sense that their
two-point correlation functions decay at most polynomially and the expected
cluster sizes are infinite.|
|**2023-06-08**|**Focus for Free in Density-Based Counting**|Zenglin Shi et.al.|[2306.05129v1](http://arxiv.org/abs/2306.05129v1)|null|This work considers supervised learning to count from images and their
corresponding point annotations. Where density-based counting methods typically
use the point annotations only to create Gaussian-density maps, which act as
the supervision signal, the starting point of this work is that point
annotations have counting potential beyond density map generation. We introduce
two methods that repurpose the available point annotations to enhance counting
performance. The first is a counting-specific augmentation that leverages point
annotations to simulate occluded objects in both input and density images to
enhance the network's robustness to occlusions. The second method, foreground
distillation, generates foreground masks from the point annotations, from which
we train an auxiliary network on images with blacked-out backgrounds. By doing
so, it learns to extract foreground counting knowledge without interference
from the background. These methods can be seamlessly integrated with existing
counting advances and are adaptable to different loss functions. We demonstrate
complementary effects of the approaches, allowing us to achieve robust counting
results even in challenging scenarios such as background clutter, occlusion,
and varying crowd densities. Our proposed approach achieves strong counting
results on multiple datasets, including ShanghaiTech Part\_A and Part\_B,
UCF\_QNRF, JHU-Crowd++, and NWPU-Crowd.|
|**2023-06-08**|**On irregular states and Argyres-Douglas theories**|Francesco Fucito et.al.|[2306.05127v1](http://arxiv.org/abs/2306.05127v1)|null|Conformal theories of the Argyres-Douglas type are notoriously hard to study
given that they are isolated and strongly coupled thus lacking a lagrangian
description. In flat space, an exact description is provided by the
Seiberg-Witten theory. Turning on a $\Omega$-background makes the geometry
``quantum" and tractable only in the weak curvature limit. In this paper we use
the AGT correspondence to derive $\Omega$-exact formulae for the partition
function, in the nearby of monopole points where the dynamics is described by
irregular conformal blocks of the CFT. The results are checked against those
obtained by the recursion relations coming from a conformal anomaly in the
region where the two approaches overlap. The Nekrasov-Shatashvili limit is also
discussed. Finally, we comment on the existence of black holes in De Sitter
space whose low energy dynamics is described by an Argyres-Douglas theory.|
|**2023-06-08**|**Proof-theoretic Semantics for Intuitionistic Multiplicative Linear Logic**|Alexander V. Gheorghiu et.al.|[2306.05106v1](http://arxiv.org/abs/2306.05106v1)|null|This work is the first exploration of proof-theoretic semantics for a
substructural logic. It focuses on the base-extension semantics (B-eS) for
intuitionistic multiplicative linear logic (IMLL). The starting point is a
review of Sandqvist's B-eS for intuitionistic propositional logic (IPL), for
which we propose an alternative treatment of conjunction that takes the form of
the generalized elimination rule for the connective. The resulting semantics is
shown to be sound and complete. This motivates our main contribution, a B-eS
for IMLL, in which the definitions of the logical constants all take the form
of their elimination rule and for which soundness and completeness are
established.|
|**2023-06-08**|**Diagnosing quantum phase transition via holographic entanglement entropy at finite temperature**|Huajie Gong et.al.|[2306.05096v1](http://arxiv.org/abs/2306.05096v1)|null|We investigate the behavior of the holographic entanglement entropy (HEE) in
proximity to the quantum critical points (QCPs) of the metal-insulator
transition (MIT) in the Einstein-Maxwell-dilaton-axions (EMDA) model. Due to
the fact that the ground state entropy density of the EMDA model is vanishing
for insulating phase, but non-vanishing for the metallic phase, we used to
expect that it is the HEE itself that characterizes the QCPs. This expectation
is validated for certain case, however, we make a noteworthy observation: for a
specific scenario, it is not the HEE itself but rather the second-order
derivative of HEE with respect to the lattice wave number that effectively
characterizes the quantum phase transition (QPT). This distinction arises due
to the influence of thermal effects. These findings present novel insights into
the interplay between HEE and QPTs in the context of the MIT, and have
significant implications for studying QPT at finite temperatures.|
|**2023-06-08**|**FAST reveals new evidence for M94 as a merger**|Ruilei Zhou et.al.|[2306.05080v1](http://arxiv.org/abs/2306.05080v1)|null|We report the first high-sensitivity HI observation toward the spiral galaxy
M94 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). From
these observations, we discovered that M94 has a very extended HI disk, twice
larger than that observed by THINGS, which is accompanied by an HI filament and
seven HVCs (high velocity clouds) at different distances. The projected
distances of these clouds and filament are less than 50 kpc from the galactic
center. We measured a total integrated flux (including all clouds/filament) of
127.3 ($\pm$1) Jy km s$^{-1}$, corresponding to a H I mass of
(6.51$\pm$0.06)$\times$10$^{8}$M$_{\odot}$, which is 63.0% more than that
observed by THINGS. By comparing numerical simulations with the HI maps and the
optical morphology of M94, we suggest that M94 is likely a remnant of a major
merger of two galaxies, and the HVCs and HI filament could be the tidal
features originated from the first collision of the merger happened about 5 Gyr
ago. Furthermore, we found a seemingly isolated HI cloud at a projection
distance of 109 kpc without any optical counterpart detected. We discussed the
possibilities of the origin of this cloud, such as dark dwarf galaxy and RELHIC
(REionization-Limited HI Cloud). Our results demonstrate that high-sensitivity
and wide-field HI imaging is important in revealing the diffuse cold gas
structures and tidal debris which is crucial to understanding the dynamical
evolution of galaxies.|
|**2023-06-08**|**Resolving nonclassical magnon composition of a magnetic ground state via a qubit**|Anna-Luisa E. Rmling et.al.|[2306.05065v1](http://arxiv.org/abs/2306.05065v1)|null|Recently gained insights into equilibrium squeezing and entanglement harbored
by magnets point towards exciting opportunities for quantum science and
technology, while concrete protocols for exploiting these are needed. Here, we
theoretically demonstrate that a direct dispersive coupling between a qubit and
a noneigenmode magnon enables detecting the magnonic number states' quantum
superposition that forms the ground state of the actual eigenmode -
squeezed-magnon - via qubit excitation spectroscopy. Furthermore, this unique
coupling is found to enable control over the equilibrium magnon squeezing and a
deterministic generation of squeezed even Fock states via the qubit state and
its excitation. Our work demonstrates direct dispersive coupling to
noneigenmodes, realizable in spin systems, as a general pathway to exploiting
the equilibrium squeezing and related quantum properties thereby motivating a
search for similar realizations in other platforms.|
|**2023-06-08**|**On the central value of Rankin $L$-functions for self-dual algebraic representations of linear groups over totally real fields**|Laurent Clozel et.al.|[2306.05049v1](http://arxiv.org/abs/2306.05049v1)|null|Deligne has formulated extremely influential conjectures about certain
special values of the $L$-functions of (Grothendieck) motives over a number
field $F$. Given the conjectural dictionary between motives and 'algebraic'
automorphic representations of $\textrm{GL}(N, {\mathbb A}_F)$, where ${\mathbb
A}_F$ denotes the ad\`eles of $F$, they translate into conjectures concerning
the $L$-functions of these automorphic representations. These complex
representations, when they are 'regular', can be conjugated by the
automorphisms of the complex field ${\mathbb C}$. It then follows, as a weak
consequence of Deligne's conjectures, that the vanishing at critical points
(integers of half-integers) of the automorphic $L$-functions should be
invariant by automorphisms of ${\mathbb C}$. If $F$ is totally imaginary, this
has been proven by Moeglin, for standard or Rankin $L$-functions. Here we
extend the result to Rankin $L$-fuctions for totally real fields $F$, under a
parity and a regularity assumption. The proof relies on Eisenstein cohomology
and the Zucker conjecture (a theorem of Looijenga and Saper-Stern.)|

### Point Cloud Segmentation
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Scalar curvature rigidity of degenerate warped product spaces**|Jinmin Wang et.al.|[2306.05413v1](http://arxiv.org/abs/2306.05413v1)|null|In this paper we prove the scalar curvature extremality and rigidity for a
class of warped product spaces that are possibly degenerate at the two ends.
The leaves of these warped product spaces can be any closed Riemannian manifold
with nonnegative curvature operator and nonvanishing Euler characteristic, flat
tori, round spheres and their direct products. In particular, we obtain the
scalar curvature extremality and rigidity for certain degenerate toric bands
and also for round spheres with two antipodal points removed. This answers the
corresponding questions of Gromov in all dimensions.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Picard and Brauer groups of $K(n)$-local spectra via profinite Galois descent**|Itamar Mor et.al.|[2306.05393v1](http://arxiv.org/abs/2306.05393v1)|null|Using the pro\'etale site, we construct models for the continuous actions of
the Morava stabiliser group on Morava E-theory, its $\infty$-category of
$K(n)$-local modules, and its Picard spectrum. For the two sheaves of spectra,
we evaluate the resulting descent spectral sequences: these can be thought of
as homotopy fixed point spectral sequences for the profinite Galois extension
$L_{K(n)} \mathbb S \to E_n$. We show that the descent spectral sequence for
the Morava E-theory sheaf is the $K(n)$-local $E_n$-Adams spectral sequence.
The spectral sequence for the sheaf of Picard spectra is closely related to one
recently defined by Heard; our formalism allows us to compare many
differentials with those in the $K(n)$-local $E_n$-Adams spectral sequence, and
isolate the exotic Picard elements in the $0$-stem. In particular, we show how
this recovers the computation due to Hopkins, Mahowald and Sadofsky of the
group $\mathrm{Pic}_1$ at all primes. We also use these methods to bound the
Brauer group of $K(n)$-local spectra, and compute this bound at height one.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**Improving the Reporting of Threats to Construct Validity**|Dag I. K. Sjberg et.al.|[2306.05336v1](http://arxiv.org/abs/2306.05336v1)|null|Background: Construct validity concerns the use of indicators to measure a
concept that is not directly measurable. Aim: This study intends to identify,
categorize, assess and quantify discussions of threats to construct validity in
empirical software engineering literature and use the findings to suggest ways
to improve the reporting of construct validity issues. Method: We analyzed 83
articles that report human-centric experiments published in five top-tier
software engineering journals from 2015 to 2019. The articles' text concerning
threats to construct validity was divided into segments (the unit of analysis)
based on predefined categories. The segments were then evaluated regarding
whether they clearly discussed a threat and a construct. Results: Three-fifths
of the segments were associated with topics not related to construct validity.
Two-thirds of the articles discussed construct validity without using the
definition of construct validity given in the article. The threats were clearly
described in more than four-fifths of the segments, but the construct in
question was clearly described in only two-thirds of the segments. The
construct was unclear when the discussion was not related to construct validity
but to other types of validity. Conclusions: The results show potential for
improving the understanding of construct validity in software engineering.
Recommendations addressing the identified weaknesses are given to improve the
awareness and reporting of CV.|
|**2023-06-08**|**FCNC charmed-hadron decays with invisible singlet particles in light of recent data**|Geng Li et.al.|[2306.05333v1](http://arxiv.org/abs/2306.05333v1)|null|The flavor-changing neutral current (FCNC) decays of charmed hadrons with
missing energy ($\slashed E$) can serve as potentially promising hunting
grounds for hints of new physics, as the standard-model backgrounds are very
suppressed. A few of such processes have been searched for in recent
experiments, particularly $D^0\to\slashed E$ by Belle and $D^0\to\pi^0\slashed
E$ and $\Lambda_c^+\to p\slashed E$ by BESIII, resulting in upper bounds on
their branching fractions. We consider them to illuminate the possible
contributions of the quark transition $c\to u\slashed E$ with a couple of
invisible spinless bosons carrying away the missing energy, assuming that they
are not charge conjugates of each other and hence can have unequal masses. We
find that these data are complementary in that they constrain different sets of
the underlying operators and do not cover the same ranges of the bosons'
masses, but there are regions not yet accessible. From the allowed parameter
space, we show that other $D$-meson decays, such as $D\to\rho\slashed E$, and
the charmed-baryon ones $\Xi_c\to(\Sigma,\Lambda)\slashed E$ can have sizable
branching fractions and therefore may offer further probes of the new-physics
interactions. We point out the importance of $D^0\to\gamma\slashed E$ which are
not yet searched for but could access parts of the parameter space beyond the
reach of the other modes. In addition, we look at a scenario where the
invisibles are instead fermionic, namely sterile neutrinos, and a scalar
leptoquark mediates $c\to u\slashed E$. We discuss the implications of the
aforesaid bounds for this model. The predictions we make for the various
charmed-hadron decays in the different scenarios may be testable in the near
future by BESIII and Belle II.|
|**2023-06-08**|**Hybridizable discontinuous Galerkin methods for the Monge-Ampere equation**|Ngoc Cuong Nguyen et.al.|[2306.05296v1](http://arxiv.org/abs/2306.05296v1)|null|We introduce two hybridizable discontinuous Galerkin (HDG) methods for
numerically solving the Monge-Ampere equation. The first HDG method is devised
to solve the nonlinear elliptic Monge-Ampere equation by using Newton's method.
The second HDG method is devised to solve a sequence of the Poisson equation
until convergence to a fixed-point solution of the Monge-Ampere equation is
reached. Numerical examples are presented to demonstrate the convergence and
accuracy of the HDG methods. Furthermore, the HDG methods are applied to
r-adaptive mesh generation by redistributing a given scalar density function
via the optimal transport theory. This r-adaptivity methodology leads to the
Monge-Ampere equation with a nonlinear Neumann boundary condition arising from
the optimal transport of the density function to conform the resulting
high-order mesh to the boundary. Hence, we extend the HDG methods to treat the
nonlinear Neumann boundary condition. Numerical experiments are presented to
illustrate the generation of r-adaptive high-order meshes on planar and curved
domains.|
|**2023-06-08**|**Positive Geometries for Scattering Amplitudes in Momentum Space**|Robert Moerman et.al.|[2306.05287v1](http://arxiv.org/abs/2306.05287v1)|null|Positive geometries provide a purely geometric point of departure for
studying scattering amplitudes in quantum field theory. A positive geometry is
a specific semi-algebraic set equipped with a unique rational top form - the
canonical form. There are known examples where the canonical form of some
positive geometry, defined in some kinematic space, encodes a scattering
amplitude in some theory. Remarkably, the boundaries of the positive geometry
are in bijection with the physical singularities of the scattering amplitude.
The Amplituhedron, discovered by Arkani-Hamed and Trnka, is a prototypical
positive geometry. It lives in momentum twistor space and describes tree-level
(and the integrands of planar loop-level) scattering amplitudes in maximally
supersymmetric Yang-Mills theory.
  In this dissertation, we study three positive geometries defined in on-shell
momentum space: the Arkani-Hamed-Bai-He-Yan (ABHY) associahedron, the Momentum
Amplituhedron, and the orthogonal Momentum Amplituhedron. Each describes
tree-level scattering amplitudes for different theories in different spacetime
dimensions. The three positive geometries share a series of interrelations in
terms of their boundary posets and canonical forms. We review these
relationships in detail, highlighting the author's contributions. We study
their boundary posets, classifying all boundaries and hence all physical
singularities at the tree level. We develop new combinatorial results to derive
rank-generating functions which enumerate boundaries according to their
dimension. These generating functions allow us to prove that the Euler
characteristics of the three positive geometries are one. In addition, we
discuss methods for manipulating canonical forms using ideas from computational
algebraic geometry.|
|**2023-06-08**|**How to Incorporate Systematic Effects into Parameter Determination**|David van Dyk et.al.|[2306.05271v1](http://arxiv.org/abs/2306.05271v1)|null|We describe two different approaches for incorporating systematics into
analyses for parameter determination in the physical sciences. We refer to
these as the Pragmatic and the Full methods, with the latter coming in two
variants: Full Likelihood and Fully Bayesian. By the use of a simple and
readily understood example, we point out the advantage of using the Full
Likelihood and Fully Bayesian approaches; a more realistic example from
Astrophysics is also presented. This could be relevant for data analyses in a
wide range of scientific fields, for situations where systematic effects need
to be incorporated in the analysis procedure. This note is an extension of part
of the talk by van Dyk at the PHYSTAT-Systematics meeting.|
|**2023-06-08**|**Linking Frequentist and Bayesian Change-Point Methods**|David Ardia et.al.|[2306.05265v1](http://arxiv.org/abs/2306.05265v1)|null|We show that the two-stage minimum description length (MDL) criterion widely
used to estimate linear change-point (CP) models corresponds to the marginal
likelihood of a Bayesian model with a specific class of prior distributions.
This allows results from the frequentist and Bayesian paradigms to be bridged
together. Thanks to this link, one can rely on the consistency of the number
and locations of the estimated CPs and the computational efficiency of
frequentist methods, and obtain a probability of observing a CP at a given
time, compute model posterior probabilities, and select or combine CP methods
via Bayesian posteriors. Furthermore, we adapt several CP methods to take
advantage of the MDL probabilistic representation. Based on simulated data, we
show that the adapted CP methods can improve structural break detection
compared to state-of-the-art approaches. Finally, we empirically illustrate the
usefulness of combining CP detection methods when dealing with long time series
and forecasting.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation**|Shishuai Hu et.al.|[2306.05254v1](http://arxiv.org/abs/2306.05254v1)|null|Deep learning-based medical image segmentation models suffer from performance
degradation when deployed to a new healthcare center. To address this issue,
unsupervised domain adaptation and multi-source domain generalization methods
have been proposed, which, however, are less favorable for clinical practice
due to the cost of acquiring target-domain data and the privacy concerns
associated with redistributing the data from multiple source domains. In this
paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle
\textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical
image segmentation. In C$^2$SDG, the shallower features of each image and its
style-augmented counterpart are extracted and used for contrastive training,
resulting in the disentangled style representations and structure
representations. The segmentation is performed based solely on the structure
representations. Our method is novel in the contrastive perspective that
enables channel-wise feature disentanglement using a single source domain. We
evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup
and optic disc segmentation benchmark. Our results suggest the effectiveness of
each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the
baseline and all competing methods with a large margin. The code will be
available at \url{https://github.com/ShishuaiHu/CCSDG}.|
|**2023-06-08**|**Quantum computing algorithms for inverse problems on graphs and an NP-complete inverse problem**|Joonas Ilmavirta et.al.|[2306.05253v1](http://arxiv.org/abs/2306.05253v1)|null|We consider an inverse problem for a finite graph $(X,E)$ where we are given
a subset of vertices $B\subset X$ and the distances $d_{(X,E)}(b_1,b_2)$ of all
vertices $b_1,b_2\in B$. The distance of points $x_1,x_2\in X$ is defined as
the minimal number of edges needed to connect two vertices, so all edges have
length 1. The inverse problem is a discrete version of the boundary rigidity
problem in Riemannian geometry or the inverse travel time problem in
geophysics. We will show that this problem has unique solution under certain
conditions and develop quantum computing methods to solve it. We prove the
following uniqueness result: when $(X,E)$ is a tree and $B$ is the set of
leaves of the tree, the graph $(X,E)$ can be uniquely determined in the class
of all graphs having a fixed number of vertices. We present a quantum computing
algorithm which produces a graph $(X,E)$, or one of those, which has a given
number of vertices and the required distances between vertices in $B$. To this
end we develop an algorithm that takes in a qubit representation of a graph and
combine it with Grover's search algorithm. The algorithm can be implemented
using only $O(|X|^2)$ qubits, the same order as the number of elements in the
adjacency matrix of $(X,E)$. It also has a quadratic improvement in
computational cost compared to standard classical algorithms. Finally, we
consider applications in theory of computation, and show that a slight
modification of the above inverse problem is NP-complete: all NP-problems can
be reduced to a discrete inverse problem we consider.|
|**2023-06-08**|**Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation**|Qiujie Dong et.al.|[2306.05246v1](http://arxiv.org/abs/2306.05246v1)|null|With the rapid development of geometric deep learning techniques, many
mesh-based convolutional operators have been proposed to bridge irregular mesh
structures and popular backbone networks. In this paper, we show that while
convolutions are helpful, a simple architecture based exclusively on
multi-layer perceptrons (MLPs) is competent enough to deal with mesh
classification and semantic segmentation. Our new network architecture, named
Mesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) and
dihedral angles as the input, replaces the convolution module of a ResNet with
Multi-layer Perceptron (MLP), and utilizes layer normalization (LN) to perform
the normalization of the layers. The all-MLP architecture operates in an
end-to-end fashion and does not include a pooling module. Extensive
experimental results on the mesh classification/segmentation tasks validate the
effectiveness of the all-MLP architecture.|
|**2023-06-08**|**Efficient Multi-Task Scene Analysis with RGB-D Transformers**|Shnke Benedikt Fischedick et.al.|[2306.05242v1](http://arxiv.org/abs/2306.05242v1)|[link](https://github.com/tui-nicr/nicr-scene-analysis-datasets)|Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Investigating the OH-H2 relation in diffuse Galactic clouds**|Katherine Rawlins et.al.|[2306.05213v1](http://arxiv.org/abs/2306.05213v1)|null|We investigate the correlation between OH and H2 column densities in diffuse
Galactic clouds, in order to identify potential molecular tracers of
interstellar H2. For this, we analyse near-UV spectra extracted from the
ESO/VLT archives towards seventeen sightlines (five of them new) with known
N(H2), along with nine sightlines with no H2 information. N(OH) shows only
marginal correlation with N(H2) (10$^{20}$ to 2 x 10$^{21}$ cm$^{-2}$), at the
95 per cent confidence level. We use orthogonal distance regression analysis to
obtain N(OH)/N(H2) = (1.32+/-0.15) x 10$^{-7}$, which is ~ 33 per cent higher
than the previous estimates based on near-UV data. We also obtain N(CH)/N(H2) =
(3.83+/-0.23) x 10$^{-8}$ and a significant correlation between N(OH) and
N(CH), with N(OH) = (2.61+/-0.19) x N(CH), both of which are consistent with
previous results. Comparison with predictions of numerical models indicate that
OH absorption arises from diffuse gas (nH ~ 50 cm$^{-3}$) illuminated by
radiation fields ~ 0.5-5 G0, while CH is associated with higher density of 500
cm$^{-3}$. We posit that the apparent dichotomy in the properties of the
diffuse clouds giving rise to OH and CH absorption could be due to either (a)
the presence of multiple spectroscopically unresolved clouds along the
line-of-sight, or, (b) density gradients along the line-of-sight within a
single cloud.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**Channel prior convolutional attention for medical image segmentation**|Hejun Huang et.al.|[2306.05196v1](http://arxiv.org/abs/2306.05196v1)|null|Characteristics such as low contrast and significant organ shape variations
are often exhibited in medical images. The improvement of segmentation
performance in medical imaging is limited by the generally insufficient
adaptive capabilities of existing attention mechanisms. An efficient Channel
Prior Convolutional Attention (CPCA) method is proposed in this paper,
supporting the dynamic distribution of attention weights in both channel and
spatial dimensions. Spatial relationships are effectively extracted while
preserving the channel prior by employing a multi-scale depth-wise
convolutional module. The ability to focus on informative channels and
important regions is possessed by CPCA. A segmentation network called CPCANet
for medical image segmentation is proposed based on CPCA. CPCANet is validated
on two publicly available datasets. Improved segmentation performance is
achieved by CPCANet while requiring fewer computational resources through
comparisons with state-of-the-art algorithms. Our code is publicly available at
\url{https://github.com/Cuthbert-Huang/CPCANet}.|
|**2023-06-08**|**On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs**|Constantin Christof et.al.|[2306.05185v1](http://arxiv.org/abs/2306.05185v1)|null|We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean**|Spyros Kondylatos et.al.|[2306.05144v1](http://arxiv.org/abs/2306.05144v1)|[link](https://github.com/orion-ai-lab/mesogeos)|We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.|

### 3D Object Tracking
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**Stochastic Multi-Person 3D Motion Forecasting**|Sirui Xu et.al.|[2306.05421v1](http://arxiv.org/abs/2306.05421v1)|null|This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**A ship-in-a-bottle quantum gas microscope for magnetic mixtures**|Maximilian Sohmen et.al.|[2306.05404v1](http://arxiv.org/abs/2306.05404v1)|null|Quantum gas microscopes are versatile and powerful tools for fundamental
science as well as promising candidates for enticing applications such as in
quantum simulation or quantum computation. Here we present a quantum gas
microscopy setup for experiments with highly magnetic atoms of the lanthanoid
elements erbium and dysprosium. Our setup features a non-magnetic,
non-conducting, large-working-distance, high-numerical-aperture, in-vacuum
microscope objective, mounted inside a glue-free quartz glass cell. The quartz
glass cell is enclosed by a compact multi-shell ferromagnetic shield that
passively suppresses external magnetic field noise by a factor of more than a
thousand. Our setup will enable direct manipulation and probing of the rich
quantum many-body physics of dipolar atoms in optical lattices, and bears the
potential to put exciting theory proposals -- including exotic magnetic phases
and quantum phase transitions -- to an experimental test.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Improving structural damage tolerance and fracture energy via bamboo-inspired void patterns**|Xiaoheng Zhu et.al.|[2306.05365v1](http://arxiv.org/abs/2306.05365v1)|null|Bamboo has a functionally-graded microstructure that endows it with a
combination of desirable properties, such as high failure strain, high
toughness, and a low density. As a result, bamboo has been widely used in
load-bearing structures. In this work, we study the use of bamboo-inspired void
patterns to geometrically improve the failure properties of structures made
from brittle polymers. We perform finite element analysis and experiments on
3D-printed structures to quantify the effect of the shape and spatial
distribution of voids on the fracture behavior. The introduction of periodic,
uniformly distributed voids in notched bend specimens leads to a 15-fold
increase in the work of fracture relative to solid specimens. Adding a gradient
to the pattern of voids leads to a cumulative 55-fold improvement in the work
of fracture. Mechanistically, the individual voids result in crack blunting,
which suppresses crack initiation, while neighboring voids redistribute
stresses throughout the sample to enable large deformation before failure. In
addition, we conduct qualitative, low-energy impact experiments on PMMA plates
with laser-cut void patterns, illustrating the broader potential for this
strategy to improve damage tolerance and energy absorption in a wide range of
materials systems.|
|**2023-06-08**|**An inflation model for massive primordial black holes to interpret the JWST observations**|Bing-Yu Su et.al.|[2306.05364v1](http://arxiv.org/abs/2306.05364v1)|null|The first observations of the James Webb Space Telescope (JWST) have
identified six massive galaxy candidates with the stellar masses $M_\ast\gtrsim
10^{10}\,M_\odot$ at high redshifts $7.4\lesssim z\lesssim 9.1$, with two most
massive high-$z$ objects having the cumulative comoving number densities
$n_{\rm G}$ up to $1.6\times 10^{-5}\, {\rm Mpc}^{-3}$. The presence of such
massive sources in the early universe challenges the standard $\Lambda$CDM
model since the needed star formation efficiency is unrealistically high. This
tension can be alleviated via the accretion of massive primordial black holes
(PBHs). In this work, with the updated data from the first JWST observations,
we find that the PBHs with mass $10^8\,M_\odot\lesssim M_{\rm PBH}\lesssim
10^{11}\,M_\odot$ can act as the seeds of extremely massive galaxies even with
a low abundance $10^{-7}\lesssim f_{\rm PBH}\lesssim 10^{-3}$. We construct an
ultraslow-roll inflation model and investigate its possibility of producing the
required PBHs. We explore the model in two cases, depending on whether there is
a perfect plateau on the inflaton potential. If the plateau is allowed to
incline slightly, our model can produce the PBHs that cover the required PBH
mass and abundance range to explain the JWST data.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models**|Tiantian Feng et.al.|[2306.05350v1](http://arxiv.org/abs/2306.05350v1)|null|Many recent studies have focused on fine-tuning pre-trained models for speech
emotion recognition (SER), resulting in promising performance compared to
traditional methods that rely largely on low-level, knowledge-inspired acoustic
features. These pre-trained speech models learn general-purpose speech
representations using self-supervised or weakly-supervised learning objectives
from large-scale datasets. Despite the significant advances made in SER through
the use of pre-trained architecture, fine-tuning these large pre-trained models
for different datasets requires saving copies of entire weight parameters,
rendering them impractical to deploy in real-world settings. As an alternative,
this work explores parameter-efficient fine-tuning (PEFT) approaches for
adapting pre-trained speech models for emotion recognition. Specifically, we
evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa
(Low-rank approximation) on four popular SER testbeds. Our results reveal that
LoRa achieves the best fine-tuning performance in emotion recognition while
enhancing fairness and requiring only a minimal extra amount of weight
parameters. Furthermore, our findings offer novel insights into future research
directions in SER, distinct from existing approaches focusing on directly
fine-tuning the model architecture. Our code is publicly available under:
https://github.com/usc-sail/peft-ser.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**A Review of the Recent Developments in the Fabrication Processes of CMOS Image Sensors for Smartphones**|Kirthika Nahalingam et.al.|[2306.05339v1](http://arxiv.org/abs/2306.05339v1)|null|CMOS Image Sensors are experiencing significant growth due to their
capabilities to be integrated in smartphones with refined image quality. One of
the major contributions to the growth of image sensors is the innovation
brought about in their fabrication processes. This paper presents a detailed
review of the different fabrication processes of the CMOS Image Sensors and its
impact on the image quality of smartphone pictures. Fabrication of CMOS image
sensors using wafer bonding technologies such as Through Silicon Vias and CuCu
hybrid bonding along with their experimental results are discussed. A 2 layer
architecture of photodiode and pixel transistors has adopted the 3D sequential
integration, by which the wafers are bonded together one after the other in the
fabrication process. Electrical characteristics and reliability test results
are presented for the former two fabrication processes and the improvements in
the pixels performance such as conversion gain, quantum efficiency, full well
capacity and dynamic range for the 2 layer architecture are discussed.|
|**2023-06-08**|**Spontaneous Self-Constraint in Active Nematic Flows**|Louise C. Head et.al.|[2306.05328v1](http://arxiv.org/abs/2306.05328v1)|null|Active processes drive and guide biological dynamics across scales -- from
subcellular cytoskeletal remodelling, through tissue development in
embryogenesis, to population-level bacterial colonies expansion. In each of
these, biological functionality requires collective flows to occur while
self-organized structures are protected; however, the mechanisms by which
active flows can spontaneously constrain their dynamics to preserve structure
have not previously been explained. By studying collective flows and defect
dynamics in active nematic films, we demonstrate the existence of a
self-constraint -- a two-way, spontaneously arising relationship between
activity-driven isosurfaces of flow boundaries and mesoscale nematic
structures. Our results show that self-motile defects are tightly constrained
to viscometric surfaces -- contours along which vorticity and strain-rate
balance. This in turn reveals that self-motile defects break mirror symmetry
when they move along a single viscometric surface, in contrast with
expectations. This is explained by an interdependence between viscometric
surfaces and bend walls -- elongated narrow kinks in the orientation field.
Although we focus on extensile nematic films, numerical results show the
constraint holds whenever activity leads to motile half-charge defects. This
mesoscale cross-field self-constraint offers a new framework for tackling
complex 3D active turbulence, designing dynamic control into biomimetic
materials, and understanding how biological systems can employ active stress
for dynamic self-organization.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**Spin squeezing in mixed-dimensional anisotropic lattice models**|Mikhail Mamaev et.al.|[2306.05313v1](http://arxiv.org/abs/2306.05313v1)|null|We describe a theoretical scheme for generating scalable spin squeezing with
nearest-neighbour interactions between spin-1/2 particles in a 3D lattice,
which are naturally present in state-of-the-art 3D optical lattice clocks. We
propose to use strong isotropic Heisenberg interactions within individual
planes of the lattice, forcing the constituent spin-1/2s to behave as large
collective spins. These large spins are then coupled with XXZ anisotropic
interactions along a third direction of the lattice. This system can be
realized via superexchange interactions in a 3D optical lattice subject to an
external linear potential, such as gravity, and in the presence of spin-orbit
coupling (SOC) to generate spin anisotropic interactions. We show there is a
wide range of parameters in this setting where the spin squeezing improves with
increasing system size even in the presence of holes.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and
neutron star mergers are obtained from nuclear matter dynamical structure
functions that encode many-body effects from nuclear mean fields and
correlations. We employ nuclear interactions from chiral effective field theory
to calculate the density, spin, isospin, and spin-isospin response functions of
warm beta-equilibrium nuclear matter. We include corrections to the
single-particle energies in the mean field approximation as well as vertex
corrections resummed in the random phase approximation (RPA), including, for
the first time, both direct and exchange diagrams. We find that correlations
included through the RPA redistribute the strength of the response to higher
energy for neutrino absorption and lower energy for antineutrino absorption.
This tends to suppress the absorption rate of electron neutrinos across all
relevant energy scales. In contrast, the inclusion of RPA correlations enhances
the electron antineutrino absorption rate at low energy and supresses the rate
at high energy. These effects are especially important at high-density and in
the vicinity of the neutrino decoupling region. Implications for heavy element
nucleosynthesis, electromagnetic signatures of compact object mergers,
supernova dynamics, and neutrino detection from galactic supernovae are
discussed briefly.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|

### 3D Object Detection
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**Stochastic Multi-Person 3D Motion Forecasting**|Sirui Xu et.al.|[2306.05421v1](http://arxiv.org/abs/2306.05421v1)|null|This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**A ship-in-a-bottle quantum gas microscope for magnetic mixtures**|Maximilian Sohmen et.al.|[2306.05404v1](http://arxiv.org/abs/2306.05404v1)|null|Quantum gas microscopes are versatile and powerful tools for fundamental
science as well as promising candidates for enticing applications such as in
quantum simulation or quantum computation. Here we present a quantum gas
microscopy setup for experiments with highly magnetic atoms of the lanthanoid
elements erbium and dysprosium. Our setup features a non-magnetic,
non-conducting, large-working-distance, high-numerical-aperture, in-vacuum
microscope objective, mounted inside a glue-free quartz glass cell. The quartz
glass cell is enclosed by a compact multi-shell ferromagnetic shield that
passively suppresses external magnetic field noise by a factor of more than a
thousand. Our setup will enable direct manipulation and probing of the rich
quantum many-body physics of dipolar atoms in optical lattices, and bears the
potential to put exciting theory proposals -- including exotic magnetic phases
and quantum phase transitions -- to an experimental test.|
|**2023-06-08**|**Implications of atmospheric non-detections for Trappist-1 inner planets on atmospheric retention prospects for outer planets**|Joshua Krissansen-Totton et.al.|[2306.05397v1](http://arxiv.org/abs/2306.05397v1)|null|JWST secondary eclipse observations of Trappist-1b seemingly disfavor
atmospheres >~1 bar since heat redistribution is expected to yield dayside
emission temperature below the ~500 K observed. Given the similar densities of
Trappist-1 planets, and the theoretical potential for atmospheric erosion
around late M-dwarfs, this observation might be assumed to imply substantial
atmospheres are also unlikely for the outer planets. However, the processes
governing atmosphere erosion and replenishment are fundamentally different for
inner and outer planets. Here, an atmosphere-interior evolution model is used
to show that an airless Trappist-1b (and c) only weakly constrains stellar
evolution, and that the odds of outer planets e and f retaining substantial
atmospheres remain largely unchanged. This is true even if the initial volatile
inventories of planets in the Trappist-1 system are highly correlated. The
reason for this result is that b and c sit unambiguously interior to the
runaway greenhouse limit, and so have potentially experienced ~8 Gyr of
XUV-driven hydrodynamic escape; complete atmospheric erosion in this
environment only weakly constrains stellar evolution and escape
parameterizations. In contrast, e and f reside within the habitable zone, and
likely experienced a comparatively short steam atmosphere during Trappist-1's
pre-main sequence, and consequently complete atmospheric erosion remains
unlikely across a broad swath of parameter space (e and f retain atmospheres in
~98% of model runs). Naturally, it is still possible that all Trappist-1
planets formed volatile-poor and are all airless today. But the airlessness of
b (and c) does not require this, and as such, JWST transit spectroscopy of e
and f remains the best near-term opportunity to characterize the atmospheres of
habitable zone terrestrial planets.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Detecting Neural Trojans Through Merkle Trees**|Joshua Strubel et.al.|[2306.05368v1](http://arxiv.org/abs/2306.05368v1)|null|Deep neural networks are utilized in a growing number of industries. Much of
the current literature focuses on the applications of deep neural networks
without discussing the security of the network itself. One security issue
facing deep neural networks is neural trojans. Through a neural trojan, a
malicious actor may force the deep neural network to act in unintended ways.
Several potential defenses have been proposed, but they are computationally
expensive, complex, or unusable in commercial applications. We propose Merkle
trees as a novel way to detect and isolate neural trojans.|
|**2023-06-08**|**Improving structural damage tolerance and fracture energy via bamboo-inspired void patterns**|Xiaoheng Zhu et.al.|[2306.05365v1](http://arxiv.org/abs/2306.05365v1)|null|Bamboo has a functionally-graded microstructure that endows it with a
combination of desirable properties, such as high failure strain, high
toughness, and a low density. As a result, bamboo has been widely used in
load-bearing structures. In this work, we study the use of bamboo-inspired void
patterns to geometrically improve the failure properties of structures made
from brittle polymers. We perform finite element analysis and experiments on
3D-printed structures to quantify the effect of the shape and spatial
distribution of voids on the fracture behavior. The introduction of periodic,
uniformly distributed voids in notched bend specimens leads to a 15-fold
increase in the work of fracture relative to solid specimens. Adding a gradient
to the pattern of voids leads to a cumulative 55-fold improvement in the work
of fracture. Mechanistically, the individual voids result in crack blunting,
which suppresses crack initiation, while neighboring voids redistribute
stresses throughout the sample to enable large deformation before failure. In
addition, we conduct qualitative, low-energy impact experiments on PMMA plates
with laser-cut void patterns, illustrating the broader potential for this
strategy to improve damage tolerance and energy absorption in a wide range of
materials systems.|
|**2023-06-08**|**An inflation model for massive primordial black holes to interpret the JWST observations**|Bing-Yu Su et.al.|[2306.05364v1](http://arxiv.org/abs/2306.05364v1)|null|The first observations of the James Webb Space Telescope (JWST) have
identified six massive galaxy candidates with the stellar masses $M_\ast\gtrsim
10^{10}\,M_\odot$ at high redshifts $7.4\lesssim z\lesssim 9.1$, with two most
massive high-$z$ objects having the cumulative comoving number densities
$n_{\rm G}$ up to $1.6\times 10^{-5}\, {\rm Mpc}^{-3}$. The presence of such
massive sources in the early universe challenges the standard $\Lambda$CDM
model since the needed star formation efficiency is unrealistically high. This
tension can be alleviated via the accretion of massive primordial black holes
(PBHs). In this work, with the updated data from the first JWST observations,
we find that the PBHs with mass $10^8\,M_\odot\lesssim M_{\rm PBH}\lesssim
10^{11}\,M_\odot$ can act as the seeds of extremely massive galaxies even with
a low abundance $10^{-7}\lesssim f_{\rm PBH}\lesssim 10^{-3}$. We construct an
ultraslow-roll inflation model and investigate its possibility of producing the
required PBHs. We explore the model in two cases, depending on whether there is
a perfect plateau on the inflaton potential. If the plateau is allowed to
incline slightly, our model can produce the PBHs that cover the required PBH
mass and abundance range to explain the JWST data.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models**|Tiantian Feng et.al.|[2306.05350v1](http://arxiv.org/abs/2306.05350v1)|null|Many recent studies have focused on fine-tuning pre-trained models for speech
emotion recognition (SER), resulting in promising performance compared to
traditional methods that rely largely on low-level, knowledge-inspired acoustic
features. These pre-trained speech models learn general-purpose speech
representations using self-supervised or weakly-supervised learning objectives
from large-scale datasets. Despite the significant advances made in SER through
the use of pre-trained architecture, fine-tuning these large pre-trained models
for different datasets requires saving copies of entire weight parameters,
rendering them impractical to deploy in real-world settings. As an alternative,
this work explores parameter-efficient fine-tuning (PEFT) approaches for
adapting pre-trained speech models for emotion recognition. Specifically, we
evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa
(Low-rank approximation) on four popular SER testbeds. Our results reveal that
LoRa achieves the best fine-tuning performance in emotion recognition while
enhancing fairness and requiring only a minimal extra amount of weight
parameters. Furthermore, our findings offer novel insights into future research
directions in SER, distinct from existing approaches focusing on directly
fine-tuning the model architecture. Our code is publicly available under:
https://github.com/usc-sail/peft-ser.|
|**2023-06-08**|**First constraints on the strength of the extragalactic magnetic field from $$-ray observations of GRB 221009A**|Timur A. Dzhatdoev et.al.|[2306.05347v1](http://arxiv.org/abs/2306.05347v1)|null|The extragalactic magnetic field (EGMF) could be probed with $\gamma$-ray
observations of distant sources. Primary very high energy (VHE) $\gamma$-rays
from these sources absorb on extragalactic background light photons, and
secondary electrons/positrons from the pair production acts create cascade
$\gamma$-rays. These cascade $\gamma$-rays could be detected with space
$\gamma$-ray telescopes such as Fermi-LAT. The $\gamma$-ray burst GRB 221009A
was an exceptionally bright transient well suited for intergalactic
$\gamma$-ray propagation studies. Using publicly-available Fermi-LAT data, we
obtain upper limits on the spectrum of delayed emission from GRB 221009A during
the time window of 30 days after the burst, and compare these with model
spectra calculated for various EGMF strengths $B$, obtaining lower limits on
$B$. We show that the values of $B < 10^{-18}$ G are excluded. For some
optimistic models of the VHE spectrum of GRB 221009A, the values of $B <
10^{-17}$ G are excluded.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**A Review of the Recent Developments in the Fabrication Processes of CMOS Image Sensors for Smartphones**|Kirthika Nahalingam et.al.|[2306.05339v1](http://arxiv.org/abs/2306.05339v1)|null|CMOS Image Sensors are experiencing significant growth due to their
capabilities to be integrated in smartphones with refined image quality. One of
the major contributions to the growth of image sensors is the innovation
brought about in their fabrication processes. This paper presents a detailed
review of the different fabrication processes of the CMOS Image Sensors and its
impact on the image quality of smartphone pictures. Fabrication of CMOS image
sensors using wafer bonding technologies such as Through Silicon Vias and CuCu
hybrid bonding along with their experimental results are discussed. A 2 layer
architecture of photodiode and pixel transistors has adopted the 3D sequential
integration, by which the wafers are bonded together one after the other in the
fabrication process. Electrical characteristics and reliability test results
are presented for the former two fabrication processes and the improvements in
the pixels performance such as conversion gain, quantum efficiency, full well
capacity and dynamic range for the 2 layer architecture are discussed.|
|**2023-06-08**|**Spontaneous Self-Constraint in Active Nematic Flows**|Louise C. Head et.al.|[2306.05328v1](http://arxiv.org/abs/2306.05328v1)|null|Active processes drive and guide biological dynamics across scales -- from
subcellular cytoskeletal remodelling, through tissue development in
embryogenesis, to population-level bacterial colonies expansion. In each of
these, biological functionality requires collective flows to occur while
self-organized structures are protected; however, the mechanisms by which
active flows can spontaneously constrain their dynamics to preserve structure
have not previously been explained. By studying collective flows and defect
dynamics in active nematic films, we demonstrate the existence of a
self-constraint -- a two-way, spontaneously arising relationship between
activity-driven isosurfaces of flow boundaries and mesoscale nematic
structures. Our results show that self-motile defects are tightly constrained
to viscometric surfaces -- contours along which vorticity and strain-rate
balance. This in turn reveals that self-motile defects break mirror symmetry
when they move along a single viscometric surface, in contrast with
expectations. This is explained by an interdependence between viscometric
surfaces and bend walls -- elongated narrow kinks in the orientation field.
Although we focus on extensile nematic films, numerical results show the
constraint holds whenever activity leads to motile half-charge defects. This
mesoscale cross-field self-constraint offers a new framework for tackling
complex 3D active turbulence, designing dynamic control into biomimetic
materials, and understanding how biological systems can employ active stress
for dynamic self-organization.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.|
|**2023-06-08**|**Spin squeezing in mixed-dimensional anisotropic lattice models**|Mikhail Mamaev et.al.|[2306.05313v1](http://arxiv.org/abs/2306.05313v1)|null|We describe a theoretical scheme for generating scalable spin squeezing with
nearest-neighbour interactions between spin-1/2 particles in a 3D lattice,
which are naturally present in state-of-the-art 3D optical lattice clocks. We
propose to use strong isotropic Heisenberg interactions within individual
planes of the lattice, forcing the constituent spin-1/2s to behave as large
collective spins. These large spins are then coupled with XXZ anisotropic
interactions along a third direction of the lattice. This system can be
realized via superexchange interactions in a 3D optical lattice subject to an
external linear potential, such as gravity, and in the presence of spin-orbit
coupling (SOC) to generate spin anisotropic interactions. We show there is a
wide range of parameters in this setting where the spin squeezing improves with
increasing system size even in the presence of holes.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|

### Point Cloud
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Scalar curvature rigidity of degenerate warped product spaces**|Jinmin Wang et.al.|[2306.05413v1](http://arxiv.org/abs/2306.05413v1)|null|In this paper we prove the scalar curvature extremality and rigidity for a
class of warped product spaces that are possibly degenerate at the two ends.
The leaves of these warped product spaces can be any closed Riemannian manifold
with nonnegative curvature operator and nonvanishing Euler characteristic, flat
tori, round spheres and their direct products. In particular, we obtain the
scalar curvature extremality and rigidity for certain degenerate toric bands
and also for round spheres with two antipodal points removed. This answers the
corresponding questions of Gromov in all dimensions.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Picard and Brauer groups of $K(n)$-local spectra via profinite Galois descent**|Itamar Mor et.al.|[2306.05393v1](http://arxiv.org/abs/2306.05393v1)|null|Using the pro\'etale site, we construct models for the continuous actions of
the Morava stabiliser group on Morava E-theory, its $\infty$-category of
$K(n)$-local modules, and its Picard spectrum. For the two sheaves of spectra,
we evaluate the resulting descent spectral sequences: these can be thought of
as homotopy fixed point spectral sequences for the profinite Galois extension
$L_{K(n)} \mathbb S \to E_n$. We show that the descent spectral sequence for
the Morava E-theory sheaf is the $K(n)$-local $E_n$-Adams spectral sequence.
The spectral sequence for the sheaf of Picard spectra is closely related to one
recently defined by Heard; our formalism allows us to compare many
differentials with those in the $K(n)$-local $E_n$-Adams spectral sequence, and
isolate the exotic Picard elements in the $0$-stem. In particular, we show how
this recovers the computation due to Hopkins, Mahowald and Sadofsky of the
group $\mathrm{Pic}_1$ at all primes. We also use these methods to bound the
Brauer group of $K(n)$-local spectra, and compute this bound at height one.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**FCNC charmed-hadron decays with invisible singlet particles in light of recent data**|Geng Li et.al.|[2306.05333v1](http://arxiv.org/abs/2306.05333v1)|null|The flavor-changing neutral current (FCNC) decays of charmed hadrons with
missing energy ($\slashed E$) can serve as potentially promising hunting
grounds for hints of new physics, as the standard-model backgrounds are very
suppressed. A few of such processes have been searched for in recent
experiments, particularly $D^0\to\slashed E$ by Belle and $D^0\to\pi^0\slashed
E$ and $\Lambda_c^+\to p\slashed E$ by BESIII, resulting in upper bounds on
their branching fractions. We consider them to illuminate the possible
contributions of the quark transition $c\to u\slashed E$ with a couple of
invisible spinless bosons carrying away the missing energy, assuming that they
are not charge conjugates of each other and hence can have unequal masses. We
find that these data are complementary in that they constrain different sets of
the underlying operators and do not cover the same ranges of the bosons'
masses, but there are regions not yet accessible. From the allowed parameter
space, we show that other $D$-meson decays, such as $D\to\rho\slashed E$, and
the charmed-baryon ones $\Xi_c\to(\Sigma,\Lambda)\slashed E$ can have sizable
branching fractions and therefore may offer further probes of the new-physics
interactions. We point out the importance of $D^0\to\gamma\slashed E$ which are
not yet searched for but could access parts of the parameter space beyond the
reach of the other modes. In addition, we look at a scenario where the
invisibles are instead fermionic, namely sterile neutrinos, and a scalar
leptoquark mediates $c\to u\slashed E$. We discuss the implications of the
aforesaid bounds for this model. The predictions we make for the various
charmed-hadron decays in the different scenarios may be testable in the near
future by BESIII and Belle II.|
|**2023-06-08**|**Hybridizable discontinuous Galerkin methods for the Monge-Ampere equation**|Ngoc Cuong Nguyen et.al.|[2306.05296v1](http://arxiv.org/abs/2306.05296v1)|null|We introduce two hybridizable discontinuous Galerkin (HDG) methods for
numerically solving the Monge-Ampere equation. The first HDG method is devised
to solve the nonlinear elliptic Monge-Ampere equation by using Newton's method.
The second HDG method is devised to solve a sequence of the Poisson equation
until convergence to a fixed-point solution of the Monge-Ampere equation is
reached. Numerical examples are presented to demonstrate the convergence and
accuracy of the HDG methods. Furthermore, the HDG methods are applied to
r-adaptive mesh generation by redistributing a given scalar density function
via the optimal transport theory. This r-adaptivity methodology leads to the
Monge-Ampere equation with a nonlinear Neumann boundary condition arising from
the optimal transport of the density function to conform the resulting
high-order mesh to the boundary. Hence, we extend the HDG methods to treat the
nonlinear Neumann boundary condition. Numerical experiments are presented to
illustrate the generation of r-adaptive high-order meshes on planar and curved
domains.|
|**2023-06-08**|**Positive Geometries for Scattering Amplitudes in Momentum Space**|Robert Moerman et.al.|[2306.05287v1](http://arxiv.org/abs/2306.05287v1)|null|Positive geometries provide a purely geometric point of departure for
studying scattering amplitudes in quantum field theory. A positive geometry is
a specific semi-algebraic set equipped with a unique rational top form - the
canonical form. There are known examples where the canonical form of some
positive geometry, defined in some kinematic space, encodes a scattering
amplitude in some theory. Remarkably, the boundaries of the positive geometry
are in bijection with the physical singularities of the scattering amplitude.
The Amplituhedron, discovered by Arkani-Hamed and Trnka, is a prototypical
positive geometry. It lives in momentum twistor space and describes tree-level
(and the integrands of planar loop-level) scattering amplitudes in maximally
supersymmetric Yang-Mills theory.
  In this dissertation, we study three positive geometries defined in on-shell
momentum space: the Arkani-Hamed-Bai-He-Yan (ABHY) associahedron, the Momentum
Amplituhedron, and the orthogonal Momentum Amplituhedron. Each describes
tree-level scattering amplitudes for different theories in different spacetime
dimensions. The three positive geometries share a series of interrelations in
terms of their boundary posets and canonical forms. We review these
relationships in detail, highlighting the author's contributions. We study
their boundary posets, classifying all boundaries and hence all physical
singularities at the tree level. We develop new combinatorial results to derive
rank-generating functions which enumerate boundaries according to their
dimension. These generating functions allow us to prove that the Euler
characteristics of the three positive geometries are one. In addition, we
discuss methods for manipulating canonical forms using ideas from computational
algebraic geometry.|
|**2023-06-08**|**How to Incorporate Systematic Effects into Parameter Determination**|David van Dyk et.al.|[2306.05271v1](http://arxiv.org/abs/2306.05271v1)|null|We describe two different approaches for incorporating systematics into
analyses for parameter determination in the physical sciences. We refer to
these as the Pragmatic and the Full methods, with the latter coming in two
variants: Full Likelihood and Fully Bayesian. By the use of a simple and
readily understood example, we point out the advantage of using the Full
Likelihood and Fully Bayesian approaches; a more realistic example from
Astrophysics is also presented. This could be relevant for data analyses in a
wide range of scientific fields, for situations where systematic effects need
to be incorporated in the analysis procedure. This note is an extension of part
of the talk by van Dyk at the PHYSTAT-Systematics meeting.|
|**2023-06-08**|**Linking Frequentist and Bayesian Change-Point Methods**|David Ardia et.al.|[2306.05265v1](http://arxiv.org/abs/2306.05265v1)|null|We show that the two-stage minimum description length (MDL) criterion widely
used to estimate linear change-point (CP) models corresponds to the marginal
likelihood of a Bayesian model with a specific class of prior distributions.
This allows results from the frequentist and Bayesian paradigms to be bridged
together. Thanks to this link, one can rely on the consistency of the number
and locations of the estimated CPs and the computational efficiency of
frequentist methods, and obtain a probability of observing a CP at a given
time, compute model posterior probabilities, and select or combine CP methods
via Bayesian posteriors. Furthermore, we adapt several CP methods to take
advantage of the MDL probabilistic representation. Based on simulated data, we
show that the adapted CP methods can improve structural break detection
compared to state-of-the-art approaches. Finally, we empirically illustrate the
usefulness of combining CP detection methods when dealing with long time series
and forecasting.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Quantum computing algorithms for inverse problems on graphs and an NP-complete inverse problem**|Joonas Ilmavirta et.al.|[2306.05253v1](http://arxiv.org/abs/2306.05253v1)|null|We consider an inverse problem for a finite graph $(X,E)$ where we are given
a subset of vertices $B\subset X$ and the distances $d_{(X,E)}(b_1,b_2)$ of all
vertices $b_1,b_2\in B$. The distance of points $x_1,x_2\in X$ is defined as
the minimal number of edges needed to connect two vertices, so all edges have
length 1. The inverse problem is a discrete version of the boundary rigidity
problem in Riemannian geometry or the inverse travel time problem in
geophysics. We will show that this problem has unique solution under certain
conditions and develop quantum computing methods to solve it. We prove the
following uniqueness result: when $(X,E)$ is a tree and $B$ is the set of
leaves of the tree, the graph $(X,E)$ can be uniquely determined in the class
of all graphs having a fixed number of vertices. We present a quantum computing
algorithm which produces a graph $(X,E)$, or one of those, which has a given
number of vertices and the required distances between vertices in $B$. To this
end we develop an algorithm that takes in a qubit representation of a graph and
combine it with Grover's search algorithm. The algorithm can be implemented
using only $O(|X|^2)$ qubits, the same order as the number of elements in the
adjacency matrix of $(X,E)$. It also has a quadratic improvement in
computational cost compared to standard classical algorithms. Finally, we
consider applications in theory of computation, and show that a slight
modification of the above inverse problem is NP-complete: all NP-problems can
be reduced to a discrete inverse problem we consider.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Investigating the OH-H2 relation in diffuse Galactic clouds**|Katherine Rawlins et.al.|[2306.05213v1](http://arxiv.org/abs/2306.05213v1)|null|We investigate the correlation between OH and H2 column densities in diffuse
Galactic clouds, in order to identify potential molecular tracers of
interstellar H2. For this, we analyse near-UV spectra extracted from the
ESO/VLT archives towards seventeen sightlines (five of them new) with known
N(H2), along with nine sightlines with no H2 information. N(OH) shows only
marginal correlation with N(H2) (10$^{20}$ to 2 x 10$^{21}$ cm$^{-2}$), at the
95 per cent confidence level. We use orthogonal distance regression analysis to
obtain N(OH)/N(H2) = (1.32+/-0.15) x 10$^{-7}$, which is ~ 33 per cent higher
than the previous estimates based on near-UV data. We also obtain N(CH)/N(H2) =
(3.83+/-0.23) x 10$^{-8}$ and a significant correlation between N(OH) and
N(CH), with N(OH) = (2.61+/-0.19) x N(CH), both of which are consistent with
previous results. Comparison with predictions of numerical models indicate that
OH absorption arises from diffuse gas (nH ~ 50 cm$^{-3}$) illuminated by
radiation fields ~ 0.5-5 G0, while CH is associated with higher density of 500
cm$^{-3}$. We posit that the apparent dichotomy in the properties of the
diffuse clouds giving rise to OH and CH absorption could be due to either (a)
the presence of multiple spectroscopically unresolved clouds along the
line-of-sight, or, (b) density gradients along the line-of-sight within a
single cloud.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs**|Constantin Christof et.al.|[2306.05185v1](http://arxiv.org/abs/2306.05185v1)|null|We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean**|Spyros Kondylatos et.al.|[2306.05144v1](http://arxiv.org/abs/2306.05144v1)|[link](https://github.com/orion-ai-lab/mesogeos)|We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.|
|**2023-06-08**|**Partition functions of non-Lagrangian theories from the holomorphic anomaly**|Francesco Fucito et.al.|[2306.05141v1](http://arxiv.org/abs/2306.05141v1)|null|The computation of the partition function in certain quantum field theories,
such as those of the Argyres-Douglas or Minahan-Nemeschansky type, is
problematic due to the lack of a Lagrangian description. In this paper, we use
the holomorphic anomaly equation to derive the gravitational corrections to the
prepotential of such theories at rank one by deforming them from the conformal
point. In the conformal limit, we find a general formula for the partition
function as a sum of hypergeometric functions. We show explicit results for the
round sphere and the Nekrasov-Shatashvili phases of the $\Omega$ background.
The first case is relevant for the derivation of extremal correlators in flat
space, whereas the second one has interesting applications for the study of
anharmonic oscillators.|
|**2023-06-08**|**Octree-based hierarchical sampling optimization for the volumetric super-resolution of scientific data**|Xinjie Wang et.al.|[2306.05133v1](http://arxiv.org/abs/2306.05133v1)|null|When introducing physics-constrained deep learning solutions to the
volumetric super-resolution of scientific data, the training is challenging to
converge and always time-consuming. We propose a new hierarchical sampling
method based on octree to solve these difficulties. In our approach, scientific
data is preprocessed before training, and a hierarchical octree-based data
structure is built to guide sampling on the latent context grid. Each leaf node
in the octree corresponds to an indivisible subblock of the volumetric data.
The dimensions of the subblocks are different, making the number of sample
points in each randomly cropped training data block to be adaptive. We
reconstruct the octree at intervals according to loss distribution to perform
the multi-stage training. With the Rayleigh-B\'enard convection problem, we
deploy our method to state-of-the-art models. We constructed adequate
experiments to evaluate the training performance and model accuracy of our
method. Experiments indicate that our sampling optimization improves the
convergence performance of physics-constrained deep learning super-resolution
solutions. Furthermore, the sample points and training time are significantly
reduced with no drop in model accuracy. We also test our method in training
tasks of other deep neural networks, and the results show our sampling
optimization has extensive effectiveness and applicability. The code is
publicly available at https://github.com/xinjiewang/octree-based_sampling.|
|**2023-06-08**|**The Uniform Even Subgraph and Its Connection to Phase Transitions of Graphical Representations of the Ising Model**|Ulrik Thinggaard Hansen et.al.|[2306.05130v1](http://arxiv.org/abs/2306.05130v1)|null|The uniform even subgraph is intimately related to the Ising model, the
random-cluster model, the random current model and the loop $\mathrm{O}$(1)
model. In this paper, we first prove that the uniform even subgraph of
$\mathbb{Z}^d$ percolates for $d \geq 2$ using its characterisation as the Haar
measure on the group of even graphs. We then tighten the result by showing that
the loop $\mathrm{O}$(1) model on $\mathbb{Z}^d$ percolates for $d \geq 2$ on
some interval $(1-\varepsilon,1]$. Finally, our main theorem is that the loop
$\mathrm{O}$(1) model and random current models corresponding to a
supercritical Ising model are always at least critical, in the sense that their
two-point correlation functions decay at most polynomially and the expected
cluster sizes are infinite.|
|**2023-06-08**|**Focus for Free in Density-Based Counting**|Zenglin Shi et.al.|[2306.05129v1](http://arxiv.org/abs/2306.05129v1)|null|This work considers supervised learning to count from images and their
corresponding point annotations. Where density-based counting methods typically
use the point annotations only to create Gaussian-density maps, which act as
the supervision signal, the starting point of this work is that point
annotations have counting potential beyond density map generation. We introduce
two methods that repurpose the available point annotations to enhance counting
performance. The first is a counting-specific augmentation that leverages point
annotations to simulate occluded objects in both input and density images to
enhance the network's robustness to occlusions. The second method, foreground
distillation, generates foreground masks from the point annotations, from which
we train an auxiliary network on images with blacked-out backgrounds. By doing
so, it learns to extract foreground counting knowledge without interference
from the background. These methods can be seamlessly integrated with existing
counting advances and are adaptable to different loss functions. We demonstrate
complementary effects of the approaches, allowing us to achieve robust counting
results even in challenging scenarios such as background clutter, occlusion,
and varying crowd densities. Our proposed approach achieves strong counting
results on multiple datasets, including ShanghaiTech Part\_A and Part\_B,
UCF\_QNRF, JHU-Crowd++, and NWPU-Crowd.|
|**2023-06-08**|**On irregular states and Argyres-Douglas theories**|Francesco Fucito et.al.|[2306.05127v1](http://arxiv.org/abs/2306.05127v1)|null|Conformal theories of the Argyres-Douglas type are notoriously hard to study
given that they are isolated and strongly coupled thus lacking a lagrangian
description. In flat space, an exact description is provided by the
Seiberg-Witten theory. Turning on a $\Omega$-background makes the geometry
``quantum" and tractable only in the weak curvature limit. In this paper we use
the AGT correspondence to derive $\Omega$-exact formulae for the partition
function, in the nearby of monopole points where the dynamics is described by
irregular conformal blocks of the CFT. The results are checked against those
obtained by the recursion relations coming from a conformal anomaly in the
region where the two approaches overlap. The Nekrasov-Shatashvili limit is also
discussed. Finally, we comment on the existence of black holes in De Sitter
space whose low energy dynamics is described by an Argyres-Douglas theory.|
|**2023-06-08**|**Proof-theoretic Semantics for Intuitionistic Multiplicative Linear Logic**|Alexander V. Gheorghiu et.al.|[2306.05106v1](http://arxiv.org/abs/2306.05106v1)|null|This work is the first exploration of proof-theoretic semantics for a
substructural logic. It focuses on the base-extension semantics (B-eS) for
intuitionistic multiplicative linear logic (IMLL). The starting point is a
review of Sandqvist's B-eS for intuitionistic propositional logic (IPL), for
which we propose an alternative treatment of conjunction that takes the form of
the generalized elimination rule for the connective. The resulting semantics is
shown to be sound and complete. This motivates our main contribution, a B-eS
for IMLL, in which the definitions of the logical constants all take the form
of their elimination rule and for which soundness and completeness are
established.|
|**2023-06-08**|**Diagnosing quantum phase transition via holographic entanglement entropy at finite temperature**|Huajie Gong et.al.|[2306.05096v1](http://arxiv.org/abs/2306.05096v1)|null|We investigate the behavior of the holographic entanglement entropy (HEE) in
proximity to the quantum critical points (QCPs) of the metal-insulator
transition (MIT) in the Einstein-Maxwell-dilaton-axions (EMDA) model. Due to
the fact that the ground state entropy density of the EMDA model is vanishing
for insulating phase, but non-vanishing for the metallic phase, we used to
expect that it is the HEE itself that characterizes the QCPs. This expectation
is validated for certain case, however, we make a noteworthy observation: for a
specific scenario, it is not the HEE itself but rather the second-order
derivative of HEE with respect to the lattice wave number that effectively
characterizes the quantum phase transition (QPT). This distinction arises due
to the influence of thermal effects. These findings present novel insights into
the interplay between HEE and QPTs in the context of the MIT, and have
significant implications for studying QPT at finite temperatures.|
|**2023-06-08**|**FAST reveals new evidence for M94 as a merger**|Ruilei Zhou et.al.|[2306.05080v1](http://arxiv.org/abs/2306.05080v1)|null|We report the first high-sensitivity HI observation toward the spiral galaxy
M94 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). From
these observations, we discovered that M94 has a very extended HI disk, twice
larger than that observed by THINGS, which is accompanied by an HI filament and
seven HVCs (high velocity clouds) at different distances. The projected
distances of these clouds and filament are less than 50 kpc from the galactic
center. We measured a total integrated flux (including all clouds/filament) of
127.3 ($\pm$1) Jy km s$^{-1}$, corresponding to a H I mass of
(6.51$\pm$0.06)$\times$10$^{8}$M$_{\odot}$, which is 63.0% more than that
observed by THINGS. By comparing numerical simulations with the HI maps and the
optical morphology of M94, we suggest that M94 is likely a remnant of a major
merger of two galaxies, and the HVCs and HI filament could be the tidal
features originated from the first collision of the merger happened about 5 Gyr
ago. Furthermore, we found a seemingly isolated HI cloud at a projection
distance of 109 kpc without any optical counterpart detected. We discussed the
possibilities of the origin of this cloud, such as dark dwarf galaxy and RELHIC
(REionization-Limited HI Cloud). Our results demonstrate that high-sensitivity
and wide-field HI imaging is important in revealing the diffuse cold gas
structures and tidal debris which is crucial to understanding the dynamical
evolution of galaxies.|
|**2023-06-08**|**Resolving nonclassical magnon composition of a magnetic ground state via a qubit**|Anna-Luisa E. Rmling et.al.|[2306.05065v1](http://arxiv.org/abs/2306.05065v1)|null|Recently gained insights into equilibrium squeezing and entanglement harbored
by magnets point towards exciting opportunities for quantum science and
technology, while concrete protocols for exploiting these are needed. Here, we
theoretically demonstrate that a direct dispersive coupling between a qubit and
a noneigenmode magnon enables detecting the magnonic number states' quantum
superposition that forms the ground state of the actual eigenmode -
squeezed-magnon - via qubit excitation spectroscopy. Furthermore, this unique
coupling is found to enable control over the equilibrium magnon squeezing and a
deterministic generation of squeezed even Fock states via the qubit state and
its excitation. Our work demonstrates direct dispersive coupling to
noneigenmodes, realizable in spin systems, as a general pathway to exploiting
the equilibrium squeezing and related quantum properties thereby motivating a
search for similar realizations in other platforms.|
|**2023-06-08**|**On the central value of Rankin $L$-functions for self-dual algebraic representations of linear groups over totally real fields**|Laurent Clozel et.al.|[2306.05049v1](http://arxiv.org/abs/2306.05049v1)|null|Deligne has formulated extremely influential conjectures about certain
special values of the $L$-functions of (Grothendieck) motives over a number
field $F$. Given the conjectural dictionary between motives and 'algebraic'
automorphic representations of $\textrm{GL}(N, {\mathbb A}_F)$, where ${\mathbb
A}_F$ denotes the ad\`eles of $F$, they translate into conjectures concerning
the $L$-functions of these automorphic representations. These complex
representations, when they are 'regular', can be conjugated by the
automorphisms of the complex field ${\mathbb C}$. It then follows, as a weak
consequence of Deligne's conjectures, that the vanishing at critical points
(integers of half-integers) of the automorphic $L$-functions should be
invariant by automorphisms of ${\mathbb C}$. If $F$ is totally imaginary, this
has been proven by Moeglin, for standard or Rankin $L$-functions. Here we
extend the result to Rankin $L$-fuctions for totally real fields $F$, under a
parity and a regularity assumption. The proof relies on Eisenstein cohomology
and the Zucker conjecture (a theorem of Looijenga and Saper-Stern.)|

### Point Cloud Completion
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**Scalar curvature rigidity of degenerate warped product spaces**|Jinmin Wang et.al.|[2306.05413v1](http://arxiv.org/abs/2306.05413v1)|null|In this paper we prove the scalar curvature extremality and rigidity for a
class of warped product spaces that are possibly degenerate at the two ends.
The leaves of these warped product spaces can be any closed Riemannian manifold
with nonnegative curvature operator and nonvanishing Euler characteristic, flat
tori, round spheres and their direct products. In particular, we obtain the
scalar curvature extremality and rigidity for certain degenerate toric bands
and also for round spheres with two antipodal points removed. This answers the
corresponding questions of Gromov in all dimensions.|
|**2023-06-08**|**Quantum symmetries in 2+1 dimensions: Carroll, (a)dS-Carroll, Galilei and (a)dS-Galilei**|Tomasz Trzeniewski et.al.|[2306.05409v1](http://arxiv.org/abs/2306.05409v1)|null|There is a surge of research devoted to the formalism and physical
manifestations of non-Lorentzian kinematical symmetries, which focuses
especially on the ones associated with the Galilei and Carroll relativistic
limits (the speed of light taken to infinity or to zero, respectively). The
investigations has also been extended to quantum deformations of the Carrollian
and Galilean symmetries, in the sense of (quantum) Hopf algebras. The case of
2+1 dimensions is particularly worth to study due to both the mathematical
nature of the corresponding (classical) theory of gravity, and the recently
finalized classification of all quantum-deformed algebras of spacetime
isometries. Consequently, the list of all quantum deformations of (anti-)de
Sitter-Carroll algebra is immediately provided by its well-known isomorphism
with either Poincar\'{e} or Euclidean algebra. Quantum contractions from the
(anti-)de Sitter to (anti-)de Sitter-Carroll classification allow to almost
completely recover the latter. One may therefore conjecture that the analogous
contractions from the (anti-)de Sitter to (anti-)de Sitter-Galilei $r$-matrices
provide (almost) all coboundary deformations of (anti-)de Sitter-Galilei
algebra. This scheme is complemented by deriving (Carrollian and Galilean)
quantum contractions of deformations of Poincar\'{e} algebra, leading to
coboundary deformations of Carroll and Galilei algebras.|
|**2023-06-08**|**Fully Robust Federated Submodel Learning in a Distributed Storage System**|Zhusheng Wang et.al.|[2306.05402v1](http://arxiv.org/abs/2306.05402v1)|null|We consider the federated submodel learning (FSL) problem in a distributed
storage system. In the FSL framework, the full learning model at the server
side is divided into multiple submodels such that each selected client needs to
download only the required submodel(s) and upload the corresponding update(s)
in accordance with its local training data. The server comprises multiple
independent databases and the full model is stored across these databases. An
eavesdropper passively observes all the storage and listens to all the
communicated data, of its controlled databases, to gain knowledge about the
remote client data and the submodel information. In addition, a subset of
databases may fail, negatively affecting the FSL process, as FSL process may
take a non-negligible amount of time for large models. To resolve these two
issues together (i.e., security and database repair), we propose a novel coding
mechanism coined ramp secure regenerating coding (RSRC), to store the full
model in a distributed manner. Using our new RSRC method, the eavesdropper is
permitted to learn a controllable amount of submodel information for the sake
of reducing the communication and storage costs. Further, during the database
repair process, in the construction of the replacement database, the submodels
to be updated are stored in the form of their latest version from updating
clients, while the remaining submodels are obtained from the previous version
in other databases through routing clients. Our new RSRC-based distributed FSL
approach is constructed on top of our earlier two-database FSL scheme which
uses private set union (PSU). A complete one-round FSL process consists of
FSL-PSU phase, FSL-write phase and additional auxiliary phases. Our proposed
FSL scheme is also robust against database drop-outs, client drop-outs, client
late-arrivals and an active adversary controlling databases.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Implications of atmospheric non-detections for Trappist-1 inner planets on atmospheric retention prospects for outer planets**|Joshua Krissansen-Totton et.al.|[2306.05397v1](http://arxiv.org/abs/2306.05397v1)|null|JWST secondary eclipse observations of Trappist-1b seemingly disfavor
atmospheres >~1 bar since heat redistribution is expected to yield dayside
emission temperature below the ~500 K observed. Given the similar densities of
Trappist-1 planets, and the theoretical potential for atmospheric erosion
around late M-dwarfs, this observation might be assumed to imply substantial
atmospheres are also unlikely for the outer planets. However, the processes
governing atmosphere erosion and replenishment are fundamentally different for
inner and outer planets. Here, an atmosphere-interior evolution model is used
to show that an airless Trappist-1b (and c) only weakly constrains stellar
evolution, and that the odds of outer planets e and f retaining substantial
atmospheres remain largely unchanged. This is true even if the initial volatile
inventories of planets in the Trappist-1 system are highly correlated. The
reason for this result is that b and c sit unambiguously interior to the
runaway greenhouse limit, and so have potentially experienced ~8 Gyr of
XUV-driven hydrodynamic escape; complete atmospheric erosion in this
environment only weakly constrains stellar evolution and escape
parameterizations. In contrast, e and f reside within the habitable zone, and
likely experienced a comparatively short steam atmosphere during Trappist-1's
pre-main sequence, and consequently complete atmospheric erosion remains
unlikely across a broad swath of parameter space (e and f retain atmospheres in
~98% of model runs). Naturally, it is still possible that all Trappist-1
planets formed volatile-poor and are all airless today. But the airlessness of
b (and c) does not require this, and as such, JWST transit spectroscopy of e
and f remains the best near-term opportunity to characterize the atmospheres of
habitable zone terrestrial planets.|
|**2023-06-08**|**Picard and Brauer groups of $K(n)$-local spectra via profinite Galois descent**|Itamar Mor et.al.|[2306.05393v1](http://arxiv.org/abs/2306.05393v1)|null|Using the pro\'etale site, we construct models for the continuous actions of
the Morava stabiliser group on Morava E-theory, its $\infty$-category of
$K(n)$-local modules, and its Picard spectrum. For the two sheaves of spectra,
we evaluate the resulting descent spectral sequences: these can be thought of
as homotopy fixed point spectral sequences for the profinite Galois extension
$L_{K(n)} \mathbb S \to E_n$. We show that the descent spectral sequence for
the Morava E-theory sheaf is the $K(n)$-local $E_n$-Adams spectral sequence.
The spectral sequence for the sheaf of Picard spectra is closely related to one
recently defined by Heard; our formalism allows us to compare many
differentials with those in the $K(n)$-local $E_n$-Adams spectral sequence, and
isolate the exotic Picard elements in the $0$-stem. In particular, we show how
this recovers the computation due to Hopkins, Mahowald and Sadofsky of the
group $\mathrm{Pic}_1$ at all primes. We also use these methods to bound the
Brauer group of $K(n)$-local spectra, and compute this bound at height one.|
|**2023-06-08**|**Hybrid data-driven magnetofrictional and magnetohydrodynamic simulations of an eruptive solar active region**|A. Afanasyev et.al.|[2306.05388v1](http://arxiv.org/abs/2306.05388v1)|null|We present first results of the hybrid data-driven magnetofrictional (MF) and
data-constrained magnetohydrodynamic (MHD) simulations of solar active region
NOAA 11158, which produced an X-class flare and coronal mass ejection on 2011
February 15. First, we apply the MF approach to build the coronal magnetic
configuration corresponding to the SDO/HMI photospheric magnetograms by using
the JSOC PDFI SS electric field inversions at the bottom boundary of the
simulation domain. We then use the pre-eruptive MF state at about 1.5 hour
before the observed X-class flare as the initial state for the MHD simulation,
assuming a stratified polytropic solar corona. The MHD run shows that the
initial magnetic configuration containing twisted magnetic fluxes and a 3D
magnetic null point is out of equilibrium. We find the eruption of a complex
magnetic structure consisting of two magnetic flux ropes, as well as the
development of flare ribbons, with their morphology being in good agreement
with observations. We conclude that the combination of the data-driven MF and
data-constrained MHD simulations is a useful practical tool for understanding
the 3D magnetic structures of real solar ARs that are unobservable otherwise.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**Relativistic BGK model for gas mixtures**|Byung-Hoon Hwang et.al.|[2306.05349v1](http://arxiv.org/abs/2306.05349v1)|null|Unlike the case for classical particles, the literature on BGK type models
for relativistic gas mixture is extremely limited. There are a few results
%\cite{Kremer,Kremer3,KP} in which such relativistic BGK models for gas mixture
are employed to compute transport coefficients. However, to the best knowledge
of authors, relativistic BGK models for gas mixtures with complete presentation
of the relaxation operators are missing in the literature.
  In this paper, we fill this gap by suggesting a BGK model for relativistic
gas mixtures for which the existence of each equilibrium coefficients in the
relaxation operator is rigorously guaranteed in a way that all the essential
physical properties are satisfied such as the conservation laws, the H-theorem,
the capturing of the correct equilibrium state, the indifferentiability
principle, and the recovery of the classical BGK model in the Newtonian limit.|
|**2023-06-08**|**$$-left-exact and $$-regular copresheaves on $( ,)$-coherent categories**|Kristf Kanalas et.al.|[2306.05345v1](http://arxiv.org/abs/2306.05345v1)|null|$\kappa $-left-exact copresheaves on a $(\kappa ,\kappa )$-coherent category
$\mathcal{C}$ form a full reflective subcategory in $\mathcal{C}\downarrow
\mathbf{Coh}^{\sim }_{\kappa ,\kappa }$, the category of $(\kappa ,\kappa
)$-coherent functors out of $\mathcal{C}$. Given a $\kappa $-lex
$F:\mathcal{C}\to \mathbf{Set}$ the construction extends $\mathcal{C}$ by
adding constants along $F$.
  $\kappa $-regular copresheaves are identified with $(\kappa ,\kappa
)$-coherent functors to sheaves over $\kappa $-complete, $(\kappa ,\kappa
)$-distributive Boolean-algebras. As an application we prove an infinitary
version of the Heyting-valued completeness theorem, and characterise which
$\kappa $-filtered colimit preserving functors $\mathbf{Coh}_{\kappa ,\kappa
}(\mathcal{D},\mathbf{Set})\to \mathbf{Coh}_{\kappa ,\kappa
}(\mathcal{C},\mathbf{Set})$ are coming from an interpretation $\mathcal{C}\to
\mathcal{D}$.
  We investigate the functorial properties of two semiring invariants of lex
copresheaves over coherent categories.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**FCNC charmed-hadron decays with invisible singlet particles in light of recent data**|Geng Li et.al.|[2306.05333v1](http://arxiv.org/abs/2306.05333v1)|null|The flavor-changing neutral current (FCNC) decays of charmed hadrons with
missing energy ($\slashed E$) can serve as potentially promising hunting
grounds for hints of new physics, as the standard-model backgrounds are very
suppressed. A few of such processes have been searched for in recent
experiments, particularly $D^0\to\slashed E$ by Belle and $D^0\to\pi^0\slashed
E$ and $\Lambda_c^+\to p\slashed E$ by BESIII, resulting in upper bounds on
their branching fractions. We consider them to illuminate the possible
contributions of the quark transition $c\to u\slashed E$ with a couple of
invisible spinless bosons carrying away the missing energy, assuming that they
are not charge conjugates of each other and hence can have unequal masses. We
find that these data are complementary in that they constrain different sets of
the underlying operators and do not cover the same ranges of the bosons'
masses, but there are regions not yet accessible. From the allowed parameter
space, we show that other $D$-meson decays, such as $D\to\rho\slashed E$, and
the charmed-baryon ones $\Xi_c\to(\Sigma,\Lambda)\slashed E$ can have sizable
branching fractions and therefore may offer further probes of the new-physics
interactions. We point out the importance of $D^0\to\gamma\slashed E$ which are
not yet searched for but could access parts of the parameter space beyond the
reach of the other modes. In addition, we look at a scenario where the
invisibles are instead fermionic, namely sterile neutrinos, and a scalar
leptoquark mediates $c\to u\slashed E$. We discuss the implications of the
aforesaid bounds for this model. The predictions we make for the various
charmed-hadron decays in the different scenarios may be testable in the near
future by BESIII and Belle II.|
|**2023-06-08**|**Hybridizable discontinuous Galerkin methods for the Monge-Ampere equation**|Ngoc Cuong Nguyen et.al.|[2306.05296v1](http://arxiv.org/abs/2306.05296v1)|null|We introduce two hybridizable discontinuous Galerkin (HDG) methods for
numerically solving the Monge-Ampere equation. The first HDG method is devised
to solve the nonlinear elliptic Monge-Ampere equation by using Newton's method.
The second HDG method is devised to solve a sequence of the Poisson equation
until convergence to a fixed-point solution of the Monge-Ampere equation is
reached. Numerical examples are presented to demonstrate the convergence and
accuracy of the HDG methods. Furthermore, the HDG methods are applied to
r-adaptive mesh generation by redistributing a given scalar density function
via the optimal transport theory. This r-adaptivity methodology leads to the
Monge-Ampere equation with a nonlinear Neumann boundary condition arising from
the optimal transport of the density function to conform the resulting
high-order mesh to the boundary. Hence, we extend the HDG methods to treat the
nonlinear Neumann boundary condition. Numerical experiments are presented to
illustrate the generation of r-adaptive high-order meshes on planar and curved
domains.|
|**2023-06-08**|**Positive Geometries for Scattering Amplitudes in Momentum Space**|Robert Moerman et.al.|[2306.05287v1](http://arxiv.org/abs/2306.05287v1)|null|Positive geometries provide a purely geometric point of departure for
studying scattering amplitudes in quantum field theory. A positive geometry is
a specific semi-algebraic set equipped with a unique rational top form - the
canonical form. There are known examples where the canonical form of some
positive geometry, defined in some kinematic space, encodes a scattering
amplitude in some theory. Remarkably, the boundaries of the positive geometry
are in bijection with the physical singularities of the scattering amplitude.
The Amplituhedron, discovered by Arkani-Hamed and Trnka, is a prototypical
positive geometry. It lives in momentum twistor space and describes tree-level
(and the integrands of planar loop-level) scattering amplitudes in maximally
supersymmetric Yang-Mills theory.
  In this dissertation, we study three positive geometries defined in on-shell
momentum space: the Arkani-Hamed-Bai-He-Yan (ABHY) associahedron, the Momentum
Amplituhedron, and the orthogonal Momentum Amplituhedron. Each describes
tree-level scattering amplitudes for different theories in different spacetime
dimensions. The three positive geometries share a series of interrelations in
terms of their boundary posets and canonical forms. We review these
relationships in detail, highlighting the author's contributions. We study
their boundary posets, classifying all boundaries and hence all physical
singularities at the tree level. We develop new combinatorial results to derive
rank-generating functions which enumerate boundaries according to their
dimension. These generating functions allow us to prove that the Euler
characteristics of the three positive geometries are one. In addition, we
discuss methods for manipulating canonical forms using ideas from computational
algebraic geometry.|
|**2023-06-08**|**How to Incorporate Systematic Effects into Parameter Determination**|David van Dyk et.al.|[2306.05271v1](http://arxiv.org/abs/2306.05271v1)|null|We describe two different approaches for incorporating systematics into
analyses for parameter determination in the physical sciences. We refer to
these as the Pragmatic and the Full methods, with the latter coming in two
variants: Full Likelihood and Fully Bayesian. By the use of a simple and
readily understood example, we point out the advantage of using the Full
Likelihood and Fully Bayesian approaches; a more realistic example from
Astrophysics is also presented. This could be relevant for data analyses in a
wide range of scientific fields, for situations where systematic effects need
to be incorporated in the analysis procedure. This note is an extension of part
of the talk by van Dyk at the PHYSTAT-Systematics meeting.|
|**2023-06-08**|**Linking Frequentist and Bayesian Change-Point Methods**|David Ardia et.al.|[2306.05265v1](http://arxiv.org/abs/2306.05265v1)|null|We show that the two-stage minimum description length (MDL) criterion widely
used to estimate linear change-point (CP) models corresponds to the marginal
likelihood of a Bayesian model with a specific class of prior distributions.
This allows results from the frequentist and Bayesian paradigms to be bridged
together. Thanks to this link, one can rely on the consistency of the number
and locations of the estimated CPs and the computational efficiency of
frequentist methods, and obtain a probability of observing a CP at a given
time, compute model posterior probabilities, and select or combine CP methods
via Bayesian posteriors. Furthermore, we adapt several CP methods to take
advantage of the MDL probabilistic representation. Based on simulated data, we
show that the adapted CP methods can improve structural break detection
compared to state-of-the-art approaches. Finally, we empirically illustrate the
usefulness of combining CP detection methods when dealing with long time series
and forecasting.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Quantum computing algorithms for inverse problems on graphs and an NP-complete inverse problem**|Joonas Ilmavirta et.al.|[2306.05253v1](http://arxiv.org/abs/2306.05253v1)|null|We consider an inverse problem for a finite graph $(X,E)$ where we are given
a subset of vertices $B\subset X$ and the distances $d_{(X,E)}(b_1,b_2)$ of all
vertices $b_1,b_2\in B$. The distance of points $x_1,x_2\in X$ is defined as
the minimal number of edges needed to connect two vertices, so all edges have
length 1. The inverse problem is a discrete version of the boundary rigidity
problem in Riemannian geometry or the inverse travel time problem in
geophysics. We will show that this problem has unique solution under certain
conditions and develop quantum computing methods to solve it. We prove the
following uniqueness result: when $(X,E)$ is a tree and $B$ is the set of
leaves of the tree, the graph $(X,E)$ can be uniquely determined in the class
of all graphs having a fixed number of vertices. We present a quantum computing
algorithm which produces a graph $(X,E)$, or one of those, which has a given
number of vertices and the required distances between vertices in $B$. To this
end we develop an algorithm that takes in a qubit representation of a graph and
combine it with Grover's search algorithm. The algorithm can be implemented
using only $O(|X|^2)$ qubits, the same order as the number of elements in the
adjacency matrix of $(X,E)$. It also has a quadratic improvement in
computational cost compared to standard classical algorithms. Finally, we
consider applications in theory of computation, and show that a slight
modification of the above inverse problem is NP-complete: all NP-problems can
be reduced to a discrete inverse problem we consider.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Investigating the OH-H2 relation in diffuse Galactic clouds**|Katherine Rawlins et.al.|[2306.05213v1](http://arxiv.org/abs/2306.05213v1)|null|We investigate the correlation between OH and H2 column densities in diffuse
Galactic clouds, in order to identify potential molecular tracers of
interstellar H2. For this, we analyse near-UV spectra extracted from the
ESO/VLT archives towards seventeen sightlines (five of them new) with known
N(H2), along with nine sightlines with no H2 information. N(OH) shows only
marginal correlation with N(H2) (10$^{20}$ to 2 x 10$^{21}$ cm$^{-2}$), at the
95 per cent confidence level. We use orthogonal distance regression analysis to
obtain N(OH)/N(H2) = (1.32+/-0.15) x 10$^{-7}$, which is ~ 33 per cent higher
than the previous estimates based on near-UV data. We also obtain N(CH)/N(H2) =
(3.83+/-0.23) x 10$^{-8}$ and a significant correlation between N(OH) and
N(CH), with N(OH) = (2.61+/-0.19) x N(CH), both of which are consistent with
previous results. Comparison with predictions of numerical models indicate that
OH absorption arises from diffuse gas (nH ~ 50 cm$^{-3}$) illuminated by
radiation fields ~ 0.5-5 G0, while CH is associated with higher density of 500
cm$^{-3}$. We posit that the apparent dichotomy in the properties of the
diffuse clouds giving rise to OH and CH absorption could be due to either (a)
the presence of multiple spectroscopically unresolved clouds along the
line-of-sight, or, (b) density gradients along the line-of-sight within a
single cloud.|
|**2023-06-08**|**RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit**|Jiongnan Liu et.al.|[2306.05212v1](http://arxiv.org/abs/2306.05212v1)|[link](https://github.com/ruc-gsai/yulan-ir)|Although Large Language Models (LLMs) have demonstrated extraordinary
capabilities in many domains, they still have a tendency to hallucinate and
generate fictitious responses to user requests. This problem can be alleviated
by augmenting LLMs with information retrieval (IR) systems (also known as
retrieval-augmented LLMs). Applying this strategy, LLMs can generate more
factual texts in response to user input according to the relevant content
retrieved by IR systems from external corpora as references. In addition, by
incorporating external knowledge, retrieval-augmented LLMs can answer in-domain
questions that cannot be answered by solely relying on the world knowledge
stored in parameters. To support research in this area and facilitate the
development of retrieval-augmented LLM systems, we develop RETA-LLM, a
{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline
to help researchers and users build their customized in-domain LLM-based
systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM
provides more plug-and-play modules to support better interaction between IR
systems and LLMs, including {request rewriting, document retrieval, passage
extraction, answer generation, and fact checking} modules. Our toolkit is
publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.|
|**2023-06-08**|**The Qupit Stabiliser ZX-travaganza: Simplified Axioms, Normal Forms and Graph-Theoretic Simplification**|Boldizsr Por et.al.|[2306.05204v1](http://arxiv.org/abs/2306.05204v1)|null|We present a smorgasbord of results on the stabiliser ZX-calculus for odd
prime-dimensional qudits (i.e. qupits). We derive a simplified rule set that
closely resembles the original rules of qubit ZX-calculus. Using these rules,
we demonstrate analogues of the spider-removing local complementation and
pivoting rules. This allows for efficient reduction of diagrams to the affine
with phases normal form. We also demonstrate a reduction to a unique form,
providing an alternative and simpler proof of completeness. Furthermore, we
introduce a different reduction to the graph state with local Cliffords normal
form, which leads to a novel layered decomposition for qupit Clifford
unitaries. Additionally, we propose a new approach to handle scalars formally,
closely reflecting their practical usage. Finally, we have implemented many of
these findings in DiZX, a new open-source Python library for qudit
ZX-diagrammatic reasoning.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs**|Constantin Christof et.al.|[2306.05185v1](http://arxiv.org/abs/2306.05185v1)|null|We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean**|Spyros Kondylatos et.al.|[2306.05144v1](http://arxiv.org/abs/2306.05144v1)|[link](https://github.com/orion-ai-lab/mesogeos)|We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.|
|**2023-06-08**|**Partition functions of non-Lagrangian theories from the holomorphic anomaly**|Francesco Fucito et.al.|[2306.05141v1](http://arxiv.org/abs/2306.05141v1)|null|The computation of the partition function in certain quantum field theories,
such as those of the Argyres-Douglas or Minahan-Nemeschansky type, is
problematic due to the lack of a Lagrangian description. In this paper, we use
the holomorphic anomaly equation to derive the gravitational corrections to the
prepotential of such theories at rank one by deforming them from the conformal
point. In the conformal limit, we find a general formula for the partition
function as a sum of hypergeometric functions. We show explicit results for the
round sphere and the Nekrasov-Shatashvili phases of the $\Omega$ background.
The first case is relevant for the derivation of extremal correlators in flat
space, whereas the second one has interesting applications for the study of
anharmonic oscillators.|

### Image Matching
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Improving Negative-Prompt Inversion via Proximal Guidance**|Ligong Han et.al.|[2306.05414v1](http://arxiv.org/abs/2306.05414v1)|[link](https://github.com/phymhan/prompt-to-prompt)|DDIM inversion has revealed the remarkable potential of real image editing
within diffusion-based methods. However, the accuracy of DDIM reconstruction
degrades as larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align
the reconstruction and inversion trajectories with larger CFG scales, enabling
real image editing with cross-attention control. Negative-prompt inversion
(NPI) further offers a training-free closed-form solution of NTI. However, it
may introduce artifacts and is still constrained by DDIM reconstruction
quality. To overcome these limitations, we propose Proximal Negative-Prompt
Inversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI with
a regularization term and reconstruction guidance, which reduces artifacts
while capitalizing on its training-free nature. Our method provides an
efficient and straightforward approach, effectively addressing real image
editing tasks with minimal computational overhead.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Resonant Anti-Reflection Metasurface for Infrared Transmission Optics**|John Brewer et.al.|[2306.05405v1](http://arxiv.org/abs/2306.05405v1)|null|A fundamental capability for any transmissive optical component is
anti-reflection, yet this capability is challenging to achieve in a
cost-efficient manner over longer infrared wavelengths. We demonstrate that Mie
resonant nanophotonic structures enhance transmission in Silicon, allowing it
to function as an effective optical material over long-wave infrared
wavelengths. This approach enables a window optic with up to 40\% greater
transmission than equal thickness unpatterned Si. Imaging comparisons with
unpatterned silicon and off-the-shelf Germanium optics are shown, as well as
basic broadband slant edge MTF measurements. Overall, we demonstrate how
Mie-resonant structures can be used to improve optical transmission through
window optics of arbitrary lithographically patternable optical media, and
highlight their possible use in imaging applications.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Bayesian model calibration for diblock copolymer thin film self-assembly using power spectrum of microscopy data**|Lianghao Cao et.al.|[2306.05398v1](http://arxiv.org/abs/2306.05398v1)|null|Identifying parameters of computational models from experimental data, or
model calibration, is fundamental for assessing and improving the
predictability and reliability of computer simulations. In this work, we
propose a method for Bayesian calibration of models that predict morphological
patterns of diblock copolymer (Di-BCP) thin film self-assembly while accounting
for various sources of uncertainties in pattern formation and data acquisition.
This method extracts the azimuthally-averaged power spectrum (AAPS) of the
top-down microscopy characterization of Di-BCP thin film patterns as summary
statistics for Bayesian inference of model parameters via the pseudo-marginal
method. We derive the analytical and approximate form of a conditional
likelihood for the AAPS of image data. We demonstrate that AAPS-based image
data reduction retains the mutual information, particularly on important length
scales, between image data and model parameters while being relatively agnostic
to the aleatoric uncertainties associated with the random long-range disorder
of Di-BCP patterns. Additionally, we propose a phase-informed prior
distribution for Bayesian model calibration. Furthermore, reducing image data
to AAPS enables us to efficiently build surrogate models to accelerate the
proposed Bayesian model calibration procedure. We present the formulation and
training of two multi-layer perceptrons for approximating the
parameter-to-spectrum map, which enables fast integrated likelihood
evaluations. We validate the proposed Bayesian model calibration method through
numerical examples, for which the neural network surrogate delivers a fivefold
reduction of the number of model simulations performed for a single calibration
task.|
|**2023-06-08**|**Modular Visual Question Answering via Code Generation**|Sanjay Subramanian et.al.|[2306.05392v1](http://arxiv.org/abs/2306.05392v1)|[link](https://github.com/sanjayss34/codevqa)|We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**|Ge Yuan et.al.|[2306.05356v1](http://arxiv.org/abs/2306.05356v1)|[link](https://github.com/ygtxr1997/reliableswap)|Almost all advanced face swapping approaches use reconstruction as the proxy
task, i.e., supervision only exists when the target and source belong to the
same person. Otherwise, lacking pixel-level supervision, these methods struggle
for source identity preservation. This paper proposes to construct reliable
supervision, dubbed cycle triplets, which serves as the image-level guidance
when the source identity differs from the target one during training.
Specifically, we use face reenactment and blending techniques to synthesize the
swapped face from real images in advance, where the synthetic face preserves
source identity and target attributes. However, there may be some artifacts in
such a synthetic face. To avoid the potential artifacts and drive the
distribution of the network output close to the natural one, we reversely take
synthetic images as input while the real face as reliable supervision during
the training stage of face swapping. Besides, we empirically find that the
existing methods tend to lose lower-face details like face shape and mouth from
the source. This paper additionally designs a FixerNet, providing
discriminative embeddings of lower faces as an enhancement. Our face swapping
framework, named ReliableSwap, can boost the performance of any existing face
swapping network with negligible overhead. Extensive experiments demonstrate
the efficacy of our ReliableSwap, especially in identity preservation. The
project page is https://reliable-swap.github.io/.|
|**2023-06-08**|**The correlations between galaxy properties in different environments of the cosmic web**|Anindita Nandi et.al.|[2306.05354v1](http://arxiv.org/abs/2306.05354v1)|null|We study the correlations between galaxy properties in different environments
of the cosmic web using a volume limited sample from the SDSS. We determine the
geometric environment at the location of each galaxy using the eigenvalues of
the tidal tensor. The correlations are then separately analyzed in different
cosmic web environments. We use the Pearson correlation coefficient and the
normalized mutual information for measuring the correlations. Using a
two-tailed t-test, we find that the correlations between the galaxy properties
are sensitive to the geometric environments. The stellar mass can be an
important link between the galaxy properties and the environment. We repeat the
analysis after matching the stellar mass distributions in different
environments and find that the conclusions remain unchanged for most of the
relations. Our study suggests that the galaxy properties and their
interrelationships are susceptible to the geometric environments of the cosmic
web.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**A Review of the Recent Developments in the Fabrication Processes of CMOS Image Sensors for Smartphones**|Kirthika Nahalingam et.al.|[2306.05339v1](http://arxiv.org/abs/2306.05339v1)|null|CMOS Image Sensors are experiencing significant growth due to their
capabilities to be integrated in smartphones with refined image quality. One of
the major contributions to the growth of image sensors is the innovation
brought about in their fabrication processes. This paper presents a detailed
review of the different fabrication processes of the CMOS Image Sensors and its
impact on the image quality of smartphone pictures. Fabrication of CMOS image
sensors using wafer bonding technologies such as Through Silicon Vias and CuCu
hybrid bonding along with their experimental results are discussed. A 2 layer
architecture of photodiode and pixel transistors has adopted the 3D sequential
integration, by which the wafers are bonded together one after the other in the
fabrication process. Electrical characteristics and reliability test results
are presented for the former two fabrication processes and the improvements in
the pixels performance such as conversion gain, quantum efficiency, full well
capacity and dynamic range for the 2 layer architecture are discussed.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of
the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our
solver relies on a relaxation method in which convergence is sought as the
steady-state solution of a parabolic equation, whose time-discretization is
governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings
indicate that the RKL-based Poisson solver, which is both fully parallel and
rapidly convergent, has the potential to serve as a practical alternative to
conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and
\textit{successive over-relaxation} (SOR) methods. Additionally, it can
mitigate some of the drawbacks of these traditional techniques. We incorporate
our algorithm into a multigrid solver to provide a simple and efficient gravity
solver that can be used to obtain the gravitational potentials in
self-gravitational hydrodynamics. We test our implementation against a broad
range of standard self-gravitating astrophysical problems designed to examine
different aspects of the code. We demonstrate that the results match
excellently with the analytical predictions (when available), and the findings
of similar previous studies.|
|**2023-06-08**|**Actively learning a Bayesian matrix fusion model with deep side information**|Yangyang Yu et.al.|[2306.05331v1](http://arxiv.org/abs/2306.05331v1)|null|High-dimensional deep neural network representations of images and concepts
can be aligned to predict human annotations of diverse stimuli. However, such
alignment requires the costly collection of behavioral responses, such that, in
practice, the deep-feature spaces are only ever sparsely sampled. Here, we
propose an active learning approach to adaptively sampling experimental stimuli
to efficiently learn a Bayesian matrix factorization model with deep side
information. We observe a significant efficiency gain over a passive baseline.
Furthermore, with a sequential batched sampling strategy, the algorithm is
applicable not only to small datasets collected from traditional laboratory
experiments but also to settings where large-scale crowdsourced data collection
is needed to accurately align the high-dimensional deep feature representations
derived from pre-trained networks.|
|**2023-06-08**|**Federated Learning under Covariate Shifts with Generalization Guarantees**|Ali Ramezani-Kebrya et.al.|[2306.05325v1](http://arxiv.org/abs/2306.05325v1)|null|This paper addresses intra-client and inter-client covariate shifts in
federated learning (FL) with a focus on the overall generalization performance.
To handle covariate shifts, we formulate a new global model training paradigm
and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)
along with improving density ratio matching methods without requiring perfect
knowledge of the supremum over true ratios. We also propose the
communication-efficient variant FITW-ERM with the same level of privacy
guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM
achieves smaller generalization error than classical ERM under certain
settings. Experimental results demonstrate the superiority of FTW-ERM over
existing FL baselines in challenging imbalanced federated settings in terms of
data distribution shifts across clients.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**A physically motivated analytical expression for the temperature dependence of the zero-field splitting of the nitrogen-vacancy center in diamond**|M. C. Cambria et.al.|[2306.05318v1](http://arxiv.org/abs/2306.05318v1)|null|The temperature dependence of the zero-field splitting (ZFS) between the
$|m_{s}=0\rangle$ and $|m_{s}=\pm 1\rangle$ levels of the nitrogen-vacancy (NV)
center's electronic ground-state spin triplet can be used as a robust nanoscale
thermometer in a broad range of environments. However, despite numerous
measurements of this dependence in different temperature ranges, to our
knowledge no analytical expression has been put forward that captures the
scaling of the ZFS of the NV center across all relevant temperatures. Here we
present a simple, analytical, and physically motivated expression for the
temperature dependence of the NV center's ZFS that matches all experimental
observations, in which the ZFS shifts in proportion to the occupation numbers
of two representative phonon modes. In contrast to prior models our expression
does not diverge outside the regions of fitting. We show that our model
quantitatively matches experimental measurements of the ZFS from 15 to 500 K in
single NV centers in ultra-pure bulk diamond, and we compare our model and
measurements to prior models and experimental data.|
|**2023-06-08**|**A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments**|Guangyao Zheng et.al.|[2306.05310v1](http://arxiv.org/abs/2306.05310v1)|null|While Deep Reinforcement Learning has been widely researched in medical
imaging, the training and deployment of these models usually require powerful
GPUs. Since imaging environments evolve rapidly and can be generated by edge
devices, the algorithm is required to continually learn and adapt to changing
environments, and adjust to low-compute devices. To this end, we developed
three image coreset algorithms to compress and denoise medical images for
selective experience replayed-based lifelong reinforcement learning. We
implemented neighborhood averaging coreset, neighborhood sensitivity-based
sampling coreset, and maximum entropy coreset on full-body DIXON water and
DIXON fat MRI images. All three coresets produced 27x compression with
excellent performance in localizing five anatomical landmarks: left knee, right
trochanter, left kidney, spleen, and lung across both imaging environments.
Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$
average distance error, compared to the conventional lifelong learning
framework's $19.24\pm 50.77$.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis**|Gongshu Wang et.al.|[2306.05297v1](http://arxiv.org/abs/2306.05297v1)|null|Structural magnetic resonance imaging (sMRI) has shown great clinical value
and has been widely used in deep learning (DL) based computer-aided brain
disease diagnosis. Previous approaches focused on local shapes and textures in
sMRI that may be significant only within a particular domain. The learned
representations are likely to contain spurious information and have a poor
generalization ability in other diseases and datasets. To facilitate capturing
meaningful and robust features, it is necessary to first comprehensively
understand the intrinsic pattern of the brain that is not restricted within a
single data/task domain. Considering that the brain is a complex connectome of
interlinked neurons, the connectional properties in the brain have strong
biological significance, which is shared across multiple domains and covers
most pathological information. In this work, we propose a connectional style
contextual representation learning model (CS-CRL) to capture the intrinsic
pattern of the brain, used for multiple brain disease diagnosis. Specifically,
it has a vision transformer (ViT) encoder and leverages mask reconstruction as
the proxy task and Gram matrices to guide the representation of connectional
information. It facilitates the capture of global context and the aggregation
of features with biological plausibility. The results indicate that CS-CRL
achieves superior accuracy in multiple brain disease diagnosis tasks across six
datasets and three diseases and outperforms state-of-the-art models.
Furthermore, we demonstrate that CS-CRL captures more brain-network-like
properties, better aggregates features, is easier to optimize and is more
robust to noise, which explains its superiority in theory. Our source code will
be released soon.|

### Keypoint Detection
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**Gradient-Informed Quality Diversity for the Illumination of Discrete Spaces**|Raphael Boige et.al.|[2306.05138v1](http://arxiv.org/abs/2306.05138v1)|null|Quality Diversity (QD) algorithms have been proposed to search for a large
collection of both diverse and high-performing solutions instead of a single
set of local optima. While early QD algorithms view the objective and
descriptor functions as black-box functions, novel tools have been introduced
to use gradient information to accelerate the search and improve overall
performance of those algorithms over continuous input spaces. However a broad
range of applications involve discrete spaces, such as drug discovery or image
generation. Exploring those spaces is challenging as they are combinatorially
large and gradients cannot be used in the same manner as in continuous spaces.
We introduce map-elites with a Gradient-Informed Discrete Emitter (ME-GIDE),
which extends QD optimisation with differentiable functions over discrete
search spaces. ME-GIDE leverages the gradient information of the objective and
descriptor functions with respect to its discrete inputs to propose
gradient-informed updates that guide the search towards a diverse set of high
quality solutions. We evaluate our method on challenging benchmarks including
protein design and discrete latent space illumination and find that our method
outperforms state-of-the-art QD algorithms in all benchmarks.|
|**2023-06-07**|**3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels**|Zhenzhen Weng et.al.|[2306.04745v1](http://arxiv.org/abs/2306.04745v1)|null|Training a 3D human keypoint detector from point clouds in a supervised
manner requires large volumes of high quality labels. While it is relatively
easy to capture large amounts of human point clouds, annotating 3D keypoints is
expensive, subjective, error prone and especially difficult for long-tail cases
(pedestrians with rare poses, scooterists, etc.). In this work, we propose
GC-KPL - Geometry Consistency inspired Key Point Leaning, an approach for
learning 3D human joint locations from point clouds without human labels. We
achieve this by our novel unsupervised loss formulations that account for the
structure and movement of the human body. We show that by training on a large
training set from Waymo Open Dataset without any human annotated keypoints, we
are able to achieve reasonable performance as compared to the fully supervised
approach. Further, the backbone benefits from the unsupervised training and is
useful in downstream fewshot learning of keypoints, where fine-tuning on only
10 percent of the labeled training data gives comparable performance to
fine-tuning on the entire set. We demonstrated that GC-KPL outperforms by a
large margin over SoTA when trained on entire dataset and efficiently leverages
large volumes of unlabeled data.|
|**2023-06-07**|**Automatic graph representation algorithm for heterogeneous catalysis**|Zachary Gariepy et.al.|[2306.04742v1](http://arxiv.org/abs/2306.04742v1)|null|One of the most appealing aspects of machine learning for material design is
its high throughput exploration of chemical spaces, but to reach the ceiling of
ML-aided exploration, more than current model architectures and processing
algorithms are required. New architectures such as Graph Neural Networks (GNNs)
have seen significant research investments recently. For heterogeneous
catalysis, defining substrate intramolecular bonds and adsorbate/substrate
intermolecular bonds is a time-consuming and challenging process. Before
applying a model, dataset pre-processing, node/bond descriptor design, and
specific model constraints have to be considered. In this work, a framework
designed to solve these issues is presented in the form of an automatic graph
representation algorithm (AGRA) tool to extract the local chemical environment
of metallic surface adsorption sites is presented. This tool is able to gather
multiple adsorption geometry datasets composed of different systems and combine
them into a single model. To show AGRA's excellent transferability and reduced
computational cost compared to other graph representation methods, it was
applied to 5 different catalytic reaction datasets and benchmarked against the
Open Catalyst Projects (OCP) graph representation method. The two ORR datasets
with O/OH adsorbates obtained 0.053 eV RMSD when combined together, whereas the
three CO2RR datasets with CHO/CO/COOH obtained an average performance of 0.088
eV RMSD. To further display the algorithm's versatility and extrapolation
ability, a model was trained on a subset combination of all 5 datasets with an
RMSD of 0.105 eV. This universal model was then used to predict a wide range of
adsorption energies and an entirely new ORR catalyst system and then verified
through Density Functional Theory calculations|
|**2023-06-07**|**Machine Learning Universal Empirical Pseudopotentials**|Rokyeon Kim et.al.|[2306.04426v1](http://arxiv.org/abs/2306.04426v1)|null|Machine learning is used to generate empirical pseudopotentials that
characterize the local screened interactions in the Kohn-Sham Hamiltonian. Our
approach incorporates momentum-range-separated rotation-covariant descriptors
to capture crystal symmetries as well as crucial directional information of
bonds, thus realizing accurate descriptions of anisotropic solids. Trained
empirical potentials are shown to be versatile and transferable such that the
calculated energy bands and wave functions without cumbersome self-consistency
reproduce conventional ab initio results even for semiconductors with defects,
thus fostering faster and faithful data-driven materials researches.|
|**2023-06-07**|**Learning Probabilistic Coordinate Fields for Robust Correspondences**|Weiyue Zhao et.al.|[2306.04231v1](http://arxiv.org/abs/2306.04231v1)|null|We introduce Probabilistic Coordinate Fields (PCFs), a novel
geometric-invariant coordinate representation for image correspondence
problems. In contrast to standard Cartesian coordinates, PCFs encode
coordinates in correspondence-specific barycentric coordinate systems (BCS)
with affine invariance. To know \textit{when and where to trust} the encoded
coordinates, we implement PCFs in a probabilistic network termed PCF-Net, which
parameterizes the distribution of coordinate fields as Gaussian mixture models.
By jointly optimizing coordinate fields and their confidence conditioned on
dense flows, PCF-Net can work with various feature descriptors when quantifying
the reliability of PCFs by confidence maps. An interesting observation of this
work is that the learned confidence map converges to geometrically coherent and
semantically consistent regions, which facilitates robust coordinate
representation. By delivering the confident coordinates to keypoint/feature
descriptors, we show that PCF-Net can be used as a plug-in to existing
correspondence-dependent approaches. Extensive experiments on both indoor and
outdoor datasets suggest that accurate geometric invariant coordinates help to
achieve the state of the art in several correspondence problems, such as sparse
feature matching, dense image registration, camera pose estimation, and
consistency filtering. Further, the interpretable confidence map predicted by
PCF-Net can also be leveraged to other novel applications from texture transfer
to multi-homography classification.|
|**2023-06-06**|**Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures**|David Loiseaux et.al.|[2306.03801v1](http://arxiv.org/abs/2306.03801v1)|[link](https://github.com/davidlapous/multipers-signed-measure)|Persistent homology (PH) provides topological descriptors for geometric data,
such as weighted graphs, which are interpretable, stable to perturbations, and
invariant under, e.g., relabeling. Most applications of PH focus on the
one-parameter case -- where the descriptors summarize the changes in topology
of data as it is filtered by a single quantity of interest -- and there is now
a wide array of methods enabling the use of one-parameter PH descriptors in
data science, which rely on the stable vectorization of these descriptors as
elements of a Hilbert space. Although the multiparameter PH (MPH) of data that
is filtered by several quantities of interest encodes much richer information
than its one-parameter counterpart, the scarceness of stability results for MPH
descriptors has so far limited the available options for the stable
vectorization of MPH. In this paper, we aim to bring together the best of both
worlds by showing how the interpretation of signed barcodes -- a recent family
of MPH descriptors -- as signed measures leads to natural extensions of
vectorization strategies from one parameter to multiple parameters. The
resulting feature vectors are easy to define and to compute, and provably
stable. While, as a proof of concept, we focus on simple choices of signed
barcodes and vectorizations, we already see notable performance improvements
when comparing our feature vectors to state-of-the-art topology-based methods
on various types of data.|
|**2023-06-06**|**Scalable Concept Extraction in Industry 4.0**|Andrs Felipe Posada-Moreno et.al.|[2306.03551v1](http://arxiv.org/abs/2306.03551v1)|null|The industry 4.0 is leveraging digital technologies and machine learning
techniques to connect and optimize manufacturing processes. Central to this
idea is the ability to transform raw data into human understandable knowledge
for reliable data-driven decision-making. Convolutional Neural Networks (CNNs)
have been instrumental in processing image data, yet, their ``black box''
nature complicates the understanding of their prediction process. In this
context, recent advances in the field of eXplainable Artificial Intelligence
(XAI) have proposed the extraction and localization of concepts, or which
visual cues intervene on the prediction process of CNNs. This paper tackles the
application of concept extraction (CE) methods to industry 4.0 scenarios. To
this end, we modify a recently developed technique, ``Extracting Concepts with
Local Aggregated Descriptors'' (ECLAD), improving its scalability.
Specifically, we propose a novel procedure for calculating concept importance,
utilizing a wrapper function designed for CNNs. This process is aimed at
decreasing the number of times each image needs to be evaluated. Subsequently,
we demonstrate the potential of CE methods, by applying them in three
industrial use cases. We selected three representative use cases in the context
of quality control for material design (tailored textiles), manufacturing
(carbon fiber reinforcement), and maintenance (photovoltaic module inspection).
In these examples, CE was able to successfully extract and locate concepts
directly related to each task. This is, the visual cues related to each
concept, coincided with what human experts would use to perform the task
themselves, even when the visual cues were entangled between multiple classes.
Through empirical results, we show that CE can be applied for understanding
CNNs in an industrial context, giving useful insights that can relate to domain
knowledge.|
|**2023-06-06**|**SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving**|Honghao Fu et.al.|[2306.03538v1](http://arxiv.org/abs/2306.03538v1)|null|To mitigate the challenges arising from partial occlusion in human pose
keypoint based pedestrian detection methods , we present a novel pedestrian
pose keypoint completion method called the separation and dimensionality
reduction-based generative adversarial imputation networks (SDR-GAIN) .
Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we
isolate the head and torso keypoints of pedestrians with incomplete keypoints
due to occlusion or other factors and perform dimensionality reduction to
enhance features and further unify feature distribution. Finally, we introduce
two generative models based on the generative adversarial networks (GAN)
framework, which incorporate Huber loss, residual structure, and L1
regularization to generate missing parts of the incomplete head and torso pose
keypoints of partially occluded pedestrians, resulting in pose completion. Our
experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms
basic GAIN framework, interpolation methods PCHIP and MAkima, machine learning
methods k-NN and MissForest in terms of pose completion task. In addition, the
runtime of SDR-GAIN is approximately 0.4ms, displaying high real-time
performance and significant application value in the field of autonomous
driving.|
|**2023-06-05**|**Scene as Occupancy**|Wenwen Tong et.al.|[2306.02851v2](http://arxiv.org/abs/2306.02851v2)|[link](https://github.com/opendrivelab/occnet)|Human driver can easily describe the complex traffic scene by visual system.
Such an ability of precise perception is essential for driver's planning. To
achieve this, a geometry-aware representation that quantizes the physical 3D
scene into structured grid map with semantic labels per cell, termed as 3D
Occupancy, would be desirable. Compared to the form of bounding box, a key
insight behind occupancy is that it could capture the fine-grained details of
critical obstacles in the scene, and thereby facilitate subsequent tasks. Prior
or concurrent literature mainly concentrate on a single scene completion task,
where we might argue that the potential of this occupancy representation might
obsess broader impact. In this paper, we propose OccNet, a multi-view
vision-centric pipeline with a cascade and temporal voxel decoder to
reconstruct 3D occupancy. At the core of OccNet is a general occupancy
embedding to represent 3D physical world. Such a descriptor could be applied
towards a wide span of driving tasks, including detection, segmentation and
planning. To validate the effectiveness of this new representation and our
proposed algorithm, we propose OpenOcc, the first dense high-quality 3D
occupancy benchmark built on top of nuScenes. Empirical experiments show that
there are evident performance gain across multiple tasks, e.g., motion planning
could witness a collision rate reduction by 15%-58%, demonstrating the
superiority of our method.|
|**2023-06-05**|**A2B: Anchor to Barycentric Coordinate for Robust Correspondence**|Weiyue Zhao et.al.|[2306.02760v2](http://arxiv.org/abs/2306.02760v2)|null|There is a long-standing problem of repeated patterns in correspondence
problems, where mismatches frequently occur because of inherent ambiguity. The
unique position information associated with repeated patterns makes coordinate
representations a useful supplement to appearance representations for improving
feature correspondences. However, the issue of appropriate coordinate
representation has remained unresolved. In this study, we demonstrate that
geometric-invariant coordinate representations, such as barycentric
coordinates, can significantly reduce mismatches between features. The first
step is to establish a theoretical foundation for geometrically invariant
coordinates. We present a seed matching and filtering network (SMFNet) that
combines feature matching and consistency filtering with a coarse-to-fine
matching strategy in order to acquire reliable sparse correspondences. We then
introduce DEGREE, a novel anchor-to-barycentric (A2B) coordinate encoding
approach, which generates multiple affine-invariant correspondence coordinates
from paired images. DEGREE can be used as a plug-in with standard descriptors,
feature matchers, and consistency filters to improve the matching quality.
Extensive experiments in synthesized indoor and outdoor datasets demonstrate
that DEGREE alleviates the problem of repeated patterns and helps achieve
state-of-the-art performance. Furthermore, DEGREE also reports competitive
performance in the third Image Matching Challenge at CVPR 2021. This approach
offers a new perspective to alleviate the problem of repeated patterns and
emphasizes the importance of choosing coordinate representations for feature
correspondences.|
|**2023-06-04**|**Exploring Model Complexity in Machine Learned Potentials for Simulated Properties**|Andrew Rohskopf et.al.|[2306.02255v1](http://arxiv.org/abs/2306.02255v1)|null|Machine learning (ML) enables the development of interatomic potentials that
promise the accuracy of first principles methods while retaining the low cost
and parallel efficiency of empirical potentials. While ML potentials
traditionally use atom-centered descriptors as inputs, different models such as
linear regression and neural networks can map these descriptors to atomic
energies and forces. This begs the question: what is the improvement in
accuracy due to model complexity irrespective of choice of descriptors? We
curate three datasets to investigate this question in terms of ab initio energy
and force errors: (1) solid and liquid silicon, (2) gallium nitride, and (3)
the superionic conductor LGPS. We further investigate how these errors affect
simulated properties with these models and verify if the improvement in fitting
errors corresponds to measurable improvement in property prediction. Since
linear and nonlinear regression models have different advantages and
disadvantages, the results presented herein help researchers choose models for
their particular application. By assessing different models, we observe
correlations between fitting quantity (e.g. atomic force) error and simulated
property error with respect to ab initio values. Such observations can be
repeated by other researchers to determine the level of accuracy, and hence
model complexity, needed for their particular systems of interest.|
|**2023-06-03**|**LDEB -- Label Digitization with Emotion Binarization and Machine Learning for Emotion Recognition in Conversational Dialogues**|Amitabha Dey et.al.|[2306.02193v1](http://arxiv.org/abs/2306.02193v1)|null|Emotion recognition in conversations (ERC) is vital to the advancements of
conversational AI and its applications. Therefore, the development of an
automated ERC model using the concepts of machine learning (ML) would be
beneficial. However, the conversational dialogues present a unique problem
where each dialogue depicts nested emotions that entangle the association
between the emotional feature descriptors and emotion type (or label). This
entanglement that can be multiplied with the presence of data paucity is an
obstacle for a ML model. To overcome this problem, we proposed a novel approach
called Label Digitization with Emotion Binarization (LDEB) that disentangles
the twists by utilizing the text normalization and 7-bit digital encoding
techniques and constructs a meaningful feature space for a ML model to be
trained. We also utilized the publicly available dataset called the
FETA-DailyDialog dataset for feature learning and developed a hierarchical ERC
model using random forest (RF) and artificial neural network (ANN) classifiers.
Simulations showed that the ANN-based ERC model was able to predict emotion
with the best accuracy and precision scores of about 74% and 76%, respectively.
Simulations also showed that the ANN-model could reach a training accuracy
score of about 98% with 60 epochs. On the other hand, the RF-based ERC model
was able to predict emotions with the best accuracy and precision scores of
about 78% and 75%, respectively.|
|**2023-06-03**|**Context-TAP: Tracking Any Point Demands Spatial Context Features**|Weikang Bian et.al.|[2306.02000v1](http://arxiv.org/abs/2306.02000v1)|null|We tackle the problem of Tracking Any Point (TAP) in videos, which
specifically aims at estimating persistent long-term trajectories of query
points in videos. Previous methods attempted to estimate these trajectories
independently to incorporate longer image sequences, therefore, ignoring the
potential benefits of incorporating spatial context features. We argue that
independent video point tracking also demands spatial context features. To this
end, we propose a novel framework Context-TAP, which effectively improves point
trajectory accuracy by aggregating spatial context features in videos.
Context-TAP contains two main modules: 1) a SOurse Feature Enhancement (SOFE)
module, and 2) a TArget Feature Aggregation (TAFA) module. Context-TAP
significantly improves PIPs all-sided, reducing 11.4% Average Trajectory Error
of Occluded Points (ATE-Occ) on CroHD and increasing 11.8% Average Percentage
of Correct Keypoint (A-PCK) on TAP-Vid-Kinectics. Demos are available at this
$\href{https://wkbian.github.io/Projects/Context-TAP/}{webpage}$.|
|**2023-06-02**|**Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images**|Marcela Mera-Trujillo et.al.|[2306.01938v1](http://arxiv.org/abs/2306.01938v1)|null|Keypoint detection and matching is a fundamental task in many computer vision
problems, from shape reconstruction, to structure from motion, to AR/VR
applications and robotics. It is a well-studied problem with remarkable
successes such as SIFT, and more recent deep learning approaches. While great
robustness is exhibited by these techniques with respect to noise, illumination
variation, and rigid motion transformations, less attention has been placed on
image distortion sensitivity. In this work, we focus on the case when this is
caused by the geometry of the cameras used for image acquisition, and consider
the keypoint detection and matching problem between the hybrid scenario of a
fisheye and a projective image. We build on a state-of-the-art approach and
derive a self-supervised procedure that enables training an interest point
detector and descriptor network. We also collected two new datasets for
additional training and testing in this unexplored scenario, and we demonstrate
that current approaches are suboptimal because they are designed to work in
traditional projective conditions, while the proposed approach turns out to be
the most effective.|
|**2023-06-02**|**Unifying (Machine) Vision via Counterfactual World Modeling**|Daniel M. Bear et.al.|[2306.01828v1](http://arxiv.org/abs/2306.01828v1)|null|Leading approaches in machine vision employ different architectures for
different tasks, trained on costly task-specific labeled datasets. This
complexity has held back progress in areas, such as robotics, where robust
task-general perception remains a bottleneck. In contrast, "foundation models"
of natural language have shown how large pre-trained neural networks can
provide zero-shot solutions to a broad spectrum of apparently distinct tasks.
Here we introduce Counterfactual World Modeling (CWM), a framework for
constructing a visual foundation model: a unified, unsupervised network that
can be prompted to perform a wide variety of visual computations. CWM has two
key components, which resolve the core issues that have hindered application of
the foundation model concept to vision. The first is structured masking, a
generalization of masked prediction methods that encourages a prediction model
to capture the low-dimensional structure in visual data. The model thereby
factors the key physical components of a scene and exposes an interface to them
via small sets of visual tokens. This in turn enables CWM's second main idea --
counterfactual prompting -- the observation that many apparently distinct
visual representations can be computed, in a zero-shot manner, by comparing the
prediction model's output on real inputs versus slightly modified
("counterfactual") inputs. We show that CWM generates high-quality readouts on
real-world images and videos for a diversity of tasks, including estimation of
keypoints, optical flow, occlusions, object segments, and relative depth. Taken
together, our results show that CWM is a promising path to unifying the
manifold strands of machine vision in a conceptually simple foundation.|
|**2023-06-01**|**Labeled Interleaving Distance for Reeb Graphs**|Fangfei Lan et.al.|[2306.01186v1](http://arxiv.org/abs/2306.01186v1)|null|Merge trees, contour trees, and Reeb graphs are graph-based topological
descriptors that capture topological changes of (sub)level sets of scalar
fields. Comparing scalar fields using their topological descriptors has many
applications in topological data analysis and visualization of scientific data.
Recently, Munch and Stefanou introduced a labeled interleaving distance for
comparing two labeled merge trees, which enjoys a number of theoretical and
algorithmic properties. In particular, the labeled interleaving distance
between merge trees can be computed in polynomial time. In this work, we define
the labeled interleaving distance for labeled Reeb graphs. We then prove that
the (ordinary) interleaving distance between Reeb graphs equals the minimum of
the labeled interleaving distance over all labelings. We also provide an
efficient algorithm for computing the labeled interleaving distance between two
labeled contour trees (which are special types of Reeb graphs that arise from
simply-connected domains). In the case of merge trees, the notion of the
labeled interleaving distance was used by Gasparovic et al. to prove that the
(ordinary) interleaving distance on the set of (unlabeled) merge trees is
intrinsic. As our final contribution, we present counterexamples showing that,
on the contrary, the (ordinary) interleaving distance on (unlabeled) Reeb
graphs (and contour trees) is not intrinsic. It turns out that, under mild
conditions on the labelings, the labeled interleaving distance is a metric on
isomorphism classes of Reeb graphs, analogous to the ordinary interleaving
distance. This provides new metrics on large classes of Reeb graphs.|
|**2023-06-01**|**Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints**|Jiachen Li et.al.|[2306.01075v1](http://arxiv.org/abs/2306.01075v1)|null|Accurate understanding and prediction of human behaviors are critical
prerequisites for autonomous vehicles, especially in highly dynamic and
interactive scenarios such as intersections in dense urban areas. In this work,
we aim at identifying crossing pedestrians and predicting their future
trajectories. To achieve these goals, we not only need the context information
of road geometry and other traffic participants but also need fine-grained
information of the human pose, motion and activity, which can be inferred from
human keypoints. In this paper, we propose a novel multi-task learning
framework for pedestrian crossing action recognition and trajectory prediction,
which utilizes 3D human keypoints extracted from raw sensor data to capture
rich information on human pose and activity. Moreover, we propose to apply two
auxiliary tasks and contrastive learning to enable auxiliary supervisions to
improve the learned keypoints representation, which further enhances the
performance of major tasks. We validate our approach on a large-scale in-house
dataset, as well as a public benchmark dataset, and show that our approach
achieves state-of-the-art performance on a wide range of evaluation metrics.
The effectiveness of each model component is validated in a detailed ablation
study.|
|**2023-06-01**|**A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm**|Onur Beker et.al.|[2306.00892v1](http://arxiv.org/abs/2306.00892v1)|null|Existing object pose estimation methods commonly require a one-to-one point
matching step that forces them to be separated into two consecutive stages:
visual correspondence detection (e.g., by matching feature descriptors as part
of a perception front-end) followed by geometric alignment (e.g., by optimizing
a robust estimation objective for pointcloud registration or
perspective-n-point). Instead, we propose a matching-free probabilistic
formulation with two main benefits: i) it enables unified and concurrent
optimization of both visual correspondence and geometric alignment, and ii) it
can represent different plausible modes of the entire distribution of likely
poses. This in turn allows for a more graceful treatment of geometric
perception scenarios where establishing one-to-one matches between points is
conceptually ill-defined, such as textureless, symmetrical and/or occluded
objects and scenes where the correct pose is uncertain or there are multiple
equally valid solutions.|
|**2023-05-31**|**The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture**|Amanda A. Boatswain Jacques et.al.|[2306.00114v2](http://arxiv.org/abs/2306.00114v2)|null|Monitoring land cover using remote sensing is vital for studying
environmental changes and ensuring global food security through crop yield
forecasting. Specifically, multitemporal remote sensing imagery provides
relevant information about the dynamics of a scene, which has proven to lead to
better land cover classification results. Nevertheless, few studies have
benefited from high spatial and temporal resolution data due to the difficulty
of accessing reliable, fine-grained and high-quality annotated samples to
support their hypotheses. Therefore, we introduce a temporal patch-based
dataset of Canadian croplands, enriched with labels retrieved from the Canadian
Annual Crop Inventory. The dataset contains 78,536 manually verified
high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop
classes collected over four crop production years (2017-2020) and five months
(June-October). Each instance contains 12 spectral bands, an RGB image, and
additional vegetation index bands. Individually, each category contains at
least 4,800 images. Moreover, as a benchmark, we provide models and source code
that allow a user to predict the crop class using a single image (ResNet,
DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the same
location. In perspective, we expect this evolving dataset to propel the
creation of robust agro-environmental models that can accelerate the
comprehension of complex agricultural regions by providing accurate and
continuous monitoring of land cover.|
|**2023-05-31**|**Atom-by-atom design of metal oxide catalysts for the oxygen evolution reaction with machine learning**|Jaclyn R. Lunger et.al.|[2305.19930v1](http://arxiv.org/abs/2305.19930v1)|[link](https://github.com/learningmatter-mit/atom_by_atom)|Green hydrogen production is crucial for a sustainable future, but current
catalysts for the oxygen evolution reaction (OER) suffer from slow kinetics,
despite many efforts to produce optimal designs, particularly through the
calculation of descriptors for activity. In this study, we develop a dataset of
density functional theory calculations of bulk and surface perovskite oxides,
and adsorption energies of OER intermediates, which includes compositions up to
quaternary and facets up to (555). We demonstrate that per-site properties of
perovskite oxides such as Bader charge or band center can be tuned through
element substitution and faceting, and develop a machine learning model that
accurately predicts these properties directly from the local chemical
environment. We leverage these per-site properties to identify promising
perovskites with high theoretical OER activity. The identified design
principles and promising new materials provide a roadmap for closing the gap
between current artificial catalysts and biological enzymes.|
|**2023-05-31**|**Combining first-principles modeling and symbolic regression for designing efficient single-atom catalysts in Oxygen Evolution Reaction on Mo$_2$CO$_2$ MXenes**|Swetarekha Ram et.al.|[2305.19551v2](http://arxiv.org/abs/2305.19551v2)|null|In this study, we address the significant challenge of overcoming limitations
in catalytic efficiency for the oxygen evolution reaction (OER). The current
linear scaling relationships hinder the optimization of electrocatalytic
performance. To tackle this issue, we investigate the potential of designing
single-atom catalysts (SACs) on Mo$_2$CO$_2$ MXenes for electrochemical OER
using first-principles modeling simulations. By employing the Electrochemical
Step Symmetry Index (ESSI) method, we assess OER intermediates to fine-tune
activity and identify the optimal SAC for Mo$_2$CO$_2$ MXenes. Our findings
reveal that both Ag and Cu exhibit effectiveness as single atoms for enhancing
OER activity on Mo$_2$CO$_2$ MXenes. However, among the 21 chosen transition
metals (TMs) in this study, Cu stands out as the best catalyst for tweaking the
overpotential ($\eta_{OER}$). This is due to Cu's lowest overpotential compared
to other TMs, which makes it more favorable for OER performance. On the other
hand, Ag is closely aligned with ESSI=$\eta_{OER}$, making the tuning of its
overpotential more challenging. Furthermore, we employ symbolic regression
analysis to identify the significant factors that exhibit a correlation with
the OER overpotential. By utilizing this approach, we derive mathematical
formulas for the overpotential and identify key descriptors that affect
catalytic efficiency in electrochemical OER on Mo$_2$CO$_2$ MXenes. This
comprehensive investigation not only sheds light on the potential of MXenes in
advanced electrocatalytic processes but also highlights the prospect of
improved activity and selectivity in OER applications.|
|**2023-05-31**|**Learning by Aligning 2D Skeleton Sequences in Time**|Quoc-Huy Tran et.al.|[2305.19480v1](http://arxiv.org/abs/2305.19480v1)|null|This paper presents a novel self-supervised temporal video alignment
framework which is useful for several fine-grained human activity understanding
applications. In contrast with the state-of-the-art method of CASA, where
sequences of 3D skeleton coordinates are taken directly as input, our key idea
is to use sequences of 2D skeleton heatmaps as input. Given 2D skeleton
heatmaps, we utilize a video transformer which performs self-attention in the
spatial and temporal domains for extracting effective spatiotemporal and
contextual features. In addition, we introduce simple heatmap augmentation
techniques based on 2D skeletons for self-supervised learning. Despite the lack
of 3D information, our approach achieves not only higher accuracy but also
better robustness against missing and noisy keypoints than CASA. Extensive
evaluations on three public datasets, i.e., Penn Action, IKEA ASM, and H2O,
demonstrate that our approach outperforms previous methods in different
fine-grained human activity understanding tasks, i.e., phase classification,
phase progression, video alignment, and fine-grained frame retrieval.|
|**2023-05-30**|**Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training**|Wenshuo Chen et.al.|[2305.18743v2](http://arxiv.org/abs/2305.18743v2)|null|Estimating human pose from video is a task that receives considerable
attention due to its applicability in numerous 3D fields. The complexity of
prior knowledge of human body movements poses a challenge to neural network
models in the task of regressing keypoints. In this paper, we address this
problem by incorporating motion prior in an adversarial way. Different from
previous methods, we propose to decompose holistic motion prior to joint motion
prior, making it easier for neural networks to learn from prior knowledge
thereby boosting the performance on the task. We also utilize a novel
regularization loss to balance accuracy and smoothness introduced by motion
prior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration error
than previous methods tested on 3DPW. The estimator proves its robustness by
achieving impressive performance on in-the-wild dataset.|
|**2023-05-30**|**Align, Perturb and Decouple: Toward Better Leverage of Difference Information for RSI Change Detection**|Supeng Wang et.al.|[2305.18714v1](http://arxiv.org/abs/2305.18714v1)|[link](https://github.com/wangsp1999/cd-research)|Change detection is a widely adopted technique in remote sense imagery (RSI)
analysis in the discovery of long-term geomorphic evolution. To highlight the
areas of semantic changes, previous effort mostly pays attention to learning
representative feature descriptors of a single image, while the difference
information is either modeled with simple difference operations or implicitly
embedded via feature interactions. Nevertheless, such difference modeling can
be noisy since it suffers from non-semantic changes and lacks explicit guidance
from image content or context. In this paper, we revisit the importance of
feature difference for change detection in RSI, and propose a series of
operations to fully exploit the difference information: Alignment, Perturbation
and Decoupling (APD). Firstly, alignment leverages contextual similarity to
compensate for the non-semantic difference in feature space. Next, a difference
module trained with semantic-wise perturbation is adopted to learn more
generalized change estimators, which reversely bootstraps feature extraction
and prediction. Finally, a decoupled dual-decoder structure is designed to
predict semantic changes in both content-aware and content-agnostic manners.
Extensive experiments are conducted on benchmarks of LEVIR-CD, WHU-CD and
DSIFN-CD, demonstrating our proposed operations bring significant improvement
and achieve competitive results under similar comparative conditions. Code is
available at https://github.com/wangsp1999/CD-Research/tree/main/openAPD|
|**2023-05-29**|**Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance**|Supriya Gadi Patil et.al.|[2305.18557v1](http://arxiv.org/abs/2305.18557v1)|null|This paper analyzes the robustness of recent 3D shape descriptors to SO(3)
rotations, something that is fundamental to shape modeling. Specifically, we
formulate the task of rotated 3D object instance detection. To do so, we
consider a database of 3D indoor scenes, where objects occur in different
orientations. We benchmark different methods for feature extraction and
classification in the context of this task. We systematically contrast
different choices in a variety of experimental settings investigating the
impact on the performance of different rotation distributions, different
degrees of partial observations on the object, and the different levels of
difficulty of negative pairs. Our study, on a synthetic dataset of 3D scenes
where objects instances occur in different orientations, reveals that deep
learning-based rotation invariant methods are effective for relatively easy
settings with easy-to-distinguish pairs. However, their performance decreases
significantly when the difference in rotations on the input pair is large, or
when the degree of observation of input objects is reduced, or the difficulty
level of input pair is increased. Finally, we connect feature encodings
designed for rotation-invariant methods to 3D geometry that enable them to
acquire the property of rotation invariance.|
|**2023-05-29**|**Human Body Shape Classification Based on a Single Image**|Cameron Trotter et.al.|[2305.18480v1](http://arxiv.org/abs/2305.18480v1)|null|There is high demand for online fashion recommender systems that incorporate
the needs of the consumer's body shape. As such, we present a methodology to
classify human body shape from a single image. This is achieved through the use
of instance segmentation and keypoint estimation models, trained only on
open-source benchmarking datasets. The system is capable of performing in noisy
environments owing to to robust background subtraction. The proposed
methodology does not require 3D body recreation as a result of classification
based on estimated keypoints, nor requires historical information about a user
to operate - calculating all required measurements at the point of use. We
evaluate our methodology both qualitatively against existing body shape
classifiers and quantitatively against a novel dataset of images, which we
provide for use to the community. The resultant body shape classification can
be utilised in a variety of downstream tasks, such as input to size and fit
recommendation or virtual try-on systems.|
|**2023-05-29**|**TReR: A Lightweight Transformer Re-Ranking Approach for 3D LiDAR Place Recognition**|Tiago Barros et.al.|[2305.18013v1](http://arxiv.org/abs/2305.18013v1)|null|Autonomous driving systems often require reliable loop closure detection to
guarantee reduced localization drift. Recently, 3D LiDAR-based localization
methods have used retrieval-based place recognition to find revisited places
efficiently. However, when deployed in challenging real-world scenarios, the
place recognition models become more complex, which comes at the cost of high
computational demand. This work tackles this problem from an
information-retrieval perspective, adopting a first-retrieve-then-re-ranking
paradigm, where an initial loop candidate ranking, generated from a 3D place
recognition model, is re-ordered by a proposed lightweight transformer-based
re-ranking approach (TReR). The proposed approach relies on global descriptors
only, being agnostic to the place recognition model. The experimental
evaluation, conducted on the KITTI Odometry dataset, where we compared TReR
with s.o.t.a. re-ranking approaches such as alphaQE and SGV, indicate the
robustness and efficiency when compared to alphaQE while offering a good
trade-off between robustness and efficiency when compared to SGV.|
|**2023-05-29**|**Deep Electron Cloud-activity and Field-activity Relationships**|Lu Xu et.al.|[2305.17958v1](http://arxiv.org/abs/2305.17958v1)|null|Chemists have been pursuing the general mathematical laws to explain and
predict molecular properties for a long time. However, most of the traditional
quantitative structure-activity relationship (QSAR) models have limited
application domains, e.g., they tend to have poor generalization performance
when applied to molecules with parent structures different from those of the
trained molecules. This paper attempts to develop a new QSAR method that is
theoretically possible to predict various properties of molecules with diverse
structures. The proposed deep electron cloud-activity relationships (DECAR) and
deep field-activity relationships (DFAR) methods consist of three essentials:
(1) A large number of molecule entities with activity data as training objects
and responses; (2) three-dimensional electron cloud density (ECD) or related
field data by the accurate density functional theory methods as input
descriptors; (3) a deep learning model that is sufficiently flexible and
powerful to learn the large data described above. DECAR and DFAR are used to
distinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data
augmentation) and the classification performance is demonstrated to be
significantly better than the traditional least squares support vector machine
(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a
possible way to establish a widely applicable, cumulative, and shareable
artificial intelligence-driven QSAR system. They are likely to promote the
development of an interactive platform to collect and share the accurate ECD
and field data of millions of molecules with annotated activities. With enough
input data, we envision the appearance of several deep networks trained for
various molecular activities. Finally, we could anticipate a single DECAR or
DFAR network to learn and infer various properties of interest for chemical
molecules.|
|**2023-05-27**|**Text-to-image Editing by Image Information Removal**|Zhongping Zhang et.al.|[2305.17489v1](http://arxiv.org/abs/2305.17489v1)|null|Diffusion models have demonstrated impressive performance in text-guided
image generation. To leverage the knowledge of text-guided image generation
models in image editing, current approaches either fine-tune the pretrained
models using the input image (e.g., Imagic) or incorporate structure
information as additional constraints into the pretrained models (e.g.,
ControlNet). However, fine-tuning large-scale diffusion models on a single
image can lead to severe overfitting issues and lengthy inference time. The
information leakage from pretrained models makes it challenging to preserve the
text-irrelevant content of the input image while generating new features guided
by language descriptions. On the other hand, methods that incorporate
structural guidance (e.g., edge maps, semantic maps, keypoints) as additional
constraints face limitations in preserving other attributes of the original
image, such as colors or textures. A straightforward way to incorporate the
original image is to directly use it as an additional control. However, since
image editing methods are typically trained on the image reconstruction task,
the incorporation can lead to the identical mapping issue, where the model
learns to output an image identical to the input, resulting in limited editing
capabilities. To address these challenges, we propose a text-to-image editing
model with Image Information Removal module (IIR) to selectively erase
color-related and texture-related information from the original image, allowing
us to better preserve the text-irrelevant content and avoid the identical
mapping issue. We evaluate our model on three benchmark datasets: CUB, Outdoor
Scenes, and COCO. Our approach achieves the best editability-fidelity
trade-off, and our edited images are approximately 35% more preferred by
annotators than the prior-arts on COCO.|

## Computer Vision

### Object Tracking
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**A ship-in-a-bottle quantum gas microscope for magnetic mixtures**|Maximilian Sohmen et.al.|[2306.05404v1](http://arxiv.org/abs/2306.05404v1)|null|Quantum gas microscopes are versatile and powerful tools for fundamental
science as well as promising candidates for enticing applications such as in
quantum simulation or quantum computation. Here we present a quantum gas
microscopy setup for experiments with highly magnetic atoms of the lanthanoid
elements erbium and dysprosium. Our setup features a non-magnetic,
non-conducting, large-working-distance, high-numerical-aperture, in-vacuum
microscope objective, mounted inside a glue-free quartz glass cell. The quartz
glass cell is enclosed by a compact multi-shell ferromagnetic shield that
passively suppresses external magnetic field noise by a factor of more than a
thousand. Our setup will enable direct manipulation and probing of the rich
quantum many-body physics of dipolar atoms in optical lattices, and bears the
potential to put exciting theory proposals -- including exotic magnetic phases
and quantum phase transitions -- to an experimental test.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**An inflation model for massive primordial black holes to interpret the JWST observations**|Bing-Yu Su et.al.|[2306.05364v1](http://arxiv.org/abs/2306.05364v1)|null|The first observations of the James Webb Space Telescope (JWST) have
identified six massive galaxy candidates with the stellar masses $M_\ast\gtrsim
10^{10}\,M_\odot$ at high redshifts $7.4\lesssim z\lesssim 9.1$, with two most
massive high-$z$ objects having the cumulative comoving number densities
$n_{\rm G}$ up to $1.6\times 10^{-5}\, {\rm Mpc}^{-3}$. The presence of such
massive sources in the early universe challenges the standard $\Lambda$CDM
model since the needed star formation efficiency is unrealistically high. This
tension can be alleviated via the accretion of massive primordial black holes
(PBHs). In this work, with the updated data from the first JWST observations,
we find that the PBHs with mass $10^8\,M_\odot\lesssim M_{\rm PBH}\lesssim
10^{11}\,M_\odot$ can act as the seeds of extremely massive galaxies even with
a low abundance $10^{-7}\lesssim f_{\rm PBH}\lesssim 10^{-3}$. We construct an
ultraslow-roll inflation model and investigate its possibility of producing the
required PBHs. We explore the model in two cases, depending on whether there is
a perfect plateau on the inflaton potential. If the plateau is allowed to
incline slightly, our model can produce the PBHs that cover the required PBH
mass and abundance range to explain the JWST data.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models**|Tiantian Feng et.al.|[2306.05350v1](http://arxiv.org/abs/2306.05350v1)|null|Many recent studies have focused on fine-tuning pre-trained models for speech
emotion recognition (SER), resulting in promising performance compared to
traditional methods that rely largely on low-level, knowledge-inspired acoustic
features. These pre-trained speech models learn general-purpose speech
representations using self-supervised or weakly-supervised learning objectives
from large-scale datasets. Despite the significant advances made in SER through
the use of pre-trained architecture, fine-tuning these large pre-trained models
for different datasets requires saving copies of entire weight parameters,
rendering them impractical to deploy in real-world settings. As an alternative,
this work explores parameter-efficient fine-tuning (PEFT) approaches for
adapting pre-trained speech models for emotion recognition. Specifically, we
evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa
(Low-rank approximation) on four popular SER testbeds. Our results reveal that
LoRa achieves the best fine-tuning performance in emotion recognition while
enhancing fairness and requiring only a minimal extra amount of weight
parameters. Furthermore, our findings offer novel insights into future research
directions in SER, distinct from existing approaches focusing on directly
fine-tuning the model architecture. Our code is publicly available under:
https://github.com/usc-sail/peft-ser.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and
neutron star mergers are obtained from nuclear matter dynamical structure
functions that encode many-body effects from nuclear mean fields and
correlations. We employ nuclear interactions from chiral effective field theory
to calculate the density, spin, isospin, and spin-isospin response functions of
warm beta-equilibrium nuclear matter. We include corrections to the
single-particle energies in the mean field approximation as well as vertex
corrections resummed in the random phase approximation (RPA), including, for
the first time, both direct and exchange diagrams. We find that correlations
included through the RPA redistribute the strength of the response to higher
energy for neutrino absorption and lower energy for antineutrino absorption.
This tends to suppress the absorption rate of electron neutrinos across all
relevant energy scales. In contrast, the inclusion of RPA correlations enhances
the electron antineutrino absorption rate at low energy and supresses the rate
at high energy. These effects are especially important at high-density and in
the vicinity of the neutrino decoupling region. Implications for heavy element
nucleosynthesis, electromagnetic signatures of compact object mergers,
supernova dynamics, and neutrino detection from galactic supernovae are
discussed briefly.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth**|Zelin Liu et.al.|[2306.05238v1](http://arxiv.org/abs/2306.05238v1)|null|Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.|
|**2023-06-08**|**Global Stabilization of Antipodal Points on n-Sphere with Application to Attitude Tracking**|Xin Tong et.al.|[2306.05234v1](http://arxiv.org/abs/2306.05234v1)|null|Existing approaches to robust global asymptotic stabilization of a pair of
antipodal points on unit $n$-sphere $\mathbb{S}^n$ typically involve the
non-centrally synergistic hybrid controllers for attitude tracking on unit
quaternion space. However, when switching faults occur due to parameter errors,
the non-centrally synergistic property can lead to the unwinding problem or in
some cases, destabilize the desired set. In this work, a hybrid controller is
first proposed based on a novel centrally synergistic family of potential
functions on $\mathbb{S}^n$, which is generated from a basic potential function
through angular warping. The synergistic parameter can be explicitly expressed
if the warping angle has a positive lower bound at the undesired critical
points of the family. Next, the proposed approach induces a new
quaternion-based controller for global attitude tracking. It has three
advantageous features over existing synergistic designs: 1) it is consistent,
i.e., free from the ambiguity of unit quaternion representation; 2) it is
switching-fault-tolerant, i.e., the desired closed-loop equilibria remain
asymptotically stable even when the switching mechanism does not work; 3) it
relaxes the assumption on the parameter of the basic potential function in
literature. Comprehensive simulation confirms the high robustness of the
proposed centrally synergistic approach compared with existing non-centrally
synergistic approaches.|
|**2023-06-08**|**Boosting Adversarial Transferability by Achieving Flat Local Maxima**|Zhijin Ge et.al.|[2306.05225v1](http://arxiv.org/abs/2306.05225v1)|null|Transfer-based attack adopts the adversarial examples generated on the
surrogate model to attack various models, making it applicable in the physical
world and attracting increasing interest. Recently, various adversarial attacks
have emerged to boost adversarial transferability from different perspectives.
In this work, inspired by the fact that flat local minima are correlated with
good generalization, we assume and empirically validate that adversarial
examples at a flat local region tend to have good transferability by
introducing a penalized gradient norm to the original loss function. Since
directly optimizing the gradient regularization norm is computationally
expensive and intractable for generating adversarial examples, we propose an
approximation optimization method to simplify the gradient update of the
objective function. Specifically, we randomly sample an example and adopt the
first-order gradient to approximate the second-order Hessian matrix, which
makes computing more efficient by interpolating two Jacobian matrices.
Meanwhile, in order to obtain a more stable gradient direction, we randomly
sample multiple examples and average the gradients of these examples to reduce
the variance due to random sampling during the iterative process. Extensive
experimental results on the ImageNet-compatible dataset show that the proposed
method can generate adversarial examples at flat local regions, and
significantly improve the adversarial transferability on either normally
trained models or adversarially trained models than the state-of-the-art
attacks.|
|**2023-06-08**|**Time-Optimal Path Tracking with ISO Safety Guarantees**|Shohei Fujii et.al.|[2306.05197v1](http://arxiv.org/abs/2306.05197v1)|null|One way of ensuring operator's safety during human-robot collaboration is
through Speed and Separation Monitoring (SSM), as defined in ISO standard
ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions:
consider for instance the case when the robot does not move at all, a human
operator can still collide with it by hitting it of her own voluntary motion.
In the SSM framework, it is possible however to minimize harm by requiring
this: \emph{if} a collision ever occurs, then the robot must be in a
\emph{stationary state} (all links have zero velocity) at the time instant of
the collision. In this paper, we propose a time-optimal control policy based on
Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior.
Specifically, we show that: for any robot motion that is strictly faster than
the motion recommended by our policy, there exists a human motion that results
in a collision with the robot in a non-stationary state. Correlatively, we
show, in simulation, that our policy is strictly less conservative than
state-of-the-art safe robot control methods. Additionally, we propose a
parallelization method to reduce the computation time of our pre-computation
phase (down to 0.5 sec, practically), which enables the whole pipeline
(including the pre-computation) to be executed at runtime, nearly in real-time.
Finally, we demonstrate the application of our method in a scenario:
time-optimal, safe control of a 6-dof industrial robot.|
|**2023-06-08**|**EMO: Episodic Memory Optimization for Few-Shot Meta-Learning**|Yingjun Du et.al.|[2306.05189v1](http://arxiv.org/abs/2306.05189v1)|null|Few-shot meta-learning presents a challenge for gradient descent optimization
due to the limited number of training samples per task. To address this issue,
we propose an episodic memory optimization for meta-learning, we call
\emph{EMO}, which is inspired by the human ability to recall past learning
experiences from the brain's memory. EMO retains the gradient history of past
experienced tasks in external memory, enabling few-shot learning in a
memory-augmented way. By learning to retain and recall the learning process of
past training tasks, EMO nudges parameter updates in the right direction, even
when the gradients provided by a limited number of examples are uninformative.
We prove theoretically that our algorithm converges for smooth, strongly convex
objectives. EMO is generic, flexible, and model-agnostic, making it a simple
plug-and-play optimizer that can be seamlessly embedded into existing
optimization-based few-shot meta-learning approaches. Empirical results show
that EMO scales well with most few-shot classification benchmarks and improves
the performance of optimization-based meta-learning methods, resulting in
accelerated convergence.|
|**2023-06-08**|**M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models**|Wenxuan Zhang et.al.|[2306.05179v1](http://arxiv.org/abs/2306.05179v1)|null|Despite the existence of various benchmarks for evaluating natural language
processing models, we argue that human exams are a more suitable means of
evaluating general intelligence for large language models (LLMs), as they
inherently demand a much wider range of abilities such as language
understanding, domain knowledge, and problem-solving skills. To this end, we
introduce M3Exam, a novel benchmark sourced from real and official human exam
questions for evaluating LLMs in a multilingual, multimodal, and multilevel
context. M3Exam exhibits three unique characteristics: (1) multilingualism,
encompassing questions from multiple countries that require strong multilingual
proficiency and cultural knowledge; (2) multimodality, accounting for the
multimodal nature of many exam questions to test the model's multimodal
understanding capability; and (3) multilevel structure, featuring exams from
three critical educational periods to comprehensively assess a model's
proficiency at different levels. In total, M3Exam contains 12,317 questions in
9 diverse languages with three educational levels, where about 23\% of the
questions require processing images for successful solving. We assess the
performance of top-performing LLMs on M3Exam and find that current models,
including GPT-4, still struggle with multilingual text, particularly in
low-resource and non-Latin script languages. Multimodal LLMs also perform
poorly with complex multimodal questions. We believe that M3Exam can be a
valuable resource for comprehensively evaluating LLMs by examining their
multilingual and multimodal abilities and tracking their development. Data and
evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.|
|**2023-06-08**|**Bayesian Optimization of Expensive Nested Grey-Box Functions**|Wenjie Xu et.al.|[2306.05150v1](http://arxiv.org/abs/2306.05150v1)|null|We consider the problem of optimizing a grey-box objective function, i.e.,
nested function composed of both black-box and white-box functions. A general
formulation for such grey-box problems is given, which covers the existing
grey-box optimization formulations as special cases. We then design an
optimism-driven algorithm to solve it. Under certain regularity assumptions,
our algorithm achieves similar regret bound as that for the standard black-box
Bayesian optimization algorithm, up to a constant multiplicative term depending
on the Lipschitz constants of the functions considered. We further extend our
method to the constrained case and discuss several special cases. For the
commonly used kernel functions, the regret bounds allow us to derive a
convergence rate to the optimal solution. Experimental results show that our
grey-box optimization method empirically improves the speed of finding the
global optimal solution significantly, as compared to the standard black-box
optimization algorithm.|

### Multi-Object Tracking
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**The sum of all width-one tensors**|William Q. Erickson et.al.|[2306.05417v1](http://arxiv.org/abs/2306.05417v1)|null|This paper generalizes a recent result by the authors concerning the sum of
width-one matrices; in the present work, we consider width-one tensors of
arbitrary dimensions. A tensor is said to have width 1 if, when visualized as
an array, its nonzero entries lie along a path consisting of steps in the
directions of the standard coordinate vectors. We prove two different formulas
to compute the sum of all width-one tensors with fixed dimensions and fixed sum
of (nonnegative integer) components. The first formula is obtained by
converting width-one tensors into tuples of one-row semistandard Young
tableaux; the second formula, which extracts coefficients from products of
multiset Eulerian polynomials, is derived via Stanley-Reisner theory, making
use of the EL-shelling of the order complex on the standard basis of tensors.|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**A ship-in-a-bottle quantum gas microscope for magnetic mixtures**|Maximilian Sohmen et.al.|[2306.05404v1](http://arxiv.org/abs/2306.05404v1)|null|Quantum gas microscopes are versatile and powerful tools for fundamental
science as well as promising candidates for enticing applications such as in
quantum simulation or quantum computation. Here we present a quantum gas
microscopy setup for experiments with highly magnetic atoms of the lanthanoid
elements erbium and dysprosium. Our setup features a non-magnetic,
non-conducting, large-working-distance, high-numerical-aperture, in-vacuum
microscope objective, mounted inside a glue-free quartz glass cell. The quartz
glass cell is enclosed by a compact multi-shell ferromagnetic shield that
passively suppresses external magnetic field noise by a factor of more than a
thousand. Our setup will enable direct manipulation and probing of the rich
quantum many-body physics of dipolar atoms in optical lattices, and bears the
potential to put exciting theory proposals -- including exotic magnetic phases
and quantum phase transitions -- to an experimental test.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Modular Visual Question Answering via Code Generation**|Sanjay Subramanian et.al.|[2306.05392v1](http://arxiv.org/abs/2306.05392v1)|[link](https://github.com/sanjayss34/codevqa)|We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**An inflation model for massive primordial black holes to interpret the JWST observations**|Bing-Yu Su et.al.|[2306.05364v1](http://arxiv.org/abs/2306.05364v1)|null|The first observations of the James Webb Space Telescope (JWST) have
identified six massive galaxy candidates with the stellar masses $M_\ast\gtrsim
10^{10}\,M_\odot$ at high redshifts $7.4\lesssim z\lesssim 9.1$, with two most
massive high-$z$ objects having the cumulative comoving number densities
$n_{\rm G}$ up to $1.6\times 10^{-5}\, {\rm Mpc}^{-3}$. The presence of such
massive sources in the early universe challenges the standard $\Lambda$CDM
model since the needed star formation efficiency is unrealistically high. This
tension can be alleviated via the accretion of massive primordial black holes
(PBHs). In this work, with the updated data from the first JWST observations,
we find that the PBHs with mass $10^8\,M_\odot\lesssim M_{\rm PBH}\lesssim
10^{11}\,M_\odot$ can act as the seeds of extremely massive galaxies even with
a low abundance $10^{-7}\lesssim f_{\rm PBH}\lesssim 10^{-3}$. We construct an
ultraslow-roll inflation model and investigate its possibility of producing the
required PBHs. We explore the model in two cases, depending on whether there is
a perfect plateau on the inflaton potential. If the plateau is allowed to
incline slightly, our model can produce the PBHs that cover the required PBH
mass and abundance range to explain the JWST data.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models**|Tiantian Feng et.al.|[2306.05350v1](http://arxiv.org/abs/2306.05350v1)|null|Many recent studies have focused on fine-tuning pre-trained models for speech
emotion recognition (SER), resulting in promising performance compared to
traditional methods that rely largely on low-level, knowledge-inspired acoustic
features. These pre-trained speech models learn general-purpose speech
representations using self-supervised or weakly-supervised learning objectives
from large-scale datasets. Despite the significant advances made in SER through
the use of pre-trained architecture, fine-tuning these large pre-trained models
for different datasets requires saving copies of entire weight parameters,
rendering them impractical to deploy in real-world settings. As an alternative,
this work explores parameter-efficient fine-tuning (PEFT) approaches for
adapting pre-trained speech models for emotion recognition. Specifically, we
evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa
(Low-rank approximation) on four popular SER testbeds. Our results reveal that
LoRa achieves the best fine-tuning performance in emotion recognition while
enhancing fairness and requiring only a minimal extra amount of weight
parameters. Furthermore, our findings offer novel insights into future research
directions in SER, distinct from existing approaches focusing on directly
fine-tuning the model architecture. Our code is publicly available under:
https://github.com/usc-sail/peft-ser.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and
neutron star mergers are obtained from nuclear matter dynamical structure
functions that encode many-body effects from nuclear mean fields and
correlations. We employ nuclear interactions from chiral effective field theory
to calculate the density, spin, isospin, and spin-isospin response functions of
warm beta-equilibrium nuclear matter. We include corrections to the
single-particle energies in the mean field approximation as well as vertex
corrections resummed in the random phase approximation (RPA), including, for
the first time, both direct and exchange diagrams. We find that correlations
included through the RPA redistribute the strength of the response to higher
energy for neutrino absorption and lower energy for antineutrino absorption.
This tends to suppress the absorption rate of electron neutrinos across all
relevant energy scales. In contrast, the inclusion of RPA correlations enhances
the electron antineutrino absorption rate at low energy and supresses the rate
at high energy. These effects are especially important at high-density and in
the vicinity of the neutrino decoupling region. Implications for heavy element
nucleosynthesis, electromagnetic signatures of compact object mergers,
supernova dynamics, and neutrino detection from galactic supernovae are
discussed briefly.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors**|Peng Qi et.al.|[2306.05241v1](http://arxiv.org/abs/2306.05241v1)|null|The prevalence of short video platforms has spawned a lot of fake news
videos, which have stronger propagation ability than textual fake news. Thus,
automatically detecting fake news videos has been an important countermeasure
in practice. Previous works commonly verify each news video individually with
multimodal information. Nevertheless, news videos from different perspectives
regarding the same event are commonly posted together, which contain
complementary or contradictory information and thus can be used to evaluate
each other mutually. To this end, we introduce a new and practical paradigm,
i.e., cross-sample fake news video detection, and propose a novel framework,
Neighbor-Enhanced fakE news video Detection (NEED), which integrates the
neighborhood relationship of new videos belonging to the same event. NEED can
be readily combined with existing single-sample detectors and further enhance
their performances with the proposed graph aggregation (GA) and debunking
rectification (DR) modules. Specifically, given the feature representations
obtained from single-sample detectors, GA aggregates the neighborhood
information with the dynamic graph to enrich the features of independent
samples. After that, DR explicitly leverages the relationship between debunking
videos and fake news videos to refute the candidate videos via textual and
visual consistency. Extensive experiments on the public benchmark demonstrate
that NEED greatly improves the performance of both single-modal (up to 8.34% in
accuracy) and multimodal (up to 4.97% in accuracy) base detectors. Codes are
available in https://github.com/ICTMCG/NEED.|
|**2023-06-08**|**SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth**|Zelin Liu et.al.|[2306.05238v1](http://arxiv.org/abs/2306.05238v1)|null|Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.|

### Image Matching
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Improving Negative-Prompt Inversion via Proximal Guidance**|Ligong Han et.al.|[2306.05414v1](http://arxiv.org/abs/2306.05414v1)|[link](https://github.com/phymhan/prompt-to-prompt)|DDIM inversion has revealed the remarkable potential of real image editing
within diffusion-based methods. However, the accuracy of DDIM reconstruction
degrades as larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align
the reconstruction and inversion trajectories with larger CFG scales, enabling
real image editing with cross-attention control. Negative-prompt inversion
(NPI) further offers a training-free closed-form solution of NTI. However, it
may introduce artifacts and is still constrained by DDIM reconstruction
quality. To overcome these limitations, we propose Proximal Negative-Prompt
Inversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI with
a regularization term and reconstruction guidance, which reduces artifacts
while capitalizing on its training-free nature. Our method provides an
efficient and straightforward approach, effectively addressing real image
editing tasks with minimal computational overhead.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Resonant Anti-Reflection Metasurface for Infrared Transmission Optics**|John Brewer et.al.|[2306.05405v1](http://arxiv.org/abs/2306.05405v1)|null|A fundamental capability for any transmissive optical component is
anti-reflection, yet this capability is challenging to achieve in a
cost-efficient manner over longer infrared wavelengths. We demonstrate that Mie
resonant nanophotonic structures enhance transmission in Silicon, allowing it
to function as an effective optical material over long-wave infrared
wavelengths. This approach enables a window optic with up to 40\% greater
transmission than equal thickness unpatterned Si. Imaging comparisons with
unpatterned silicon and off-the-shelf Germanium optics are shown, as well as
basic broadband slant edge MTF measurements. Overall, we demonstrate how
Mie-resonant structures can be used to improve optical transmission through
window optics of arbitrary lithographically patternable optical media, and
highlight their possible use in imaging applications.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Bayesian model calibration for diblock copolymer thin film self-assembly using power spectrum of microscopy data**|Lianghao Cao et.al.|[2306.05398v1](http://arxiv.org/abs/2306.05398v1)|null|Identifying parameters of computational models from experimental data, or
model calibration, is fundamental for assessing and improving the
predictability and reliability of computer simulations. In this work, we
propose a method for Bayesian calibration of models that predict morphological
patterns of diblock copolymer (Di-BCP) thin film self-assembly while accounting
for various sources of uncertainties in pattern formation and data acquisition.
This method extracts the azimuthally-averaged power spectrum (AAPS) of the
top-down microscopy characterization of Di-BCP thin film patterns as summary
statistics for Bayesian inference of model parameters via the pseudo-marginal
method. We derive the analytical and approximate form of a conditional
likelihood for the AAPS of image data. We demonstrate that AAPS-based image
data reduction retains the mutual information, particularly on important length
scales, between image data and model parameters while being relatively agnostic
to the aleatoric uncertainties associated with the random long-range disorder
of Di-BCP patterns. Additionally, we propose a phase-informed prior
distribution for Bayesian model calibration. Furthermore, reducing image data
to AAPS enables us to efficiently build surrogate models to accelerate the
proposed Bayesian model calibration procedure. We present the formulation and
training of two multi-layer perceptrons for approximating the
parameter-to-spectrum map, which enables fast integrated likelihood
evaluations. We validate the proposed Bayesian model calibration method through
numerical examples, for which the neural network surrogate delivers a fivefold
reduction of the number of model simulations performed for a single calibration
task.|
|**2023-06-08**|**Modular Visual Question Answering via Code Generation**|Sanjay Subramanian et.al.|[2306.05392v1](http://arxiv.org/abs/2306.05392v1)|[link](https://github.com/sanjayss34/codevqa)|We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**|Ge Yuan et.al.|[2306.05356v1](http://arxiv.org/abs/2306.05356v1)|[link](https://github.com/ygtxr1997/reliableswap)|Almost all advanced face swapping approaches use reconstruction as the proxy
task, i.e., supervision only exists when the target and source belong to the
same person. Otherwise, lacking pixel-level supervision, these methods struggle
for source identity preservation. This paper proposes to construct reliable
supervision, dubbed cycle triplets, which serves as the image-level guidance
when the source identity differs from the target one during training.
Specifically, we use face reenactment and blending techniques to synthesize the
swapped face from real images in advance, where the synthetic face preserves
source identity and target attributes. However, there may be some artifacts in
such a synthetic face. To avoid the potential artifacts and drive the
distribution of the network output close to the natural one, we reversely take
synthetic images as input while the real face as reliable supervision during
the training stage of face swapping. Besides, we empirically find that the
existing methods tend to lose lower-face details like face shape and mouth from
the source. This paper additionally designs a FixerNet, providing
discriminative embeddings of lower faces as an enhancement. Our face swapping
framework, named ReliableSwap, can boost the performance of any existing face
swapping network with negligible overhead. Extensive experiments demonstrate
the efficacy of our ReliableSwap, especially in identity preservation. The
project page is https://reliable-swap.github.io/.|
|**2023-06-08**|**The correlations between galaxy properties in different environments of the cosmic web**|Anindita Nandi et.al.|[2306.05354v1](http://arxiv.org/abs/2306.05354v1)|null|We study the correlations between galaxy properties in different environments
of the cosmic web using a volume limited sample from the SDSS. We determine the
geometric environment at the location of each galaxy using the eigenvalues of
the tidal tensor. The correlations are then separately analyzed in different
cosmic web environments. We use the Pearson correlation coefficient and the
normalized mutual information for measuring the correlations. Using a
two-tailed t-test, we find that the correlations between the galaxy properties
are sensitive to the geometric environments. The stellar mass can be an
important link between the galaxy properties and the environment. We repeat the
analysis after matching the stellar mass distributions in different
environments and find that the conclusions remain unchanged for most of the
relations. Our study suggests that the galaxy properties and their
interrelationships are susceptible to the geometric environments of the cosmic
web.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**A Review of the Recent Developments in the Fabrication Processes of CMOS Image Sensors for Smartphones**|Kirthika Nahalingam et.al.|[2306.05339v1](http://arxiv.org/abs/2306.05339v1)|null|CMOS Image Sensors are experiencing significant growth due to their
capabilities to be integrated in smartphones with refined image quality. One of
the major contributions to the growth of image sensors is the innovation
brought about in their fabrication processes. This paper presents a detailed
review of the different fabrication processes of the CMOS Image Sensors and its
impact on the image quality of smartphone pictures. Fabrication of CMOS image
sensors using wafer bonding technologies such as Through Silicon Vias and CuCu
hybrid bonding along with their experimental results are discussed. A 2 layer
architecture of photodiode and pixel transistors has adopted the 3D sequential
integration, by which the wafers are bonded together one after the other in the
fabrication process. Electrical characteristics and reliability test results
are presented for the former two fabrication processes and the improvements in
the pixels performance such as conversion gain, quantum efficiency, full well
capacity and dynamic range for the 2 layer architecture are discussed.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of
the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our
solver relies on a relaxation method in which convergence is sought as the
steady-state solution of a parabolic equation, whose time-discretization is
governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings
indicate that the RKL-based Poisson solver, which is both fully parallel and
rapidly convergent, has the potential to serve as a practical alternative to
conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and
\textit{successive over-relaxation} (SOR) methods. Additionally, it can
mitigate some of the drawbacks of these traditional techniques. We incorporate
our algorithm into a multigrid solver to provide a simple and efficient gravity
solver that can be used to obtain the gravitational potentials in
self-gravitational hydrodynamics. We test our implementation against a broad
range of standard self-gravitating astrophysical problems designed to examine
different aspects of the code. We demonstrate that the results match
excellently with the analytical predictions (when available), and the findings
of similar previous studies.|
|**2023-06-08**|**Actively learning a Bayesian matrix fusion model with deep side information**|Yangyang Yu et.al.|[2306.05331v1](http://arxiv.org/abs/2306.05331v1)|null|High-dimensional deep neural network representations of images and concepts
can be aligned to predict human annotations of diverse stimuli. However, such
alignment requires the costly collection of behavioral responses, such that, in
practice, the deep-feature spaces are only ever sparsely sampled. Here, we
propose an active learning approach to adaptively sampling experimental stimuli
to efficiently learn a Bayesian matrix factorization model with deep side
information. We observe a significant efficiency gain over a passive baseline.
Furthermore, with a sequential batched sampling strategy, the algorithm is
applicable not only to small datasets collected from traditional laboratory
experiments but also to settings where large-scale crowdsourced data collection
is needed to accurately align the high-dimensional deep feature representations
derived from pre-trained networks.|
|**2023-06-08**|**Federated Learning under Covariate Shifts with Generalization Guarantees**|Ali Ramezani-Kebrya et.al.|[2306.05325v1](http://arxiv.org/abs/2306.05325v1)|null|This paper addresses intra-client and inter-client covariate shifts in
federated learning (FL) with a focus on the overall generalization performance.
To handle covariate shifts, we formulate a new global model training paradigm
and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)
along with improving density ratio matching methods without requiring perfect
knowledge of the supremum over true ratios. We also propose the
communication-efficient variant FITW-ERM with the same level of privacy
guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM
achieves smaller generalization error than classical ERM under certain
settings. Experimental results demonstrate the superiority of FTW-ERM over
existing FL baselines in challenging imbalanced federated settings in terms of
data distribution shifts across clients.|
|**2023-06-08**|**Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations**|Matteo Salvador et.al.|[2306.05321v1](http://arxiv.org/abs/2306.05321v1)|null|Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.|
|**2023-06-08**|**KIT's Multilingual Speech Translation System for IWSLT 2023**|Danni Liu et.al.|[2306.05320v1](http://arxiv.org/abs/2306.05320v1)|null|Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.|
|**2023-06-08**|**A physically motivated analytical expression for the temperature dependence of the zero-field splitting of the nitrogen-vacancy center in diamond**|M. C. Cambria et.al.|[2306.05318v1](http://arxiv.org/abs/2306.05318v1)|null|The temperature dependence of the zero-field splitting (ZFS) between the
$|m_{s}=0\rangle$ and $|m_{s}=\pm 1\rangle$ levels of the nitrogen-vacancy (NV)
center's electronic ground-state spin triplet can be used as a robust nanoscale
thermometer in a broad range of environments. However, despite numerous
measurements of this dependence in different temperature ranges, to our
knowledge no analytical expression has been put forward that captures the
scaling of the ZFS of the NV center across all relevant temperatures. Here we
present a simple, analytical, and physically motivated expression for the
temperature dependence of the NV center's ZFS that matches all experimental
observations, in which the ZFS shifts in proportion to the occupation numbers
of two representative phonon modes. In contrast to prior models our expression
does not diverge outside the regions of fitting. We show that our model
quantitatively matches experimental measurements of the ZFS from 15 to 500 K in
single NV centers in ultra-pure bulk diamond, and we compare our model and
measurements to prior models and experimental data.|
|**2023-06-08**|**A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments**|Guangyao Zheng et.al.|[2306.05310v1](http://arxiv.org/abs/2306.05310v1)|null|While Deep Reinforcement Learning has been widely researched in medical
imaging, the training and deployment of these models usually require powerful
GPUs. Since imaging environments evolve rapidly and can be generated by edge
devices, the algorithm is required to continually learn and adapt to changing
environments, and adjust to low-compute devices. To this end, we developed
three image coreset algorithms to compress and denoise medical images for
selective experience replayed-based lifelong reinforcement learning. We
implemented neighborhood averaging coreset, neighborhood sensitivity-based
sampling coreset, and maximum entropy coreset on full-body DIXON water and
DIXON fat MRI images. All three coresets produced 27x compression with
excellent performance in localizing five anatomical landmarks: left knee, right
trochanter, left kidney, spleen, and lung across both imaging environments.
Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$
average distance error, compared to the conventional lifelong learning
framework's $19.24\pm 50.77$.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis**|Gongshu Wang et.al.|[2306.05297v1](http://arxiv.org/abs/2306.05297v1)|null|Structural magnetic resonance imaging (sMRI) has shown great clinical value
and has been widely used in deep learning (DL) based computer-aided brain
disease diagnosis. Previous approaches focused on local shapes and textures in
sMRI that may be significant only within a particular domain. The learned
representations are likely to contain spurious information and have a poor
generalization ability in other diseases and datasets. To facilitate capturing
meaningful and robust features, it is necessary to first comprehensively
understand the intrinsic pattern of the brain that is not restricted within a
single data/task domain. Considering that the brain is a complex connectome of
interlinked neurons, the connectional properties in the brain have strong
biological significance, which is shared across multiple domains and covers
most pathological information. In this work, we propose a connectional style
contextual representation learning model (CS-CRL) to capture the intrinsic
pattern of the brain, used for multiple brain disease diagnosis. Specifically,
it has a vision transformer (ViT) encoder and leverages mask reconstruction as
the proxy task and Gram matrices to guide the representation of connectional
information. It facilitates the capture of global context and the aggregation
of features with biological plausibility. The results indicate that CS-CRL
achieves superior accuracy in multiple brain disease diagnosis tasks across six
datasets and three diseases and outperforms state-of-the-art models.
Furthermore, we demonstrate that CS-CRL captures more brain-network-like
properties, better aggregates features, is easier to optimize and is more
robust to noise, which explains its superiority in theory. Our source code will
be released soon.|

### Semantic Segmentation
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**Improving the Reporting of Threats to Construct Validity**|Dag I. K. Sjberg et.al.|[2306.05336v1](http://arxiv.org/abs/2306.05336v1)|null|Background: Construct validity concerns the use of indicators to measure a
concept that is not directly measurable. Aim: This study intends to identify,
categorize, assess and quantify discussions of threats to construct validity in
empirical software engineering literature and use the findings to suggest ways
to improve the reporting of construct validity issues. Method: We analyzed 83
articles that report human-centric experiments published in five top-tier
software engineering journals from 2015 to 2019. The articles' text concerning
threats to construct validity was divided into segments (the unit of analysis)
based on predefined categories. The segments were then evaluated regarding
whether they clearly discussed a threat and a construct. Results: Three-fifths
of the segments were associated with topics not related to construct validity.
Two-thirds of the articles discussed construct validity without using the
definition of construct validity given in the article. The threats were clearly
described in more than four-fifths of the segments, but the construct in
question was clearly described in only two-thirds of the segments. The
construct was unclear when the discussion was not related to construct validity
but to other types of validity. Conclusions: The results show potential for
improving the understanding of construct validity in software engineering.
Recommendations addressing the identified weaknesses are given to improve the
awareness and reporting of CV.|
|**2023-06-08**|**Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation**|Shishuai Hu et.al.|[2306.05254v1](http://arxiv.org/abs/2306.05254v1)|null|Deep learning-based medical image segmentation models suffer from performance
degradation when deployed to a new healthcare center. To address this issue,
unsupervised domain adaptation and multi-source domain generalization methods
have been proposed, which, however, are less favorable for clinical practice
due to the cost of acquiring target-domain data and the privacy concerns
associated with redistributing the data from multiple source domains. In this
paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle
\textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical
image segmentation. In C$^2$SDG, the shallower features of each image and its
style-augmented counterpart are extracted and used for contrastive training,
resulting in the disentangled style representations and structure
representations. The segmentation is performed based solely on the structure
representations. Our method is novel in the contrastive perspective that
enables channel-wise feature disentanglement using a single source domain. We
evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup
and optic disc segmentation benchmark. Our results suggest the effectiveness of
each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the
baseline and all competing methods with a large margin. The code will be
available at \url{https://github.com/ShishuaiHu/CCSDG}.|
|**2023-06-08**|**Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation**|Qiujie Dong et.al.|[2306.05246v1](http://arxiv.org/abs/2306.05246v1)|null|With the rapid development of geometric deep learning techniques, many
mesh-based convolutional operators have been proposed to bridge irregular mesh
structures and popular backbone networks. In this paper, we show that while
convolutions are helpful, a simple architecture based exclusively on
multi-layer perceptrons (MLPs) is competent enough to deal with mesh
classification and semantic segmentation. Our new network architecture, named
Mesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) and
dihedral angles as the input, replaces the convolution module of a ResNet with
Multi-layer Perceptron (MLP), and utilizes layer normalization (LN) to perform
the normalization of the layers. The all-MLP architecture operates in an
end-to-end fashion and does not include a pooling module. Extensive
experimental results on the mesh classification/segmentation tasks validate the
effectiveness of the all-MLP architecture.|
|**2023-06-08**|**Matching Latent Encoding for Audio-Text based Keyword Spotting**|Kumari Nishu et.al.|[2306.05245v1](http://arxiv.org/abs/2306.05245v1)|null|Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown
high-quality results, but the key challenge of how to semantically align two
embeddings for multi-word keywords of different sequence lengths remains
largely unsolved. In this paper, we propose an audio-text-based end-to-end
model architecture for flexible keyword spotting (KWS), which builds upon
learned audio and text embeddings. Our architecture uses a novel dynamic
programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally
partition the audio sequence into the same length as the word-based text
sequence using the monotonic alignment of spoken content. Our proposed model
consists of an encoder block to get audio and text embeddings, a projector
block to project individual embeddings to a common latent space, and an
audio-text aligner containing a novel DSP algorithm, which aligns the audio and
text embeddings to determine if the spoken content is the same as the text.
Experimental results show that our DSP is more effective than other
partitioning schemes, and the proposed architecture outperformed the
state-of-the-art results on the public dataset in terms of Area Under the ROC
Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.|
|**2023-06-08**|**Efficient Multi-Task Scene Analysis with RGB-D Transformers**|Shnke Benedikt Fischedick et.al.|[2306.05242v1](http://arxiv.org/abs/2306.05242v1)|[link](https://github.com/tui-nicr/nicr-scene-analysis-datasets)|Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.|
|**2023-06-08**|**Dealing with Semantic Underspecification in Multimodal NLP**|Sandro Pezzelle et.al.|[2306.05240v1](http://arxiv.org/abs/2306.05240v1)|null|Intelligent systems that aim at mastering language as humans do must deal
with its semantic underspecification, namely, the possibility for a linguistic
signal to convey only part of the information needed for communication to
succeed. Consider the usages of the pronoun they, which can leave the gender
and number of its referent(s) underspecified. Semantic underspecification is
not a bug but a crucial language feature that boosts its storage and processing
efficiency. Indeed, human speakers can quickly and effortlessly integrate
semantically-underspecified linguistic signals with a wide range of
non-linguistic information, e.g., the multimodal context, social or cultural
conventions, and shared knowledge. Standard NLP models have, in principle, no
or limited access to such extra information, while multimodal systems grounding
language into other modalities, such as vision, are naturally equipped to
account for this phenomenon. However, we show that they struggle with it, which
could negatively affect their performance and lead to harmful consequences when
used for applications. In this position paper, we argue that our community
should be aware of semantic underspecification if it aims to develop language
technology that can successfully interact with human users. We discuss some
applications where mastering it is crucial and outline a few directions toward
achieving this goal.|
|**2023-06-08**|**Channel prior convolutional attention for medical image segmentation**|Hejun Huang et.al.|[2306.05196v1](http://arxiv.org/abs/2306.05196v1)|null|Characteristics such as low contrast and significant organ shape variations
are often exhibited in medical images. The improvement of segmentation
performance in medical imaging is limited by the generally insufficient
adaptive capabilities of existing attention mechanisms. An efficient Channel
Prior Convolutional Attention (CPCA) method is proposed in this paper,
supporting the dynamic distribution of attention weights in both channel and
spatial dimensions. Spatial relationships are effectively extracted while
preserving the channel prior by employing a multi-scale depth-wise
convolutional module. The ability to focus on informative channels and
important regions is possessed by CPCA. A segmentation network called CPCANet
for medical image segmentation is proposed based on CPCA. CPCANet is validated
on two publicly available datasets. Improved segmentation performance is
achieved by CPCANet while requiring fewer computational resources through
comparisons with state-of-the-art algorithms. Our code is publicly available at
\url{https://github.com/Cuthbert-Huang/CPCANet}.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer**|Zehui Li et.al.|[2306.05143v1](http://arxiv.org/abs/2306.05143v1)|null|Given the increasing volume and quality of genomics data, extracting new
insights requires interpretable machine-learning models. This work presents
Genomic Interpreter: a novel architecture for genomic assay prediction. This
model outperforms the state-of-the-art models for genomic assay prediction
tasks. Our model can identify hierarchical dependencies in genomic sites. This
is achieved through the integration of 1D-Swin, a novel Transformer-based block
designed by us for modelling long-range hierarchical data. Evaluated on a
dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter
demonstrates superior performance in chromatin accessibility and gene
expression prediction and unmasks the underlying `syntax' of gene regulation.|
|**2023-06-08**|**Does Image Anonymization Impact Computer Vision Training?**|Hkon Hukkels et.al.|[2306.05135v1](http://arxiv.org/abs/2306.05135v1)|[link](https://github.com/hukkelas/deep_privacy2)|Image anonymization is widely adapted in practice to comply with privacy
regulations in many regions. However, anonymization often degrades the quality
of the data, reducing its utility for computer vision development. In this
paper, we investigate the impact of image anonymization for training computer
vision models on key computer vision tasks (detection, instance segmentation,
and pose estimation). Specifically, we benchmark the recognition drop on common
detection datasets, where we evaluate both traditional and realistic
anonymization for faces and full bodies. Our comprehensive experiments reflect
that traditional image anonymization substantially impacts final model
performance, particularly when anonymizing the full body. Furthermore, we find
that realistic anonymization can mitigate this decrease in performance, where
our experiments reflect a minimal performance drop for face anonymization. Our
study demonstrates that realistic anonymization can enable privacy-preserving
computer vision development with minimal performance degradation across a range
of important computer vision benchmarks.|
|**2023-06-08**|**Formalizing, Verifying and Applying ISA Security Guarantees as Universal Contracts**|Sander Huyghebaert et.al.|[2306.05128v1](http://arxiv.org/abs/2306.05128v1)|null|Progress has recently been made on specifying instruction set architectures
(ISAs) in executable formalisms rather than through prose. However, to date,
those formal specifications are limited to the functional aspects of the ISA
and do not cover its security guarantees. We present a novel, general method
for formally specifying an ISAs security guarantees to (1) balance the needs of
ISA implementations (hardware) and clients (software), (2) can be
semi-automatically verified to hold for the ISA operational semantics,
producing a high-assurance mechanically-verifiable proof, and (3) support
informal and formal reasoning about security-critical software in the presence
of adversarial code. Our method leverages universal contracts: software
contracts that express bounds on the authority of arbitrary untrusted code.
Universal contracts can be kept agnostic of software abstractions, and strike
the right balance between requiring sufficient detail for reasoning about
software and preserving implementation freedom of ISA designers and CPU
implementers. We semi-automatically verify universal contracts against Sail
implementations of ISA semantics using our Katamaran tool; a semi-automatic
separation logic verifier for Sail which produces machine-checked proofs for
successfully verified contracts. We demonstrate the generality of our method by
applying it to two ISAs that offer very different security primitives: (1)
MinimalCaps: a custom-built capability machine ISA and (2) a (somewhat
simplified) version of RISC-V with PMP. We verify a femtokernel using the
security guarantee we have formalized for RISC-V with PMP.|
|**2023-06-08**|**Unsupervised augmentation optimization for few-shot medical image segmentation**|Quan Quan et.al.|[2306.05107v1](http://arxiv.org/abs/2306.05107v1)|null|The augmentation parameters matter to few-shot semantic segmentation since
they directly affect the training outcome by feeding the networks with varying
perturbated samples. However, searching optimal augmentation parameters for
few-shot segmentation models without annotations is a challenge that current
methods fail to address. In this paper, we first propose a framework to
determine the ``optimal'' parameters without human annotations by solving a
distribution-matching problem between the intra-instance and intra-class
similarity distribution, with the intra-instance similarity describing the
similarity between the original sample of a particular anatomy and its
augmented ones and the intra-class similarity representing the similarity
between the selected sample and the others in the same class. Extensive
experiments demonstrate the superiority of our optimized augmentation in
boosting few-shot segmentation models. We greatly improve the top competing
method by 1.27\% and 1.11\% on Abd-MRI and Abd-CT datasets, respectively, and
even achieve a significant improvement for SSL-ALP on the left kidney by 3.39\%
on the Abd-CT dataset.|
|**2023-06-08**|**Proof-theoretic Semantics for Intuitionistic Multiplicative Linear Logic**|Alexander V. Gheorghiu et.al.|[2306.05106v1](http://arxiv.org/abs/2306.05106v1)|null|This work is the first exploration of proof-theoretic semantics for a
substructural logic. It focuses on the base-extension semantics (B-eS) for
intuitionistic multiplicative linear logic (IMLL). The starting point is a
review of Sandqvist's B-eS for intuitionistic propositional logic (IPL), for
which we propose an alternative treatment of conjunction that takes the form of
the generalized elimination rule for the connective. The resulting semantics is
shown to be sound and complete. This motivates our main contribution, a B-eS
for IMLL, in which the definitions of the logical constants all take the form
of their elimination rule and for which soundness and completeness are
established.|
|**2023-06-08**|**Capturing (Optimal) Relaxed Plans with Stable and Supported Models of Logic Programs**|Masood Feyzbakhsh Rankooh et.al.|[2306.05069v1](http://arxiv.org/abs/2306.05069v1)|null|We establish a novel relation between delete-free planning, an important task
for the AI Planning community also known as relaxed planning, and logic
programming. We show that given a planning problem, all subsets of actions that
could be ordered to produce relaxed plans for the problem can be bijectively
captured with stable models of a logic program describing the corresponding
relaxed planning problem. We also consider the supported model semantics of
logic programs, and introduce one causal and one diagnostic encoding of the
relaxed planning problem as logic programs, both capturing relaxed plans with
their supported models. Our experimental results show that these new encodings
can provide major performance gain when computing optimal relaxed plans, with
our diagnostic encoding outperforming state-of-the-art approaches to relaxed
planning regardless of the given time limit when measured on a wide collection
of STRIPS planning benchmarks.|
|**2023-06-08**|**Improving Visual Prompt Tuning for Self-supervised Vision Transformers**|Seungryong Yoo et.al.|[2306.05067v1](http://arxiv.org/abs/2306.05067v1)|[link](https://github.com/ryongithub/gatedprompttuning)|Visual Prompt Tuning (VPT) is an effective tuning method for adapting
pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra
learnable tokens, known as prompts, which steer the frozen pretrained ViTs.
Although VPT has demonstrated its applicability with supervised vision
transformers, it often underperforms with self-supervised ones. Through
empirical observations, we deduce that the effectiveness of VPT hinges largely
on the ViT blocks with which the prompt tokens interact. Specifically, VPT
shows improved performance on image classification tasks for MAE and MoCo v3
when the prompt tokens are inserted into later blocks rather than the first
block. These observations suggest that there exists an optimal location of
blocks for the insertion of prompt tokens. Unfortunately, identifying the
optimal blocks for prompts within each self-supervised ViT for diverse future
scenarios is a costly process. To mitigate this problem, we propose a simple
yet effective method that learns a gate for each ViT block to adjust its
intervention into the prompt tokens. With our method, prompt tokens are
selectively influenced by blocks that require steering for task adaptation. Our
method outperforms VPT variants in FGVC and VTAB image classification and
ADE20K semantic segmentation. The code is available at
https://github.com/ryongithub/GatedPromptTuning.|
|**2023-06-08**|**A Dynamic Feature Interaction Framework for Multi-task Visual Perception**|Yuling Xi et.al.|[2306.05061v1](http://arxiv.org/abs/2306.05061v1)|null|Multi-task visual perception has a wide range of applications in scene
understanding such as autonomous driving. In this work, we devise an efficient
unified framework to solve multiple common perception tasks, including instance
segmentation, semantic segmentation, monocular 3D detection, and depth
estimation. Simply sharing the same visual feature representations for these
tasks impairs the performance of tasks, while independent task-specific feature
extractors lead to parameter redundancy and latency. Thus, we design two
feature-merge branches to learn feature basis, which can be useful to, and thus
shared by, multiple perception tasks. Then, each task takes the corresponding
feature basis as the input of the prediction task head to fulfill a specific
task. In particular, one feature merge branch is designed for instance-level
recognition the other for dense predictions. To enhance inter-branch
communication, the instance branch passes pixel-wise spatial information of
each instance to the dense branch using efficient dynamic convolution
weighting. Moreover, a simple but effective dynamic routing mechanism is
proposed to isolate task-specific features and leverage common properties among
tasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to
parameter-efficient predictions for multi-task perception. In addition, as
tasks benefit from co-training with each other, our solution achieves on par
results on partially labeled settings on nuScenes and outperforms previous
works for 3D detection and depth estimation on the Cityscapes dataset with full
supervision.|
|**2023-06-08**|**Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition**|Luca Arrotta et.al.|[2306.05058v1](http://arxiv.org/abs/2306.05058v1)|null|Deep Learning models are a standard solution for sensor-based Human Activity
Recognition (HAR), but their deployment is often limited by labeled data
scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting
research direction to mitigate these issues by infusing knowledge about context
information into HAR deep learning classifiers. However, existing NeSy methods
for context-aware HAR require computationally expensive symbolic reasoners
during classification, making them less suitable for deployment on
resource-constrained devices (e.g., mobile devices). Additionally, NeSy
approaches for context-aware HAR have never been evaluated on in-the-wild
datasets, and their generalization capabilities in real-world scenarios are
questionable. In this work, we propose a novel approach based on a semantic
loss function that infuses knowledge constraints in the HAR model during the
training phase, avoiding symbolic reasoning during classification. Our results
on scripted and in-the-wild datasets show the impact of different semantic loss
functions in outperforming a purely data-driven model. We also compare our
solution with existing NeSy methods and analyze each approach's strengths and
weaknesses. Our semantic loss remains the only NeSy solution that can be
deployed as a single DNN without the need for symbolic reasoning modules,
reaching recognition rates close (and better in some cases) to existing
approaches.|
|**2023-06-08**|**Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators**|Hyein Lee et.al.|[2306.05041v1](http://arxiv.org/abs/2306.05041v1)|null|In this paper, we introduce a novel semantic generative communication (SGC)
framework, where generative users leverage text-to-image (T2I) generators to
create images locally from downloaded text prompts, while non-generative users
directly download images from a base station (BS). Although generative users
help reduce downlink transmission energy at the BS, they consume additional
energy for image generation and for uploading their generator state information
(GSI). We formulate the problem of minimizing the total energy consumption of
the BS and the users, and devise a generative user selection algorithm.
Simulation results corroborate that our proposed algorithm reduces total energy
by up to 54% compared to a baseline with all non-generative users.|
|**2023-06-08**|**COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features**|Jia-Qi Yang et.al.|[2306.05001v1](http://arxiv.org/abs/2306.05001v1)|null|With the development of the multi-media internet, visual characteristics have
become an important factor affecting user interests. Thus, incorporating visual
features is a promising direction for further performance improvements in
click-through rate (CTR) prediction. However, we found that simply injecting
the image embeddings trained with established pre-training methods only has
marginal improvements. We attribute the failure to two reasons: First, The
pre-training methods are designed for well-defined computer vision tasks
concentrating on semantic features, and they cannot learn personalized interest
in recommendations. Secondly, pre-trained image embeddings only containing
semantic information have little information gain, considering we already have
semantic features such as categories and item titles as inputs in the CTR
prediction task. We argue that a pre-training method tailored for
recommendation is necessary for further improvements. To this end, we propose a
recommendation-aware image pre-training method that can learn visual features
from user click histories. Specifically, we propose a user interest
reconstruction module to mine visual features related to user interests from
behavior histories. We further propose a contrastive training method to avoid
collapsing of embedding vectors. We conduct extensive experiments to verify
that our method can learn users' visual interests, and our method achieves
$0.46\%$ improvement in offline AUC and $0.88\%$ improvement in Taobao online
GMV with p-value$<0.01$.|
|**2023-06-08**|**The hadronic equation of state of HESS J1731-347 from the relativistic mean-field model with tensor coupling**|Kaixuan Huang et.al.|[2306.04992v1](http://arxiv.org/abs/2306.04992v1)|null|A recent report has identified a central compact object (CCO) within the
supernova remnant HESS J1731-347, with a mass and radius of
$M=0.77^{+0.20}_{-0.17}M{\odot}$ and $R=10.4^{+0.86}_{-0.78}$ km, respectively.
To investigate this light compact star, a density-dependent relativistic
mean-field (DDRMF) model, specifically the DDVT model, has been employed. The
DDVT model incorporates tensor couplings of vector mesons, which {can}
successfully describe the properties of finite nuclei, such as charge radius,
binding energy, and spin-orbit splitting. The introduction of tensor coupling
reduces the influence of scalar mesons and generates a softer equation of state
(EOS) in the outer core of the neutron star. Moreover, it has been found that
the crust segment plays a crucial role in reproducing the mass-radius relation
of HESS J1731-347, indicating a preference for a soft crust EOS. By
manipulating the coupling strength of the isovector meson in the DDVT parameter
set, a reasonable hadronic EOS has been obtained, satisfying the constraints
from the gravitational-wave signal GW170817, the simultaneous mass-radius
measurements from the NICER collaboration, and the properties of finite nuclei.
Notably, the mass-radius relations derived from this hadronic EOS also
accurately describe the observables of HESS J1731-347. Therefore, based on our
estimation, the CCO in HESS J1731-347 may represent the lightest known neutron
star.|
|**2023-06-08**|**Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping**|Jia-Qi Yang et.al.|[2306.04985v1](http://arxiv.org/abs/2306.04985v1)|null|Research has shown that deep networks tend to be overly optimistic about
their predictions, leading to an underestimation of prediction errors. Due to
the limited nature of data, existing studies have proposed various methods
based on model prediction probabilities to bin the data and evaluate
calibration error. We propose a more generalized definition of calibration
error called Partitioned Calibration Error (PCE), revealing that the key
difference among these calibration error metrics lies in how the data space is
partitioned. We put forth an intuitive proposition that an accurate model
should be calibrated across any partition, suggesting that the input space
partitioning can extend beyond just the partitioning of prediction
probabilities, and include partitions directly related to the input. Through
semantic-related partitioning functions, we demonstrate that the relationship
between model accuracy and calibration lies in the granularity of the
partitioning function. This highlights the importance of partitioning criteria
for training a calibrated and accurate model. To validate the aforementioned
analysis, we propose a method that involves jointly learning a semantic aware
grouping function based on deep model features and logits to partition the data
space into subsets. Subsequently, a separate calibration function is learned
for each subset. Experimental results demonstrate that our approach achieves
significant performance improvements across multiple datasets and network
architectures, thus highlighting the importance of the partitioning function
for calibration.|
|**2023-06-08**|**CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification**|Nan Yin et.al.|[2306.04979v1](http://arxiv.org/abs/2306.04979v1)|null|Although graph neural networks (GNNs) have achieved impressive achievements
in graph classification, they often need abundant task-specific labels, which
could be extensively costly to acquire. A credible solution is to explore
additional labeled graphs to enhance unsupervised learning on the target
domain. However, how to apply GNNs to domain adaptation remains unsolved owing
to the insufficient exploration of graph topology and the significant domain
discrepancy. In this paper, we propose \underline{Co}upled
\underline{Co}ntrastive Graph Representation Learning (\method{}), which
extracts the topological information from coupled learning branches and reduces
the domain discrepancy with coupled contrastive learning. \method{} contains a
graph convolutional network branch and a hierarchical graph kernel network
branch, which explore graph topology in implicit and explicit manners. Besides,
we incorporate coupled branches into a holistic multi-view contrastive learning
framework, which not only incorporates graph representations learned from
complementary views for enhanced understanding, but also encourages the
similarity between cross-domain example pairs with the same semantics for
domain alignment. Extensive experiments on various popular datasets show that
\method{} outperforms these competing baselines by 5.7\% to 21.0\% generally.|

### Object Detection
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**A ship-in-a-bottle quantum gas microscope for magnetic mixtures**|Maximilian Sohmen et.al.|[2306.05404v1](http://arxiv.org/abs/2306.05404v1)|null|Quantum gas microscopes are versatile and powerful tools for fundamental
science as well as promising candidates for enticing applications such as in
quantum simulation or quantum computation. Here we present a quantum gas
microscopy setup for experiments with highly magnetic atoms of the lanthanoid
elements erbium and dysprosium. Our setup features a non-magnetic,
non-conducting, large-working-distance, high-numerical-aperture, in-vacuum
microscope objective, mounted inside a glue-free quartz glass cell. The quartz
glass cell is enclosed by a compact multi-shell ferromagnetic shield that
passively suppresses external magnetic field noise by a factor of more than a
thousand. Our setup will enable direct manipulation and probing of the rich
quantum many-body physics of dipolar atoms in optical lattices, and bears the
potential to put exciting theory proposals -- including exotic magnetic phases
and quantum phase transitions -- to an experimental test.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**An inflation model for massive primordial black holes to interpret the JWST observations**|Bing-Yu Su et.al.|[2306.05364v1](http://arxiv.org/abs/2306.05364v1)|null|The first observations of the James Webb Space Telescope (JWST) have
identified six massive galaxy candidates with the stellar masses $M_\ast\gtrsim
10^{10}\,M_\odot$ at high redshifts $7.4\lesssim z\lesssim 9.1$, with two most
massive high-$z$ objects having the cumulative comoving number densities
$n_{\rm G}$ up to $1.6\times 10^{-5}\, {\rm Mpc}^{-3}$. The presence of such
massive sources in the early universe challenges the standard $\Lambda$CDM
model since the needed star formation efficiency is unrealistically high. This
tension can be alleviated via the accretion of massive primordial black holes
(PBHs). In this work, with the updated data from the first JWST observations,
we find that the PBHs with mass $10^8\,M_\odot\lesssim M_{\rm PBH}\lesssim
10^{11}\,M_\odot$ can act as the seeds of extremely massive galaxies even with
a low abundance $10^{-7}\lesssim f_{\rm PBH}\lesssim 10^{-3}$. We construct an
ultraslow-roll inflation model and investigate its possibility of producing the
required PBHs. We explore the model in two cases, depending on whether there is
a perfect plateau on the inflaton potential. If the plateau is allowed to
incline slightly, our model can produce the PBHs that cover the required PBH
mass and abundance range to explain the JWST data.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models**|Tiantian Feng et.al.|[2306.05350v1](http://arxiv.org/abs/2306.05350v1)|null|Many recent studies have focused on fine-tuning pre-trained models for speech
emotion recognition (SER), resulting in promising performance compared to
traditional methods that rely largely on low-level, knowledge-inspired acoustic
features. These pre-trained speech models learn general-purpose speech
representations using self-supervised or weakly-supervised learning objectives
from large-scale datasets. Despite the significant advances made in SER through
the use of pre-trained architecture, fine-tuning these large pre-trained models
for different datasets requires saving copies of entire weight parameters,
rendering them impractical to deploy in real-world settings. As an alternative,
this work explores parameter-efficient fine-tuning (PEFT) approaches for
adapting pre-trained speech models for emotion recognition. Specifically, we
evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa
(Low-rank approximation) on four popular SER testbeds. Our results reveal that
LoRa achieves the best fine-tuning performance in emotion recognition while
enhancing fairness and requiring only a minimal extra amount of weight
parameters. Furthermore, our findings offer novel insights into future research
directions in SER, distinct from existing approaches focusing on directly
fine-tuning the model architecture. Our code is publicly available under:
https://github.com/usc-sail/peft-ser.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|
|**2023-06-08**|**Weakly Lensed Gravitational Waves: Probing Cosmic Structures with Wave-Optics Features**|Stefano Savastano et.al.|[2306.05282v1](http://arxiv.org/abs/2306.05282v1)|null|Every signal propagating through the universe is at least weakly lensed by
the intervening gravitational field. In some situations, wave-optics phenomena
(diffraction, interference) can be observed as frequency-dependent modulations
of the waveform of gravitational waves (GWs). We will denote these signatures
as Wave-Optics Features (WOFs) and analyze them in detail. Our framework can
efficiently and accurately compute WOF in the single-image regime, of which
weak lensing is a limit. The phenomenology of WOF is rich and offers valuable
information: the dense cusps of individual halos appear as peaks in Green's
function for lensing. If resolved, these features probe the number, effective
masses, spatial distribution and inner profiles of substructures. High
signal-to-noise GW signals reveal WOFs well beyond the Einstein radius, leading
to a fair probability of observation by upcoming detectors such as LISA.
Potential applications of WOF include reconstruction of the lens' projected
density, delensing standard sirens and inferring large-scale structure
morphology and the halo mass function. Because WOF are sourced by light halos
with negligible baryonic content, their detection (or lack thereof) holds
promise to test dark matter scenarios.|
|**2023-06-08**|**Chiral EFT calculation of neutrino reactions in warm neutron-rich matter**|Eunkyoung Shin et.al.|[2306.05280v1](http://arxiv.org/abs/2306.05280v1)|null|Neutrino scattering and absorption rates of relevance to supernovae and
neutron star mergers are obtained from nuclear matter dynamical structure
functions that encode many-body effects from nuclear mean fields and
correlations. We employ nuclear interactions from chiral effective field theory
to calculate the density, spin, isospin, and spin-isospin response functions of
warm beta-equilibrium nuclear matter. We include corrections to the
single-particle energies in the mean field approximation as well as vertex
corrections resummed in the random phase approximation (RPA), including, for
the first time, both direct and exchange diagrams. We find that correlations
included through the RPA redistribute the strength of the response to higher
energy for neutrino absorption and lower energy for antineutrino absorption.
This tends to suppress the absorption rate of electron neutrinos across all
relevant energy scales. In contrast, the inclusion of RPA correlations enhances
the electron antineutrino absorption rate at low energy and supresses the rate
at high energy. These effects are especially important at high-density and in
the vicinity of the neutrino decoupling region. Implications for heavy element
nucleosynthesis, electromagnetic signatures of compact object mergers,
supernova dynamics, and neutrino detection from galactic supernovae are
discussed briefly.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors**|Peng Qi et.al.|[2306.05241v1](http://arxiv.org/abs/2306.05241v1)|null|The prevalence of short video platforms has spawned a lot of fake news
videos, which have stronger propagation ability than textual fake news. Thus,
automatically detecting fake news videos has been an important countermeasure
in practice. Previous works commonly verify each news video individually with
multimodal information. Nevertheless, news videos from different perspectives
regarding the same event are commonly posted together, which contain
complementary or contradictory information and thus can be used to evaluate
each other mutually. To this end, we introduce a new and practical paradigm,
i.e., cross-sample fake news video detection, and propose a novel framework,
Neighbor-Enhanced fakE news video Detection (NEED), which integrates the
neighborhood relationship of new videos belonging to the same event. NEED can
be readily combined with existing single-sample detectors and further enhance
their performances with the proposed graph aggregation (GA) and debunking
rectification (DR) modules. Specifically, given the feature representations
obtained from single-sample detectors, GA aggregates the neighborhood
information with the dynamic graph to enrich the features of independent
samples. After that, DR explicitly leverages the relationship between debunking
videos and fake news videos to refute the candidate videos via textual and
visual consistency. Extensive experiments on the public benchmark demonstrate
that NEED greatly improves the performance of both single-modal (up to 8.34% in
accuracy) and multimodal (up to 4.97% in accuracy) base detectors. Codes are
available in https://github.com/ICTMCG/NEED.|
|**2023-06-08**|**SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth**|Zelin Liu et.al.|[2306.05238v1](http://arxiv.org/abs/2306.05238v1)|null|Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.|
|**2023-06-08**|**Boosting Adversarial Transferability by Achieving Flat Local Maxima**|Zhijin Ge et.al.|[2306.05225v1](http://arxiv.org/abs/2306.05225v1)|null|Transfer-based attack adopts the adversarial examples generated on the
surrogate model to attack various models, making it applicable in the physical
world and attracting increasing interest. Recently, various adversarial attacks
have emerged to boost adversarial transferability from different perspectives.
In this work, inspired by the fact that flat local minima are correlated with
good generalization, we assume and empirically validate that adversarial
examples at a flat local region tend to have good transferability by
introducing a penalized gradient norm to the original loss function. Since
directly optimizing the gradient regularization norm is computationally
expensive and intractable for generating adversarial examples, we propose an
approximation optimization method to simplify the gradient update of the
objective function. Specifically, we randomly sample an example and adopt the
first-order gradient to approximate the second-order Hessian matrix, which
makes computing more efficient by interpolating two Jacobian matrices.
Meanwhile, in order to obtain a more stable gradient direction, we randomly
sample multiple examples and average the gradients of these examples to reduce
the variance due to random sampling during the iterative process. Extensive
experimental results on the ImageNet-compatible dataset show that the proposed
method can generate adversarial examples at flat local regions, and
significantly improve the adversarial transferability on either normally
trained models or adversarially trained models than the state-of-the-art
attacks.|
|**2023-06-08**|**Precision Measurements of $D_s^+ \to e^+ _e$ and $D_s^+ \to ^\prime e^+ _e$**|BESIII Collaboration et.al.|[2306.05194v1](http://arxiv.org/abs/2306.05194v1)|null|Precision measurements of the semileptonic decays $D_s^+ \to \eta e^+ \nu_e$
and $D_s^+ \to \eta^\prime e^+ \nu_e$ are performed using 7.33\,fb$^{-1}$ of
$e^+e^-$ collision data collected at center-of-mass energies between 4.128 and
4.226 GeV with the BESIII detector. The branching fractions obtained are
$\mathcal{B}(D_s^+ \to \eta e^{+} \nu_e)$ = $(2.251\pm0.039_{\rm stat.}\pm
0.051_{\rm syst.})\%$ and $\mathcal{B}(D_s^+ \to \eta^{\prime} e^{+} \nu_e)$ =
$(0.810\pm0.038_{\rm stat.}\pm 0.024_{\rm syst.})\%$. Combining these results
with the $\mathcal{B}(D^+\to\eta e^+ \nu_e)$ and $\mathcal{B}(D^+\to\eta^\prime
e^+ \nu_e)$ obtained from previous BESIII measurements, the $\eta-\eta^\prime$
mixing angle in the quark flavor basis is determined to be $\phi_{\rm P} =
(40.0\pm2.0_{\rm stat.}\pm0.6_{\rm syst.})^\circ$. Moreover, from the fits to
the partial decay rates of $D_s^+ \to \eta e^+ \nu_e$ and $D_s^+ \to
\eta^\prime e^+ \nu_e$, the products of the hadronic transition form factors
$f_+^{\eta^{(\prime)}}(0)$ and the modulus of the $c\to s$
Cabibbo-Kobayashi-Maskawa matrix element $|V_{cs}|$ are determined by using
different hadronic transition form factor parametrizations. Based on the
two-parameter series expansion, the products $f^\eta_+(0)|V_{cs}| =
0.4553\pm0.0071_{\rm stat}\pm0.0061_{\rm syst}$ and
$f^{\eta^\prime}_+(0)|V_{cs}| = 0.529\pm0.024_{\rm stat}\pm0.008_{\rm syst}$
are extracted. All results determined in this work supersede those measured in
the previous BESIII analyses based on the 3.19 fb$^{-1}$ subsample of data at
4.178 GeV.|
|**2023-06-08**|**EMO: Episodic Memory Optimization for Few-Shot Meta-Learning**|Yingjun Du et.al.|[2306.05189v1](http://arxiv.org/abs/2306.05189v1)|null|Few-shot meta-learning presents a challenge for gradient descent optimization
due to the limited number of training samples per task. To address this issue,
we propose an episodic memory optimization for meta-learning, we call
\emph{EMO}, which is inspired by the human ability to recall past learning
experiences from the brain's memory. EMO retains the gradient history of past
experienced tasks in external memory, enabling few-shot learning in a
memory-augmented way. By learning to retain and recall the learning process of
past training tasks, EMO nudges parameter updates in the right direction, even
when the gradients provided by a limited number of examples are uninformative.
We prove theoretically that our algorithm converges for smooth, strongly convex
objectives. EMO is generic, flexible, and model-agnostic, making it a simple
plug-and-play optimizer that can be seamlessly embedded into existing
optimization-based few-shot meta-learning approaches. Empirical results show
that EMO scales well with most few-shot classification benchmarks and improves
the performance of optimization-based meta-learning methods, resulting in
accelerated convergence.|
|**2023-06-08**|**Bayesian Optimization of Expensive Nested Grey-Box Functions**|Wenjie Xu et.al.|[2306.05150v1](http://arxiv.org/abs/2306.05150v1)|null|We consider the problem of optimizing a grey-box objective function, i.e.,
nested function composed of both black-box and white-box functions. A general
formulation for such grey-box problems is given, which covers the existing
grey-box optimization formulations as special cases. We then design an
optimism-driven algorithm to solve it. Under certain regularity assumptions,
our algorithm achieves similar regret bound as that for the standard black-box
Bayesian optimization algorithm, up to a constant multiplicative term depending
on the Lipschitz constants of the functions considered. We further extend our
method to the constrained case and discuss several special cases. For the
commonly used kernel functions, the regret bounds allow us to derive a
convergence rate to the optimal solution. Experimental results show that our
grey-box optimization method empirically improves the speed of finding the
global optimal solution significantly, as compared to the standard black-box
optimization algorithm.|
|**2023-06-08**|**Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose**|Wiktor Mucha et.al.|[2306.05147v1](http://arxiv.org/abs/2306.05147v1)|null|Egocentric action recognition is essential for healthcare and assistive
technology that relies on egocentric cameras because it allows for the
automatic and continuous monitoring of activities of daily living (ADLs)
without requiring any conscious effort from the user. This study explores the
feasibility of using 2D hand and object pose information for egocentric action
recognition. While current literature focuses on 3D hand pose information, our
work shows that using 2D skeleton data is a promising approach for hand-based
action classification, might offer privacy enhancement, and could be less
computationally demanding. The study uses a state-of-the-art transformer-based
method to classify sequences and achieves validation results of 94%,
outperforming other existing solutions. The accuracy of the test subset drops
to 76%, indicating the need for further generalization improvement. This
research highlights the potential of 2D hand and object pose information for
action recognition tasks and offers a promising alternative to 3D-based
methods.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**A study of the high-inclination population in the Kuiper belt -- IV. High-order mean motion resonances in the classical region**|Jian Li et.al.|[2306.05142v1](http://arxiv.org/abs/2306.05142v1)|null|In our previous study of Neptune's 4:7 mean motion resonance (MMR), we
discovered that its resonant angle can only librate within a specific
eccentricity ($e$) versus inclination ($i$) region, determined by a theoretical
limiting curve curve (Li et al. 2020). This ``permissible region'' is
independent of time and encompasses the entire possible stable region. We now
generalize this theory to investigate all high-order MMRs embedded in the main
classical Kuiper belt (MCKB). We first consider the 2nd-order 3:5 MMR in the
framework of planet migration and resonance capture, and have further validated
our limiting curve theory for both captured and observed 3:5 resonators. It
suggests that only the $(e, i)$ pairs inside the individual permissible regions
should be chosen as initial conditions for studying the in-situ evolution of
high-order resonators. With such a new setting, we proceed to explore the
long-term stability (for 4 Gyr) of different resonant populations, and our
simulations predict that: (1) the 3:5 and 4:7 resonators are comparable in
number, and they could have inclinations up to $40^{\circ}$; (2) the
populations of objects in the higher order 5:9, 6:11, 7:12 and 7:13 resonances
is about 1/10 of the 3:5 (or 4:7) resonator population, and nearly all of them
are found on the less inclined orbits with $i<10^{\circ}$; (3) for these
high-order resonances, almost all resonators reside in their individual
permissible regions. In summary, our results make predictions for the number
and orbital distributions of potential resonant objects that will be discovered
in the future throughout the MCKB.|

### Keypoint Detection
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**Gradient-Informed Quality Diversity for the Illumination of Discrete Spaces**|Raphael Boige et.al.|[2306.05138v1](http://arxiv.org/abs/2306.05138v1)|null|Quality Diversity (QD) algorithms have been proposed to search for a large
collection of both diverse and high-performing solutions instead of a single
set of local optima. While early QD algorithms view the objective and
descriptor functions as black-box functions, novel tools have been introduced
to use gradient information to accelerate the search and improve overall
performance of those algorithms over continuous input spaces. However a broad
range of applications involve discrete spaces, such as drug discovery or image
generation. Exploring those spaces is challenging as they are combinatorially
large and gradients cannot be used in the same manner as in continuous spaces.
We introduce map-elites with a Gradient-Informed Discrete Emitter (ME-GIDE),
which extends QD optimisation with differentiable functions over discrete
search spaces. ME-GIDE leverages the gradient information of the objective and
descriptor functions with respect to its discrete inputs to propose
gradient-informed updates that guide the search towards a diverse set of high
quality solutions. We evaluate our method on challenging benchmarks including
protein design and discrete latent space illumination and find that our method
outperforms state-of-the-art QD algorithms in all benchmarks.|
|**2023-06-07**|**3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels**|Zhenzhen Weng et.al.|[2306.04745v1](http://arxiv.org/abs/2306.04745v1)|null|Training a 3D human keypoint detector from point clouds in a supervised
manner requires large volumes of high quality labels. While it is relatively
easy to capture large amounts of human point clouds, annotating 3D keypoints is
expensive, subjective, error prone and especially difficult for long-tail cases
(pedestrians with rare poses, scooterists, etc.). In this work, we propose
GC-KPL - Geometry Consistency inspired Key Point Leaning, an approach for
learning 3D human joint locations from point clouds without human labels. We
achieve this by our novel unsupervised loss formulations that account for the
structure and movement of the human body. We show that by training on a large
training set from Waymo Open Dataset without any human annotated keypoints, we
are able to achieve reasonable performance as compared to the fully supervised
approach. Further, the backbone benefits from the unsupervised training and is
useful in downstream fewshot learning of keypoints, where fine-tuning on only
10 percent of the labeled training data gives comparable performance to
fine-tuning on the entire set. We demonstrated that GC-KPL outperforms by a
large margin over SoTA when trained on entire dataset and efficiently leverages
large volumes of unlabeled data.|
|**2023-06-07**|**Automatic graph representation algorithm for heterogeneous catalysis**|Zachary Gariepy et.al.|[2306.04742v1](http://arxiv.org/abs/2306.04742v1)|null|One of the most appealing aspects of machine learning for material design is
its high throughput exploration of chemical spaces, but to reach the ceiling of
ML-aided exploration, more than current model architectures and processing
algorithms are required. New architectures such as Graph Neural Networks (GNNs)
have seen significant research investments recently. For heterogeneous
catalysis, defining substrate intramolecular bonds and adsorbate/substrate
intermolecular bonds is a time-consuming and challenging process. Before
applying a model, dataset pre-processing, node/bond descriptor design, and
specific model constraints have to be considered. In this work, a framework
designed to solve these issues is presented in the form of an automatic graph
representation algorithm (AGRA) tool to extract the local chemical environment
of metallic surface adsorption sites is presented. This tool is able to gather
multiple adsorption geometry datasets composed of different systems and combine
them into a single model. To show AGRA's excellent transferability and reduced
computational cost compared to other graph representation methods, it was
applied to 5 different catalytic reaction datasets and benchmarked against the
Open Catalyst Projects (OCP) graph representation method. The two ORR datasets
with O/OH adsorbates obtained 0.053 eV RMSD when combined together, whereas the
three CO2RR datasets with CHO/CO/COOH obtained an average performance of 0.088
eV RMSD. To further display the algorithm's versatility and extrapolation
ability, a model was trained on a subset combination of all 5 datasets with an
RMSD of 0.105 eV. This universal model was then used to predict a wide range of
adsorption energies and an entirely new ORR catalyst system and then verified
through Density Functional Theory calculations|
|**2023-06-07**|**Machine Learning Universal Empirical Pseudopotentials**|Rokyeon Kim et.al.|[2306.04426v1](http://arxiv.org/abs/2306.04426v1)|null|Machine learning is used to generate empirical pseudopotentials that
characterize the local screened interactions in the Kohn-Sham Hamiltonian. Our
approach incorporates momentum-range-separated rotation-covariant descriptors
to capture crystal symmetries as well as crucial directional information of
bonds, thus realizing accurate descriptions of anisotropic solids. Trained
empirical potentials are shown to be versatile and transferable such that the
calculated energy bands and wave functions without cumbersome self-consistency
reproduce conventional ab initio results even for semiconductors with defects,
thus fostering faster and faithful data-driven materials researches.|
|**2023-06-07**|**Learning Probabilistic Coordinate Fields for Robust Correspondences**|Weiyue Zhao et.al.|[2306.04231v1](http://arxiv.org/abs/2306.04231v1)|null|We introduce Probabilistic Coordinate Fields (PCFs), a novel
geometric-invariant coordinate representation for image correspondence
problems. In contrast to standard Cartesian coordinates, PCFs encode
coordinates in correspondence-specific barycentric coordinate systems (BCS)
with affine invariance. To know \textit{when and where to trust} the encoded
coordinates, we implement PCFs in a probabilistic network termed PCF-Net, which
parameterizes the distribution of coordinate fields as Gaussian mixture models.
By jointly optimizing coordinate fields and their confidence conditioned on
dense flows, PCF-Net can work with various feature descriptors when quantifying
the reliability of PCFs by confidence maps. An interesting observation of this
work is that the learned confidence map converges to geometrically coherent and
semantically consistent regions, which facilitates robust coordinate
representation. By delivering the confident coordinates to keypoint/feature
descriptors, we show that PCF-Net can be used as a plug-in to existing
correspondence-dependent approaches. Extensive experiments on both indoor and
outdoor datasets suggest that accurate geometric invariant coordinates help to
achieve the state of the art in several correspondence problems, such as sparse
feature matching, dense image registration, camera pose estimation, and
consistency filtering. Further, the interpretable confidence map predicted by
PCF-Net can also be leveraged to other novel applications from texture transfer
to multi-homography classification.|
|**2023-06-06**|**Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures**|David Loiseaux et.al.|[2306.03801v1](http://arxiv.org/abs/2306.03801v1)|[link](https://github.com/davidlapous/multipers-signed-measure)|Persistent homology (PH) provides topological descriptors for geometric data,
such as weighted graphs, which are interpretable, stable to perturbations, and
invariant under, e.g., relabeling. Most applications of PH focus on the
one-parameter case -- where the descriptors summarize the changes in topology
of data as it is filtered by a single quantity of interest -- and there is now
a wide array of methods enabling the use of one-parameter PH descriptors in
data science, which rely on the stable vectorization of these descriptors as
elements of a Hilbert space. Although the multiparameter PH (MPH) of data that
is filtered by several quantities of interest encodes much richer information
than its one-parameter counterpart, the scarceness of stability results for MPH
descriptors has so far limited the available options for the stable
vectorization of MPH. In this paper, we aim to bring together the best of both
worlds by showing how the interpretation of signed barcodes -- a recent family
of MPH descriptors -- as signed measures leads to natural extensions of
vectorization strategies from one parameter to multiple parameters. The
resulting feature vectors are easy to define and to compute, and provably
stable. While, as a proof of concept, we focus on simple choices of signed
barcodes and vectorizations, we already see notable performance improvements
when comparing our feature vectors to state-of-the-art topology-based methods
on various types of data.|
|**2023-06-06**|**Scalable Concept Extraction in Industry 4.0**|Andrs Felipe Posada-Moreno et.al.|[2306.03551v1](http://arxiv.org/abs/2306.03551v1)|null|The industry 4.0 is leveraging digital technologies and machine learning
techniques to connect and optimize manufacturing processes. Central to this
idea is the ability to transform raw data into human understandable knowledge
for reliable data-driven decision-making. Convolutional Neural Networks (CNNs)
have been instrumental in processing image data, yet, their ``black box''
nature complicates the understanding of their prediction process. In this
context, recent advances in the field of eXplainable Artificial Intelligence
(XAI) have proposed the extraction and localization of concepts, or which
visual cues intervene on the prediction process of CNNs. This paper tackles the
application of concept extraction (CE) methods to industry 4.0 scenarios. To
this end, we modify a recently developed technique, ``Extracting Concepts with
Local Aggregated Descriptors'' (ECLAD), improving its scalability.
Specifically, we propose a novel procedure for calculating concept importance,
utilizing a wrapper function designed for CNNs. This process is aimed at
decreasing the number of times each image needs to be evaluated. Subsequently,
we demonstrate the potential of CE methods, by applying them in three
industrial use cases. We selected three representative use cases in the context
of quality control for material design (tailored textiles), manufacturing
(carbon fiber reinforcement), and maintenance (photovoltaic module inspection).
In these examples, CE was able to successfully extract and locate concepts
directly related to each task. This is, the visual cues related to each
concept, coincided with what human experts would use to perform the task
themselves, even when the visual cues were entangled between multiple classes.
Through empirical results, we show that CE can be applied for understanding
CNNs in an industrial context, giving useful insights that can relate to domain
knowledge.|
|**2023-06-06**|**SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving**|Honghao Fu et.al.|[2306.03538v1](http://arxiv.org/abs/2306.03538v1)|null|To mitigate the challenges arising from partial occlusion in human pose
keypoint based pedestrian detection methods , we present a novel pedestrian
pose keypoint completion method called the separation and dimensionality
reduction-based generative adversarial imputation networks (SDR-GAIN) .
Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we
isolate the head and torso keypoints of pedestrians with incomplete keypoints
due to occlusion or other factors and perform dimensionality reduction to
enhance features and further unify feature distribution. Finally, we introduce
two generative models based on the generative adversarial networks (GAN)
framework, which incorporate Huber loss, residual structure, and L1
regularization to generate missing parts of the incomplete head and torso pose
keypoints of partially occluded pedestrians, resulting in pose completion. Our
experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms
basic GAIN framework, interpolation methods PCHIP and MAkima, machine learning
methods k-NN and MissForest in terms of pose completion task. In addition, the
runtime of SDR-GAIN is approximately 0.4ms, displaying high real-time
performance and significant application value in the field of autonomous
driving.|
|**2023-06-05**|**Scene as Occupancy**|Wenwen Tong et.al.|[2306.02851v2](http://arxiv.org/abs/2306.02851v2)|[link](https://github.com/opendrivelab/occnet)|Human driver can easily describe the complex traffic scene by visual system.
Such an ability of precise perception is essential for driver's planning. To
achieve this, a geometry-aware representation that quantizes the physical 3D
scene into structured grid map with semantic labels per cell, termed as 3D
Occupancy, would be desirable. Compared to the form of bounding box, a key
insight behind occupancy is that it could capture the fine-grained details of
critical obstacles in the scene, and thereby facilitate subsequent tasks. Prior
or concurrent literature mainly concentrate on a single scene completion task,
where we might argue that the potential of this occupancy representation might
obsess broader impact. In this paper, we propose OccNet, a multi-view
vision-centric pipeline with a cascade and temporal voxel decoder to
reconstruct 3D occupancy. At the core of OccNet is a general occupancy
embedding to represent 3D physical world. Such a descriptor could be applied
towards a wide span of driving tasks, including detection, segmentation and
planning. To validate the effectiveness of this new representation and our
proposed algorithm, we propose OpenOcc, the first dense high-quality 3D
occupancy benchmark built on top of nuScenes. Empirical experiments show that
there are evident performance gain across multiple tasks, e.g., motion planning
could witness a collision rate reduction by 15%-58%, demonstrating the
superiority of our method.|
|**2023-06-05**|**A2B: Anchor to Barycentric Coordinate for Robust Correspondence**|Weiyue Zhao et.al.|[2306.02760v2](http://arxiv.org/abs/2306.02760v2)|null|There is a long-standing problem of repeated patterns in correspondence
problems, where mismatches frequently occur because of inherent ambiguity. The
unique position information associated with repeated patterns makes coordinate
representations a useful supplement to appearance representations for improving
feature correspondences. However, the issue of appropriate coordinate
representation has remained unresolved. In this study, we demonstrate that
geometric-invariant coordinate representations, such as barycentric
coordinates, can significantly reduce mismatches between features. The first
step is to establish a theoretical foundation for geometrically invariant
coordinates. We present a seed matching and filtering network (SMFNet) that
combines feature matching and consistency filtering with a coarse-to-fine
matching strategy in order to acquire reliable sparse correspondences. We then
introduce DEGREE, a novel anchor-to-barycentric (A2B) coordinate encoding
approach, which generates multiple affine-invariant correspondence coordinates
from paired images. DEGREE can be used as a plug-in with standard descriptors,
feature matchers, and consistency filters to improve the matching quality.
Extensive experiments in synthesized indoor and outdoor datasets demonstrate
that DEGREE alleviates the problem of repeated patterns and helps achieve
state-of-the-art performance. Furthermore, DEGREE also reports competitive
performance in the third Image Matching Challenge at CVPR 2021. This approach
offers a new perspective to alleviate the problem of repeated patterns and
emphasizes the importance of choosing coordinate representations for feature
correspondences.|
|**2023-06-04**|**Exploring Model Complexity in Machine Learned Potentials for Simulated Properties**|Andrew Rohskopf et.al.|[2306.02255v1](http://arxiv.org/abs/2306.02255v1)|null|Machine learning (ML) enables the development of interatomic potentials that
promise the accuracy of first principles methods while retaining the low cost
and parallel efficiency of empirical potentials. While ML potentials
traditionally use atom-centered descriptors as inputs, different models such as
linear regression and neural networks can map these descriptors to atomic
energies and forces. This begs the question: what is the improvement in
accuracy due to model complexity irrespective of choice of descriptors? We
curate three datasets to investigate this question in terms of ab initio energy
and force errors: (1) solid and liquid silicon, (2) gallium nitride, and (3)
the superionic conductor LGPS. We further investigate how these errors affect
simulated properties with these models and verify if the improvement in fitting
errors corresponds to measurable improvement in property prediction. Since
linear and nonlinear regression models have different advantages and
disadvantages, the results presented herein help researchers choose models for
their particular application. By assessing different models, we observe
correlations between fitting quantity (e.g. atomic force) error and simulated
property error with respect to ab initio values. Such observations can be
repeated by other researchers to determine the level of accuracy, and hence
model complexity, needed for their particular systems of interest.|
|**2023-06-03**|**LDEB -- Label Digitization with Emotion Binarization and Machine Learning for Emotion Recognition in Conversational Dialogues**|Amitabha Dey et.al.|[2306.02193v1](http://arxiv.org/abs/2306.02193v1)|null|Emotion recognition in conversations (ERC) is vital to the advancements of
conversational AI and its applications. Therefore, the development of an
automated ERC model using the concepts of machine learning (ML) would be
beneficial. However, the conversational dialogues present a unique problem
where each dialogue depicts nested emotions that entangle the association
between the emotional feature descriptors and emotion type (or label). This
entanglement that can be multiplied with the presence of data paucity is an
obstacle for a ML model. To overcome this problem, we proposed a novel approach
called Label Digitization with Emotion Binarization (LDEB) that disentangles
the twists by utilizing the text normalization and 7-bit digital encoding
techniques and constructs a meaningful feature space for a ML model to be
trained. We also utilized the publicly available dataset called the
FETA-DailyDialog dataset for feature learning and developed a hierarchical ERC
model using random forest (RF) and artificial neural network (ANN) classifiers.
Simulations showed that the ANN-based ERC model was able to predict emotion
with the best accuracy and precision scores of about 74% and 76%, respectively.
Simulations also showed that the ANN-model could reach a training accuracy
score of about 98% with 60 epochs. On the other hand, the RF-based ERC model
was able to predict emotions with the best accuracy and precision scores of
about 78% and 75%, respectively.|
|**2023-06-03**|**Context-TAP: Tracking Any Point Demands Spatial Context Features**|Weikang Bian et.al.|[2306.02000v1](http://arxiv.org/abs/2306.02000v1)|null|We tackle the problem of Tracking Any Point (TAP) in videos, which
specifically aims at estimating persistent long-term trajectories of query
points in videos. Previous methods attempted to estimate these trajectories
independently to incorporate longer image sequences, therefore, ignoring the
potential benefits of incorporating spatial context features. We argue that
independent video point tracking also demands spatial context features. To this
end, we propose a novel framework Context-TAP, which effectively improves point
trajectory accuracy by aggregating spatial context features in videos.
Context-TAP contains two main modules: 1) a SOurse Feature Enhancement (SOFE)
module, and 2) a TArget Feature Aggregation (TAFA) module. Context-TAP
significantly improves PIPs all-sided, reducing 11.4% Average Trajectory Error
of Occluded Points (ATE-Occ) on CroHD and increasing 11.8% Average Percentage
of Correct Keypoint (A-PCK) on TAP-Vid-Kinectics. Demos are available at this
$\href{https://wkbian.github.io/Projects/Context-TAP/}{webpage}$.|
|**2023-06-02**|**Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images**|Marcela Mera-Trujillo et.al.|[2306.01938v1](http://arxiv.org/abs/2306.01938v1)|null|Keypoint detection and matching is a fundamental task in many computer vision
problems, from shape reconstruction, to structure from motion, to AR/VR
applications and robotics. It is a well-studied problem with remarkable
successes such as SIFT, and more recent deep learning approaches. While great
robustness is exhibited by these techniques with respect to noise, illumination
variation, and rigid motion transformations, less attention has been placed on
image distortion sensitivity. In this work, we focus on the case when this is
caused by the geometry of the cameras used for image acquisition, and consider
the keypoint detection and matching problem between the hybrid scenario of a
fisheye and a projective image. We build on a state-of-the-art approach and
derive a self-supervised procedure that enables training an interest point
detector and descriptor network. We also collected two new datasets for
additional training and testing in this unexplored scenario, and we demonstrate
that current approaches are suboptimal because they are designed to work in
traditional projective conditions, while the proposed approach turns out to be
the most effective.|
|**2023-06-02**|**Unifying (Machine) Vision via Counterfactual World Modeling**|Daniel M. Bear et.al.|[2306.01828v1](http://arxiv.org/abs/2306.01828v1)|null|Leading approaches in machine vision employ different architectures for
different tasks, trained on costly task-specific labeled datasets. This
complexity has held back progress in areas, such as robotics, where robust
task-general perception remains a bottleneck. In contrast, "foundation models"
of natural language have shown how large pre-trained neural networks can
provide zero-shot solutions to a broad spectrum of apparently distinct tasks.
Here we introduce Counterfactual World Modeling (CWM), a framework for
constructing a visual foundation model: a unified, unsupervised network that
can be prompted to perform a wide variety of visual computations. CWM has two
key components, which resolve the core issues that have hindered application of
the foundation model concept to vision. The first is structured masking, a
generalization of masked prediction methods that encourages a prediction model
to capture the low-dimensional structure in visual data. The model thereby
factors the key physical components of a scene and exposes an interface to them
via small sets of visual tokens. This in turn enables CWM's second main idea --
counterfactual prompting -- the observation that many apparently distinct
visual representations can be computed, in a zero-shot manner, by comparing the
prediction model's output on real inputs versus slightly modified
("counterfactual") inputs. We show that CWM generates high-quality readouts on
real-world images and videos for a diversity of tasks, including estimation of
keypoints, optical flow, occlusions, object segments, and relative depth. Taken
together, our results show that CWM is a promising path to unifying the
manifold strands of machine vision in a conceptually simple foundation.|
|**2023-06-01**|**Labeled Interleaving Distance for Reeb Graphs**|Fangfei Lan et.al.|[2306.01186v1](http://arxiv.org/abs/2306.01186v1)|null|Merge trees, contour trees, and Reeb graphs are graph-based topological
descriptors that capture topological changes of (sub)level sets of scalar
fields. Comparing scalar fields using their topological descriptors has many
applications in topological data analysis and visualization of scientific data.
Recently, Munch and Stefanou introduced a labeled interleaving distance for
comparing two labeled merge trees, which enjoys a number of theoretical and
algorithmic properties. In particular, the labeled interleaving distance
between merge trees can be computed in polynomial time. In this work, we define
the labeled interleaving distance for labeled Reeb graphs. We then prove that
the (ordinary) interleaving distance between Reeb graphs equals the minimum of
the labeled interleaving distance over all labelings. We also provide an
efficient algorithm for computing the labeled interleaving distance between two
labeled contour trees (which are special types of Reeb graphs that arise from
simply-connected domains). In the case of merge trees, the notion of the
labeled interleaving distance was used by Gasparovic et al. to prove that the
(ordinary) interleaving distance on the set of (unlabeled) merge trees is
intrinsic. As our final contribution, we present counterexamples showing that,
on the contrary, the (ordinary) interleaving distance on (unlabeled) Reeb
graphs (and contour trees) is not intrinsic. It turns out that, under mild
conditions on the labelings, the labeled interleaving distance is a metric on
isomorphism classes of Reeb graphs, analogous to the ordinary interleaving
distance. This provides new metrics on large classes of Reeb graphs.|
|**2023-06-01**|**Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints**|Jiachen Li et.al.|[2306.01075v1](http://arxiv.org/abs/2306.01075v1)|null|Accurate understanding and prediction of human behaviors are critical
prerequisites for autonomous vehicles, especially in highly dynamic and
interactive scenarios such as intersections in dense urban areas. In this work,
we aim at identifying crossing pedestrians and predicting their future
trajectories. To achieve these goals, we not only need the context information
of road geometry and other traffic participants but also need fine-grained
information of the human pose, motion and activity, which can be inferred from
human keypoints. In this paper, we propose a novel multi-task learning
framework for pedestrian crossing action recognition and trajectory prediction,
which utilizes 3D human keypoints extracted from raw sensor data to capture
rich information on human pose and activity. Moreover, we propose to apply two
auxiliary tasks and contrastive learning to enable auxiliary supervisions to
improve the learned keypoints representation, which further enhances the
performance of major tasks. We validate our approach on a large-scale in-house
dataset, as well as a public benchmark dataset, and show that our approach
achieves state-of-the-art performance on a wide range of evaluation metrics.
The effectiveness of each model component is validated in a detailed ablation
study.|
|**2023-06-01**|**A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm**|Onur Beker et.al.|[2306.00892v1](http://arxiv.org/abs/2306.00892v1)|null|Existing object pose estimation methods commonly require a one-to-one point
matching step that forces them to be separated into two consecutive stages:
visual correspondence detection (e.g., by matching feature descriptors as part
of a perception front-end) followed by geometric alignment (e.g., by optimizing
a robust estimation objective for pointcloud registration or
perspective-n-point). Instead, we propose a matching-free probabilistic
formulation with two main benefits: i) it enables unified and concurrent
optimization of both visual correspondence and geometric alignment, and ii) it
can represent different plausible modes of the entire distribution of likely
poses. This in turn allows for a more graceful treatment of geometric
perception scenarios where establishing one-to-one matches between points is
conceptually ill-defined, such as textureless, symmetrical and/or occluded
objects and scenes where the correct pose is uncertain or there are multiple
equally valid solutions.|
|**2023-05-31**|**The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture**|Amanda A. Boatswain Jacques et.al.|[2306.00114v2](http://arxiv.org/abs/2306.00114v2)|null|Monitoring land cover using remote sensing is vital for studying
environmental changes and ensuring global food security through crop yield
forecasting. Specifically, multitemporal remote sensing imagery provides
relevant information about the dynamics of a scene, which has proven to lead to
better land cover classification results. Nevertheless, few studies have
benefited from high spatial and temporal resolution data due to the difficulty
of accessing reliable, fine-grained and high-quality annotated samples to
support their hypotheses. Therefore, we introduce a temporal patch-based
dataset of Canadian croplands, enriched with labels retrieved from the Canadian
Annual Crop Inventory. The dataset contains 78,536 manually verified
high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop
classes collected over four crop production years (2017-2020) and five months
(June-October). Each instance contains 12 spectral bands, an RGB image, and
additional vegetation index bands. Individually, each category contains at
least 4,800 images. Moreover, as a benchmark, we provide models and source code
that allow a user to predict the crop class using a single image (ResNet,
DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the same
location. In perspective, we expect this evolving dataset to propel the
creation of robust agro-environmental models that can accelerate the
comprehension of complex agricultural regions by providing accurate and
continuous monitoring of land cover.|
|**2023-05-31**|**Atom-by-atom design of metal oxide catalysts for the oxygen evolution reaction with machine learning**|Jaclyn R. Lunger et.al.|[2305.19930v1](http://arxiv.org/abs/2305.19930v1)|[link](https://github.com/learningmatter-mit/atom_by_atom)|Green hydrogen production is crucial for a sustainable future, but current
catalysts for the oxygen evolution reaction (OER) suffer from slow kinetics,
despite many efforts to produce optimal designs, particularly through the
calculation of descriptors for activity. In this study, we develop a dataset of
density functional theory calculations of bulk and surface perovskite oxides,
and adsorption energies of OER intermediates, which includes compositions up to
quaternary and facets up to (555). We demonstrate that per-site properties of
perovskite oxides such as Bader charge or band center can be tuned through
element substitution and faceting, and develop a machine learning model that
accurately predicts these properties directly from the local chemical
environment. We leverage these per-site properties to identify promising
perovskites with high theoretical OER activity. The identified design
principles and promising new materials provide a roadmap for closing the gap
between current artificial catalysts and biological enzymes.|
|**2023-05-31**|**Combining first-principles modeling and symbolic regression for designing efficient single-atom catalysts in Oxygen Evolution Reaction on Mo$_2$CO$_2$ MXenes**|Swetarekha Ram et.al.|[2305.19551v2](http://arxiv.org/abs/2305.19551v2)|null|In this study, we address the significant challenge of overcoming limitations
in catalytic efficiency for the oxygen evolution reaction (OER). The current
linear scaling relationships hinder the optimization of electrocatalytic
performance. To tackle this issue, we investigate the potential of designing
single-atom catalysts (SACs) on Mo$_2$CO$_2$ MXenes for electrochemical OER
using first-principles modeling simulations. By employing the Electrochemical
Step Symmetry Index (ESSI) method, we assess OER intermediates to fine-tune
activity and identify the optimal SAC for Mo$_2$CO$_2$ MXenes. Our findings
reveal that both Ag and Cu exhibit effectiveness as single atoms for enhancing
OER activity on Mo$_2$CO$_2$ MXenes. However, among the 21 chosen transition
metals (TMs) in this study, Cu stands out as the best catalyst for tweaking the
overpotential ($\eta_{OER}$). This is due to Cu's lowest overpotential compared
to other TMs, which makes it more favorable for OER performance. On the other
hand, Ag is closely aligned with ESSI=$\eta_{OER}$, making the tuning of its
overpotential more challenging. Furthermore, we employ symbolic regression
analysis to identify the significant factors that exhibit a correlation with
the OER overpotential. By utilizing this approach, we derive mathematical
formulas for the overpotential and identify key descriptors that affect
catalytic efficiency in electrochemical OER on Mo$_2$CO$_2$ MXenes. This
comprehensive investigation not only sheds light on the potential of MXenes in
advanced electrocatalytic processes but also highlights the prospect of
improved activity and selectivity in OER applications.|
|**2023-05-31**|**Learning by Aligning 2D Skeleton Sequences in Time**|Quoc-Huy Tran et.al.|[2305.19480v1](http://arxiv.org/abs/2305.19480v1)|null|This paper presents a novel self-supervised temporal video alignment
framework which is useful for several fine-grained human activity understanding
applications. In contrast with the state-of-the-art method of CASA, where
sequences of 3D skeleton coordinates are taken directly as input, our key idea
is to use sequences of 2D skeleton heatmaps as input. Given 2D skeleton
heatmaps, we utilize a video transformer which performs self-attention in the
spatial and temporal domains for extracting effective spatiotemporal and
contextual features. In addition, we introduce simple heatmap augmentation
techniques based on 2D skeletons for self-supervised learning. Despite the lack
of 3D information, our approach achieves not only higher accuracy but also
better robustness against missing and noisy keypoints than CASA. Extensive
evaluations on three public datasets, i.e., Penn Action, IKEA ASM, and H2O,
demonstrate that our approach outperforms previous methods in different
fine-grained human activity understanding tasks, i.e., phase classification,
phase progression, video alignment, and fine-grained frame retrieval.|
|**2023-05-30**|**Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training**|Wenshuo Chen et.al.|[2305.18743v2](http://arxiv.org/abs/2305.18743v2)|null|Estimating human pose from video is a task that receives considerable
attention due to its applicability in numerous 3D fields. The complexity of
prior knowledge of human body movements poses a challenge to neural network
models in the task of regressing keypoints. In this paper, we address this
problem by incorporating motion prior in an adversarial way. Different from
previous methods, we propose to decompose holistic motion prior to joint motion
prior, making it easier for neural networks to learn from prior knowledge
thereby boosting the performance on the task. We also utilize a novel
regularization loss to balance accuracy and smoothness introduced by motion
prior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration error
than previous methods tested on 3DPW. The estimator proves its robustness by
achieving impressive performance on in-the-wild dataset.|
|**2023-05-30**|**Align, Perturb and Decouple: Toward Better Leverage of Difference Information for RSI Change Detection**|Supeng Wang et.al.|[2305.18714v1](http://arxiv.org/abs/2305.18714v1)|[link](https://github.com/wangsp1999/cd-research)|Change detection is a widely adopted technique in remote sense imagery (RSI)
analysis in the discovery of long-term geomorphic evolution. To highlight the
areas of semantic changes, previous effort mostly pays attention to learning
representative feature descriptors of a single image, while the difference
information is either modeled with simple difference operations or implicitly
embedded via feature interactions. Nevertheless, such difference modeling can
be noisy since it suffers from non-semantic changes and lacks explicit guidance
from image content or context. In this paper, we revisit the importance of
feature difference for change detection in RSI, and propose a series of
operations to fully exploit the difference information: Alignment, Perturbation
and Decoupling (APD). Firstly, alignment leverages contextual similarity to
compensate for the non-semantic difference in feature space. Next, a difference
module trained with semantic-wise perturbation is adopted to learn more
generalized change estimators, which reversely bootstraps feature extraction
and prediction. Finally, a decoupled dual-decoder structure is designed to
predict semantic changes in both content-aware and content-agnostic manners.
Extensive experiments are conducted on benchmarks of LEVIR-CD, WHU-CD and
DSIFN-CD, demonstrating our proposed operations bring significant improvement
and achieve competitive results under similar comparative conditions. Code is
available at https://github.com/wangsp1999/CD-Research/tree/main/openAPD|
|**2023-05-29**|**Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance**|Supriya Gadi Patil et.al.|[2305.18557v1](http://arxiv.org/abs/2305.18557v1)|null|This paper analyzes the robustness of recent 3D shape descriptors to SO(3)
rotations, something that is fundamental to shape modeling. Specifically, we
formulate the task of rotated 3D object instance detection. To do so, we
consider a database of 3D indoor scenes, where objects occur in different
orientations. We benchmark different methods for feature extraction and
classification in the context of this task. We systematically contrast
different choices in a variety of experimental settings investigating the
impact on the performance of different rotation distributions, different
degrees of partial observations on the object, and the different levels of
difficulty of negative pairs. Our study, on a synthetic dataset of 3D scenes
where objects instances occur in different orientations, reveals that deep
learning-based rotation invariant methods are effective for relatively easy
settings with easy-to-distinguish pairs. However, their performance decreases
significantly when the difference in rotations on the input pair is large, or
when the degree of observation of input objects is reduced, or the difficulty
level of input pair is increased. Finally, we connect feature encodings
designed for rotation-invariant methods to 3D geometry that enable them to
acquire the property of rotation invariance.|
|**2023-05-29**|**Human Body Shape Classification Based on a Single Image**|Cameron Trotter et.al.|[2305.18480v1](http://arxiv.org/abs/2305.18480v1)|null|There is high demand for online fashion recommender systems that incorporate
the needs of the consumer's body shape. As such, we present a methodology to
classify human body shape from a single image. This is achieved through the use
of instance segmentation and keypoint estimation models, trained only on
open-source benchmarking datasets. The system is capable of performing in noisy
environments owing to to robust background subtraction. The proposed
methodology does not require 3D body recreation as a result of classification
based on estimated keypoints, nor requires historical information about a user
to operate - calculating all required measurements at the point of use. We
evaluate our methodology both qualitatively against existing body shape
classifiers and quantitatively against a novel dataset of images, which we
provide for use to the community. The resultant body shape classification can
be utilised in a variety of downstream tasks, such as input to size and fit
recommendation or virtual try-on systems.|
|**2023-05-29**|**TReR: A Lightweight Transformer Re-Ranking Approach for 3D LiDAR Place Recognition**|Tiago Barros et.al.|[2305.18013v1](http://arxiv.org/abs/2305.18013v1)|null|Autonomous driving systems often require reliable loop closure detection to
guarantee reduced localization drift. Recently, 3D LiDAR-based localization
methods have used retrieval-based place recognition to find revisited places
efficiently. However, when deployed in challenging real-world scenarios, the
place recognition models become more complex, which comes at the cost of high
computational demand. This work tackles this problem from an
information-retrieval perspective, adopting a first-retrieve-then-re-ranking
paradigm, where an initial loop candidate ranking, generated from a 3D place
recognition model, is re-ordered by a proposed lightweight transformer-based
re-ranking approach (TReR). The proposed approach relies on global descriptors
only, being agnostic to the place recognition model. The experimental
evaluation, conducted on the KITTI Odometry dataset, where we compared TReR
with s.o.t.a. re-ranking approaches such as alphaQE and SGV, indicate the
robustness and efficiency when compared to alphaQE while offering a good
trade-off between robustness and efficiency when compared to SGV.|
|**2023-05-29**|**Deep Electron Cloud-activity and Field-activity Relationships**|Lu Xu et.al.|[2305.17958v1](http://arxiv.org/abs/2305.17958v1)|null|Chemists have been pursuing the general mathematical laws to explain and
predict molecular properties for a long time. However, most of the traditional
quantitative structure-activity relationship (QSAR) models have limited
application domains, e.g., they tend to have poor generalization performance
when applied to molecules with parent structures different from those of the
trained molecules. This paper attempts to develop a new QSAR method that is
theoretically possible to predict various properties of molecules with diverse
structures. The proposed deep electron cloud-activity relationships (DECAR) and
deep field-activity relationships (DFAR) methods consist of three essentials:
(1) A large number of molecule entities with activity data as training objects
and responses; (2) three-dimensional electron cloud density (ECD) or related
field data by the accurate density functional theory methods as input
descriptors; (3) a deep learning model that is sufficiently flexible and
powerful to learn the large data described above. DECAR and DFAR are used to
distinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data
augmentation) and the classification performance is demonstrated to be
significantly better than the traditional least squares support vector machine
(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a
possible way to establish a widely applicable, cumulative, and shareable
artificial intelligence-driven QSAR system. They are likely to promote the
development of an interactive platform to collect and share the accurate ECD
and field data of millions of molecules with annotated activities. With enough
input data, we envision the appearance of several deep networks trained for
various molecular activities. Finally, we could anticipate a single DECAR or
DFAR network to learn and infer various properties of interest for chemical
molecules.|
|**2023-05-27**|**Text-to-image Editing by Image Information Removal**|Zhongping Zhang et.al.|[2305.17489v1](http://arxiv.org/abs/2305.17489v1)|null|Diffusion models have demonstrated impressive performance in text-guided
image generation. To leverage the knowledge of text-guided image generation
models in image editing, current approaches either fine-tune the pretrained
models using the input image (e.g., Imagic) or incorporate structure
information as additional constraints into the pretrained models (e.g.,
ControlNet). However, fine-tuning large-scale diffusion models on a single
image can lead to severe overfitting issues and lengthy inference time. The
information leakage from pretrained models makes it challenging to preserve the
text-irrelevant content of the input image while generating new features guided
by language descriptions. On the other hand, methods that incorporate
structural guidance (e.g., edge maps, semantic maps, keypoints) as additional
constraints face limitations in preserving other attributes of the original
image, such as colors or textures. A straightforward way to incorporate the
original image is to directly use it as an additional control. However, since
image editing methods are typically trained on the image reconstruction task,
the incorporation can lead to the identical mapping issue, where the model
learns to output an image identical to the input, resulting in limited editing
capabilities. To address these challenges, we propose a text-to-image editing
model with Image Information Removal module (IIR) to selectively erase
color-related and texture-related information from the original image, allowing
us to better preserve the text-irrelevant content and avoid the identical
mapping issue. We evaluate our model on three benchmark datasets: CUB, Outdoor
Scenes, and COCO. Our approach achieves the best editability-fidelity
trade-off, and our edited images are approximately 35% more preferred by
annotators than the prior-arts on COCO.|

### Instance Segmentation
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**Improving the Reporting of Threats to Construct Validity**|Dag I. K. Sjberg et.al.|[2306.05336v1](http://arxiv.org/abs/2306.05336v1)|null|Background: Construct validity concerns the use of indicators to measure a
concept that is not directly measurable. Aim: This study intends to identify,
categorize, assess and quantify discussions of threats to construct validity in
empirical software engineering literature and use the findings to suggest ways
to improve the reporting of construct validity issues. Method: We analyzed 83
articles that report human-centric experiments published in five top-tier
software engineering journals from 2015 to 2019. The articles' text concerning
threats to construct validity was divided into segments (the unit of analysis)
based on predefined categories. The segments were then evaluated regarding
whether they clearly discussed a threat and a construct. Results: Three-fifths
of the segments were associated with topics not related to construct validity.
Two-thirds of the articles discussed construct validity without using the
definition of construct validity given in the article. The threats were clearly
described in more than four-fifths of the segments, but the construct in
question was clearly described in only two-thirds of the segments. The
construct was unclear when the discussion was not related to construct validity
but to other types of validity. Conclusions: The results show potential for
improving the understanding of construct validity in software engineering.
Recommendations addressing the identified weaknesses are given to improve the
awareness and reporting of CV.|
|**2023-06-08**|**ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases**|Qiaoyu Tang et.al.|[2306.05301v1](http://arxiv.org/abs/2306.05301v1)|null|Enabling large language models to effectively utilize real-world tools is
crucial for achieving embodied intelligence. Existing approaches to tool
learning have primarily relied on either extremely large language models, such
as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or
have utilized supervised learning to train limited types of tools on compact
models. However, it remains uncertain whether smaller language models can
achieve generalized tool-use abilities without specific tool-specific training.
To address this question, this paper introduces ToolAlpaca, a novel framework
designed to automatically generate a tool-use corpus and learn generalized
tool-use abilities on compact language models with minimal human intervention.
Specifically, ToolAlpaca first collects a comprehensive dataset by building a
multi-agent simulation environment, which contains 3938 tool-use instances from
more than 400 real-world tool APIs spanning 50 distinct categories.
Subsequently, the constructed corpus is employed to fine-tune compact language
models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,
respectively. Finally, we evaluate the ability of these models to utilize
previously unseen tools without specific training. Experimental results
demonstrate that ToolAlpaca achieves effective generalized tool-use
capabilities comparable to those of extremely large language models like
GPT-3.5. This validation supports the notion that learning generalized tool-use
abilities is feasible for compact language models.|
|**2023-06-08**|**Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation**|Shishuai Hu et.al.|[2306.05254v1](http://arxiv.org/abs/2306.05254v1)|null|Deep learning-based medical image segmentation models suffer from performance
degradation when deployed to a new healthcare center. To address this issue,
unsupervised domain adaptation and multi-source domain generalization methods
have been proposed, which, however, are less favorable for clinical practice
due to the cost of acquiring target-domain data and the privacy concerns
associated with redistributing the data from multiple source domains. In this
paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle
\textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical
image segmentation. In C$^2$SDG, the shallower features of each image and its
style-augmented counterpart are extracted and used for contrastive training,
resulting in the disentangled style representations and structure
representations. The segmentation is performed based solely on the structure
representations. Our method is novel in the contrastive perspective that
enables channel-wise feature disentanglement using a single source domain. We
evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup
and optic disc segmentation benchmark. Our results suggest the effectiveness of
each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the
baseline and all competing methods with a large margin. The code will be
available at \url{https://github.com/ShishuaiHu/CCSDG}.|
|**2023-06-08**|**Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation**|Qiujie Dong et.al.|[2306.05246v1](http://arxiv.org/abs/2306.05246v1)|null|With the rapid development of geometric deep learning techniques, many
mesh-based convolutional operators have been proposed to bridge irregular mesh
structures and popular backbone networks. In this paper, we show that while
convolutions are helpful, a simple architecture based exclusively on
multi-layer perceptrons (MLPs) is competent enough to deal with mesh
classification and semantic segmentation. Our new network architecture, named
Mesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) and
dihedral angles as the input, replaces the convolution module of a ResNet with
Multi-layer Perceptron (MLP), and utilizes layer normalization (LN) to perform
the normalization of the layers. The all-MLP architecture operates in an
end-to-end fashion and does not include a pooling module. Extensive
experimental results on the mesh classification/segmentation tasks validate the
effectiveness of the all-MLP architecture.|
|**2023-06-08**|**Efficient Multi-Task Scene Analysis with RGB-D Transformers**|Shnke Benedikt Fischedick et.al.|[2306.05242v1](http://arxiv.org/abs/2306.05242v1)|[link](https://github.com/tui-nicr/nicr-scene-analysis-datasets)|Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.|
|**2023-06-08**|**Time-Optimal Path Tracking with ISO Safety Guarantees**|Shohei Fujii et.al.|[2306.05197v1](http://arxiv.org/abs/2306.05197v1)|null|One way of ensuring operator's safety during human-robot collaboration is
through Speed and Separation Monitoring (SSM), as defined in ISO standard
ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions:
consider for instance the case when the robot does not move at all, a human
operator can still collide with it by hitting it of her own voluntary motion.
In the SSM framework, it is possible however to minimize harm by requiring
this: \emph{if} a collision ever occurs, then the robot must be in a
\emph{stationary state} (all links have zero velocity) at the time instant of
the collision. In this paper, we propose a time-optimal control policy based on
Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior.
Specifically, we show that: for any robot motion that is strictly faster than
the motion recommended by our policy, there exists a human motion that results
in a collision with the robot in a non-stationary state. Correlatively, we
show, in simulation, that our policy is strictly less conservative than
state-of-the-art safe robot control methods. Additionally, we propose a
parallelization method to reduce the computation time of our pre-computation
phase (down to 0.5 sec, practically), which enables the whole pipeline
(including the pre-computation) to be executed at runtime, nearly in real-time.
Finally, we demonstrate the application of our method in a scenario:
time-optimal, safe control of a 6-dof industrial robot.|
|**2023-06-08**|**Channel prior convolutional attention for medical image segmentation**|Hejun Huang et.al.|[2306.05196v1](http://arxiv.org/abs/2306.05196v1)|null|Characteristics such as low contrast and significant organ shape variations
are often exhibited in medical images. The improvement of segmentation
performance in medical imaging is limited by the generally insufficient
adaptive capabilities of existing attention mechanisms. An efficient Channel
Prior Convolutional Attention (CPCA) method is proposed in this paper,
supporting the dynamic distribution of attention weights in both channel and
spatial dimensions. Spatial relationships are effectively extracted while
preserving the channel prior by employing a multi-scale depth-wise
convolutional module. The ability to focus on informative channels and
important regions is possessed by CPCA. A segmentation network called CPCANet
for medical image segmentation is proposed based on CPCA. CPCANet is validated
on two publicly available datasets. Improved segmentation performance is
achieved by CPCANet while requiring fewer computational resources through
comparisons with state-of-the-art algorithms. Our code is publicly available at
\url{https://github.com/Cuthbert-Huang/CPCANet}.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer**|Zehui Li et.al.|[2306.05143v1](http://arxiv.org/abs/2306.05143v1)|null|Given the increasing volume and quality of genomics data, extracting new
insights requires interpretable machine-learning models. This work presents
Genomic Interpreter: a novel architecture for genomic assay prediction. This
model outperforms the state-of-the-art models for genomic assay prediction
tasks. Our model can identify hierarchical dependencies in genomic sites. This
is achieved through the integration of 1D-Swin, a novel Transformer-based block
designed by us for modelling long-range hierarchical data. Evaluated on a
dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter
demonstrates superior performance in chromatin accessibility and gene
expression prediction and unmasks the underlying `syntax' of gene regulation.|
|**2023-06-08**|**Does Image Anonymization Impact Computer Vision Training?**|Hkon Hukkels et.al.|[2306.05135v1](http://arxiv.org/abs/2306.05135v1)|[link](https://github.com/hukkelas/deep_privacy2)|Image anonymization is widely adapted in practice to comply with privacy
regulations in many regions. However, anonymization often degrades the quality
of the data, reducing its utility for computer vision development. In this
paper, we investigate the impact of image anonymization for training computer
vision models on key computer vision tasks (detection, instance segmentation,
and pose estimation). Specifically, we benchmark the recognition drop on common
detection datasets, where we evaluate both traditional and realistic
anonymization for faces and full bodies. Our comprehensive experiments reflect
that traditional image anonymization substantially impacts final model
performance, particularly when anonymizing the full body. Furthermore, we find
that realistic anonymization can mitigate this decrease in performance, where
our experiments reflect a minimal performance drop for face anonymization. Our
study demonstrates that realistic anonymization can enable privacy-preserving
computer vision development with minimal performance degradation across a range
of important computer vision benchmarks.|
|**2023-06-08**|**Unsupervised augmentation optimization for few-shot medical image segmentation**|Quan Quan et.al.|[2306.05107v1](http://arxiv.org/abs/2306.05107v1)|null|The augmentation parameters matter to few-shot semantic segmentation since
they directly affect the training outcome by feeding the networks with varying
perturbated samples. However, searching optimal augmentation parameters for
few-shot segmentation models without annotations is a challenge that current
methods fail to address. In this paper, we first propose a framework to
determine the ``optimal'' parameters without human annotations by solving a
distribution-matching problem between the intra-instance and intra-class
similarity distribution, with the intra-instance similarity describing the
similarity between the original sample of a particular anatomy and its
augmented ones and the intra-class similarity representing the similarity
between the selected sample and the others in the same class. Extensive
experiments demonstrate the superiority of our optimized augmentation in
boosting few-shot segmentation models. We greatly improve the top competing
method by 1.27\% and 1.11\% on Abd-MRI and Abd-CT datasets, respectively, and
even achieve a significant improvement for SSL-ALP on the left kidney by 3.39\%
on the Abd-CT dataset.|
|**2023-06-08**|**TransTIC: Transferring Transformer-based Image Compression from Human Visualization to Machine Perception**|Yi-Hsin Chen et.al.|[2306.05085v1](http://arxiv.org/abs/2306.05085v1)|null|This work aims for transferring a Transformer-based image compression codec
from human vision to machine perception without fine-tuning the codec. We
propose a transferable Transformer-based image compression framework, termed
TransTIC. Inspired by visual prompt tuning, we propose an instance-specific
prompt generator to inject instance-specific prompts to the encoder and
task-specific prompts to the decoder. Extensive experiments show that our
proposed method is capable of transferring the codec to various machine tasks
and outshining the competing methods significantly. To our best knowledge, this
work is the first attempt to utilize prompting on the low-level image
compression task.|
|**2023-06-08**|**Improving Visual Prompt Tuning for Self-supervised Vision Transformers**|Seungryong Yoo et.al.|[2306.05067v1](http://arxiv.org/abs/2306.05067v1)|[link](https://github.com/ryongithub/gatedprompttuning)|Visual Prompt Tuning (VPT) is an effective tuning method for adapting
pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra
learnable tokens, known as prompts, which steer the frozen pretrained ViTs.
Although VPT has demonstrated its applicability with supervised vision
transformers, it often underperforms with self-supervised ones. Through
empirical observations, we deduce that the effectiveness of VPT hinges largely
on the ViT blocks with which the prompt tokens interact. Specifically, VPT
shows improved performance on image classification tasks for MAE and MoCo v3
when the prompt tokens are inserted into later blocks rather than the first
block. These observations suggest that there exists an optimal location of
blocks for the insertion of prompt tokens. Unfortunately, identifying the
optimal blocks for prompts within each self-supervised ViT for diverse future
scenarios is a costly process. To mitigate this problem, we propose a simple
yet effective method that learns a gate for each ViT block to adjust its
intervention into the prompt tokens. With our method, prompt tokens are
selectively influenced by blocks that require steering for task adaptation. Our
method outperforms VPT variants in FGVC and VTAB image classification and
ADE20K semantic segmentation. The code is available at
https://github.com/ryongithub/GatedPromptTuning.|
|**2023-06-08**|**Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization**|Cheng Deng et.al.|[2306.05064v1](http://arxiv.org/abs/2306.05064v1)|[link](https://github.com/davendw49/k2)|Large language models (LLMs)have achieved great success in general domains of
natural language processing. In this paper, we bring LLMs to the realm of
geoscience, with the objective of advancing research and applications in this
field. To this end, we present the first-ever LLM in geoscience, K2, alongside
a suite of resources developed to further promote LLM research within
geoscience. For instance, we have curated the first geoscience instruction
tuning dataset, GeoSignal, which aims to align LLM responses to
geoscience-related user queries. Additionally, we have established the first
geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of
geoscience. In this work, we experiment with a complete recipe to adapt a
pretrained general-domain LLM to the geoscience domain. Specifically, we
further train the LLaMA-7B model on over 1 million pieces of geoscience
literature and utilize GeoSignal's supervised data to fine-tune the model.
Moreover, we share a protocol that can efficiently gather domain-specific data
and construct domain-supervised data, even in situations where manpower is
scarce. Experiments conducted on the GeoBenchmark demonstrate the the
effectiveness of our approach and datasets.|
|**2023-06-08**|**A Dynamic Feature Interaction Framework for Multi-task Visual Perception**|Yuling Xi et.al.|[2306.05061v1](http://arxiv.org/abs/2306.05061v1)|null|Multi-task visual perception has a wide range of applications in scene
understanding such as autonomous driving. In this work, we devise an efficient
unified framework to solve multiple common perception tasks, including instance
segmentation, semantic segmentation, monocular 3D detection, and depth
estimation. Simply sharing the same visual feature representations for these
tasks impairs the performance of tasks, while independent task-specific feature
extractors lead to parameter redundancy and latency. Thus, we design two
feature-merge branches to learn feature basis, which can be useful to, and thus
shared by, multiple perception tasks. Then, each task takes the corresponding
feature basis as the input of the prediction task head to fulfill a specific
task. In particular, one feature merge branch is designed for instance-level
recognition the other for dense predictions. To enhance inter-branch
communication, the instance branch passes pixel-wise spatial information of
each instance to the dense branch using efficient dynamic convolution
weighting. Moreover, a simple but effective dynamic routing mechanism is
proposed to isolate task-specific features and leverage common properties among
tasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to
parameter-efficient predictions for multi-task perception. In addition, as
tasks benefit from co-training with each other, our solution achieves on par
results on partially labeled settings on nuScenes and outperforms previous
works for 3D detection and depth estimation on the Cityscapes dataset with full
supervision.|
|**2023-06-08**|**Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification**|Ruijie Zhang et.al.|[2306.05029v1](http://arxiv.org/abs/2306.05029v1)|[link](https://github.com/hustvl/mmil-transformer)|Whole slide image (WSI) refers to a type of high-resolution scanned tissue
image, which is extensively employed in computer-assisted diagnosis (CAD). The
extremely high resolution and limited availability of region-level annotations
make it challenging to employ deep learning methods for WSI-based digital
diagnosis. Multiple instance learning (MIL) is a powerful tool to address the
weak annotation problem, while Transformer has shown great success in the field
of visual tasks. The combination of both should provide new insights for deep
learning based image diagnosis. However, due to the limitations of single-level
MIL and the attention mechanism's constraints on sequence length, directly
applying Transformer to WSI-based MIL tasks is not practical. To tackle this
issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer)
approach. By introducing a hierarchical structure to MIL, this approach enables
efficient handling of MIL tasks that involve a large number of instances. To
validate its effectiveness, we conducted a set of experiments on WSIs
classification task, where MMIL-Transformer demonstrate superior performance
compared to existing state-of-the-art methods. Our proposed approach achieves
test AUC 94.74% and test accuracy 93.41% on CAMELYON16 dataset, test AUC 99.04%
and test accuracy 94.37% on TCGA-NSCLC dataset, respectively. All code and
pre-trained models are available at: https://github.com/hustvl/MMIL-Transformer|
|**2023-06-08**|**The hadronic equation of state of HESS J1731-347 from the relativistic mean-field model with tensor coupling**|Kaixuan Huang et.al.|[2306.04992v1](http://arxiv.org/abs/2306.04992v1)|null|A recent report has identified a central compact object (CCO) within the
supernova remnant HESS J1731-347, with a mass and radius of
$M=0.77^{+0.20}_{-0.17}M{\odot}$ and $R=10.4^{+0.86}_{-0.78}$ km, respectively.
To investigate this light compact star, a density-dependent relativistic
mean-field (DDRMF) model, specifically the DDVT model, has been employed. The
DDVT model incorporates tensor couplings of vector mesons, which {can}
successfully describe the properties of finite nuclei, such as charge radius,
binding energy, and spin-orbit splitting. The introduction of tensor coupling
reduces the influence of scalar mesons and generates a softer equation of state
(EOS) in the outer core of the neutron star. Moreover, it has been found that
the crust segment plays a crucial role in reproducing the mass-radius relation
of HESS J1731-347, indicating a preference for a soft crust EOS. By
manipulating the coupling strength of the isovector meson in the DDVT parameter
set, a reasonable hadronic EOS has been obtained, satisfying the constraints
from the gravitational-wave signal GW170817, the simultaneous mass-radius
measurements from the NICER collaboration, and the properties of finite nuclei.
Notably, the mass-radius relations derived from this hadronic EOS also
accurately describe the observables of HESS J1731-347. Therefore, based on our
estimation, the CCO in HESS J1731-347 may represent the lightest known neutron
star.|
|**2023-06-08**|**Multi-Architecture Multi-Expert Diffusion Models**|Yunsung Lee et.al.|[2306.04990v1](http://arxiv.org/abs/2306.04990v1)|null|Diffusion models have achieved impressive results in generating diverse and
realistic data by employing multi-step denoising processes. However, the need
for accommodating significant variations in input noise at each time-step has
led to diffusion models requiring a large number of parameters for their
denoisers. We have observed that diffusion models effectively act as filters
for different frequency ranges at each time-step noise. While some previous
works have introduced multi-expert strategies, assigning denoisers to different
noise intervals, they overlook the importance of specialized operations for
high and low frequencies. For instance, self-attention operations are effective
at handling low-frequency components (low-pass filters), while convolutions
excel at capturing high-frequency features (high-pass filters). In other words,
existing diffusion models employ denoisers with the same architecture, without
considering the optimal operations for each time-step noise. To address this
limitation, we propose a novel approach called Multi-architecturE Multi-Expert
(MEME), which consists of multiple experts with specialized architectures
tailored to the operations required at each time-step interval. Through
extensive experiments, we demonstrate that MEME outperforms large competitors
in terms of both generation performance and computational efficiency.|
|**2023-06-08**|**Impact of street canyon morphology on heat and fluid flow-an experimental water tunnel study using simultaneous PIV-LIF technique**|Yunpeng Xue et.al.|[2306.04972v1](http://arxiv.org/abs/2306.04972v1)|null|Urban areas are known for their complex atmospheric environments, with the
building morphology having a significant impact on local climate patterns, air
quality, and overall urban microclimate. Understanding the heat and fluid flow
in complex urban environments is crucial for improving urban climate
resilience. In order to gain a better understanding of the physical processes
in urban areas, specifically in street canyons, we investigated simultaneous
heat and flow fields at high spatial and temporal resolutions using
Laser-induced Fluorescence (LIF) and Particle Image Velocimetry (PIV),
respectively. Our results of heat and flow in low-density urban models with a
site cover ratio and sky view factor of 0.21 and 0.6, respectively, indicate
that the flow is significantly influenced by a combination of factors,
including canyon configuration, the presence of buoyant force, and the
magnitude of the approaching flow. The ventilation rate and heat flux from the
street canyon, which are key factors shaping the urban microclimate, are found
dominated significantly by the street canyon morphology. For instance, changing
the aspect ratio of a street canyon results in a significant change of air
ventilation rate, ranging from as low as 0.02 to as high as 1.5 under the same
flow conditions. Additionally, canyons with high air ventilation rates exhibit
significant heat flux removal at the canyon roof level, which is accurately
described by the local Richardson number. These findings contribute to the
understanding of how the built environment affects the urban microclimate and
how this information can be used to improve the current city design that can be
better equipped to handle the challenges of urbanization and urban climate
resilience in a changing climate.|
|**2023-06-08**|**A Melting Pot of Evolution and Learning**|Moshe Sipper et.al.|[2306.04971v1](http://arxiv.org/abs/2306.04971v1)|null|We survey eight recent works by our group, involving the successful blending
of evolutionary algorithms with machine learning and deep learning: 1. Binary
and Multinomial Classification through Evolutionary Symbolic Regression, 2.
Classy Ensemble: A Novel Ensemble Algorithm for Classification, 3. EC-KitY:
Evolutionary Computation Tool Kit in Python, 4. Evolution of Activation
Functions for Deep Learning-Based Image Classification, 5. Adaptive Combination
of a Genetic Algorithm and Novelty Search for Deep Neuroevolution, 6. An
Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for
Generating Adversarial Instances in Deep Networks, 7. Foiling Explanations in
Deep Neural Networks, 8. Patch of Invisibility: Naturalistic Black-Box
Adversarial Attacks on Object Detectors.|
|**2023-06-08**|**Actively Supervised Clustering for Open Relation Extraction**|Jun Zhao et.al.|[2306.04968v1](http://arxiv.org/abs/2306.04968v1)|null|Current clustering-based Open Relation Extraction (OpenRE) methods usually
adopt a two-stage pipeline. The first stage simultaneously learns relation
representations and assignments. The second stage manually labels several
instances and thus names the relation for each cluster. However, unsupervised
objectives struggle to optimize the model to derive accurate clustering
assignments, and the number of clusters has to be supplied in advance. In this
paper, we present a novel setting, named actively supervised clustering for
OpenRE. Our insight lies in that clustering learning and relation labeling can
be alternately performed, providing the necessary guidance for clustering
without a significant increase in human effort. The key to the setting is
selecting which instances to label. Instead of using classical active labeling
strategies designed for fixed known classes, we propose a new strategy, which
is applicable to dynamically discover clusters of unknown relations.
Experimental results show that our method is able to discover almost all
relational clusters in the data and improve the SOTA methods by 10.3\% and
5.2\%, on two datasets respectively.|
|**2023-06-08**|**Open Set Relation Extraction via Unknown-Aware Training**|Jun Zhao et.al.|[2306.04950v1](http://arxiv.org/abs/2306.04950v1)|null|The existing supervised relation extraction methods have achieved impressive
performance in a closed-set setting, where the relations during both training
and testing remain the same. In a more realistic open-set setting, unknown
relations may appear in the test set. Due to the lack of supervision signals
from unknown relations, a well-performing closed-set relation extractor can
still confidently misclassify them into known relations. In this paper, we
propose an unknown-aware training method, regularizing the model by dynamically
synthesizing negative instances. To facilitate a compact decision boundary,
``difficult'' negative instances are necessary. Inspired by text adversarial
attacks, we adaptively apply small but critical perturbations to original
training instances and thus synthesizing negative instances that are more
likely to be mistaken by the model as known relations. Experimental results
show that this method achieves SOTA unknown relation detection without
compromising the classification of known relations.|
|**2023-06-08**|**Neighborhood Attention Makes the Encoder of ResUNet Stronger for Accurate Road Extraction**|Ali Jamali et.al.|[2306.04947v1](http://arxiv.org/abs/2306.04947v1)|null|In the domain of remote sensing image interpretation, road extraction from
high-resolution aerial imagery has already been a hot research topic. Although
deep CNNs have presented excellent results for semantic segmentation, the
efficiency and capabilities of vision transformers are yet to be fully
researched. As such, for accurate road extraction, a deep semantic segmentation
neural network that utilizes the abilities of residual learning, HetConvs,
UNet, and vision transformers, which is called \texttt{ResUNetFormer}, is
proposed in this letter. The developed \texttt{ResUNetFormer} is evaluated on
various cutting-edge deep learning-based road extraction techniques on the
public Massachusetts road dataset. Statistical and visual results demonstrate
the superiority of the \texttt{ResUNetFormer} over the state-of-the-art CNNs
and vision transformers for segmentation. The code will be made available
publicly at \url{https://github.com/aj1365/ResUNetFormer}.|

### Image Classification
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Improving Negative-Prompt Inversion via Proximal Guidance**|Ligong Han et.al.|[2306.05414v1](http://arxiv.org/abs/2306.05414v1)|[link](https://github.com/phymhan/prompt-to-prompt)|DDIM inversion has revealed the remarkable potential of real image editing
within diffusion-based methods. However, the accuracy of DDIM reconstruction
degrades as larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align
the reconstruction and inversion trajectories with larger CFG scales, enabling
real image editing with cross-attention control. Negative-prompt inversion
(NPI) further offers a training-free closed-form solution of NTI. However, it
may introduce artifacts and is still constrained by DDIM reconstruction
quality. To overcome these limitations, we propose Proximal Negative-Prompt
Inversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI with
a regularization term and reconstruction guidance, which reduces artifacts
while capitalizing on its training-free nature. Our method provides an
efficient and straightforward approach, effectively addressing real image
editing tasks with minimal computational overhead.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**Quantum symmetries in 2+1 dimensions: Carroll, (a)dS-Carroll, Galilei and (a)dS-Galilei**|Tomasz Trzeniewski et.al.|[2306.05409v1](http://arxiv.org/abs/2306.05409v1)|null|There is a surge of research devoted to the formalism and physical
manifestations of non-Lorentzian kinematical symmetries, which focuses
especially on the ones associated with the Galilei and Carroll relativistic
limits (the speed of light taken to infinity or to zero, respectively). The
investigations has also been extended to quantum deformations of the Carrollian
and Galilean symmetries, in the sense of (quantum) Hopf algebras. The case of
2+1 dimensions is particularly worth to study due to both the mathematical
nature of the corresponding (classical) theory of gravity, and the recently
finalized classification of all quantum-deformed algebras of spacetime
isometries. Consequently, the list of all quantum deformations of (anti-)de
Sitter-Carroll algebra is immediately provided by its well-known isomorphism
with either Poincar\'{e} or Euclidean algebra. Quantum contractions from the
(anti-)de Sitter to (anti-)de Sitter-Carroll classification allow to almost
completely recover the latter. One may therefore conjecture that the analogous
contractions from the (anti-)de Sitter to (anti-)de Sitter-Galilei $r$-matrices
provide (almost) all coboundary deformations of (anti-)de Sitter-Galilei
algebra. This scheme is complemented by deriving (Carrollian and Galilean)
quantum contractions of deformations of Poincar\'{e} algebra, leading to
coboundary deformations of Carroll and Galilei algebras.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Resonant Anti-Reflection Metasurface for Infrared Transmission Optics**|John Brewer et.al.|[2306.05405v1](http://arxiv.org/abs/2306.05405v1)|null|A fundamental capability for any transmissive optical component is
anti-reflection, yet this capability is challenging to achieve in a
cost-efficient manner over longer infrared wavelengths. We demonstrate that Mie
resonant nanophotonic structures enhance transmission in Silicon, allowing it
to function as an effective optical material over long-wave infrared
wavelengths. This approach enables a window optic with up to 40\% greater
transmission than equal thickness unpatterned Si. Imaging comparisons with
unpatterned silicon and off-the-shelf Germanium optics are shown, as well as
basic broadband slant edge MTF measurements. Overall, we demonstrate how
Mie-resonant structures can be used to improve optical transmission through
window optics of arbitrary lithographically patternable optical media, and
highlight their possible use in imaging applications.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Bayesian model calibration for diblock copolymer thin film self-assembly using power spectrum of microscopy data**|Lianghao Cao et.al.|[2306.05398v1](http://arxiv.org/abs/2306.05398v1)|null|Identifying parameters of computational models from experimental data, or
model calibration, is fundamental for assessing and improving the
predictability and reliability of computer simulations. In this work, we
propose a method for Bayesian calibration of models that predict morphological
patterns of diblock copolymer (Di-BCP) thin film self-assembly while accounting
for various sources of uncertainties in pattern formation and data acquisition.
This method extracts the azimuthally-averaged power spectrum (AAPS) of the
top-down microscopy characterization of Di-BCP thin film patterns as summary
statistics for Bayesian inference of model parameters via the pseudo-marginal
method. We derive the analytical and approximate form of a conditional
likelihood for the AAPS of image data. We demonstrate that AAPS-based image
data reduction retains the mutual information, particularly on important length
scales, between image data and model parameters while being relatively agnostic
to the aleatoric uncertainties associated with the random long-range disorder
of Di-BCP patterns. Additionally, we propose a phase-informed prior
distribution for Bayesian model calibration. Furthermore, reducing image data
to AAPS enables us to efficiently build surrogate models to accelerate the
proposed Bayesian model calibration procedure. We present the formulation and
training of two multi-layer perceptrons for approximating the
parameter-to-spectrum map, which enables fast integrated likelihood
evaluations. We validate the proposed Bayesian model calibration method through
numerical examples, for which the neural network surrogate delivers a fivefold
reduction of the number of model simulations performed for a single calibration
task.|
|**2023-06-08**|**Modular Visual Question Answering via Code Generation**|Sanjay Subramanian et.al.|[2306.05392v1](http://arxiv.org/abs/2306.05392v1)|[link](https://github.com/sanjayss34/codevqa)|We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Deformation theory for prismatic $G$-displays**|Kazuhiro Ito et.al.|[2306.05361v1](http://arxiv.org/abs/2306.05361v1)|null|We develop the deformation theory for $G$-$\mu$-displays over the prismatic
site of Bhatt-Scholze. As an application, we prove the local representability
of integral local Shimura varieties with hyperspecial level structure. We also
revisit and extend some classification results of $p$-divisible groups, using
our deformation theory.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**|Ge Yuan et.al.|[2306.05356v1](http://arxiv.org/abs/2306.05356v1)|[link](https://github.com/ygtxr1997/reliableswap)|Almost all advanced face swapping approaches use reconstruction as the proxy
task, i.e., supervision only exists when the target and source belong to the
same person. Otherwise, lacking pixel-level supervision, these methods struggle
for source identity preservation. This paper proposes to construct reliable
supervision, dubbed cycle triplets, which serves as the image-level guidance
when the source identity differs from the target one during training.
Specifically, we use face reenactment and blending techniques to synthesize the
swapped face from real images in advance, where the synthetic face preserves
source identity and target attributes. However, there may be some artifacts in
such a synthetic face. To avoid the potential artifacts and drive the
distribution of the network output close to the natural one, we reversely take
synthetic images as input while the real face as reliable supervision during
the training stage of face swapping. Besides, we empirically find that the
existing methods tend to lose lower-face details like face shape and mouth from
the source. This paper additionally designs a FixerNet, providing
discriminative embeddings of lower faces as an enhancement. Our face swapping
framework, named ReliableSwap, can boost the performance of any existing face
swapping network with negligible overhead. Extensive experiments demonstrate
the efficacy of our ReliableSwap, especially in identity preservation. The
project page is https://reliable-swap.github.io/.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**A Review of the Recent Developments in the Fabrication Processes of CMOS Image Sensors for Smartphones**|Kirthika Nahalingam et.al.|[2306.05339v1](http://arxiv.org/abs/2306.05339v1)|null|CMOS Image Sensors are experiencing significant growth due to their
capabilities to be integrated in smartphones with refined image quality. One of
the major contributions to the growth of image sensors is the innovation
brought about in their fabrication processes. This paper presents a detailed
review of the different fabrication processes of the CMOS Image Sensors and its
impact on the image quality of smartphone pictures. Fabrication of CMOS image
sensors using wafer bonding technologies such as Through Silicon Vias and CuCu
hybrid bonding along with their experimental results are discussed. A 2 layer
architecture of photodiode and pixel transistors has adopted the 3D sequential
integration, by which the wafers are bonded together one after the other in the
fabrication process. Electrical characteristics and reliability test results
are presented for the former two fabrication processes and the improvements in
the pixels performance such as conversion gain, quantum efficiency, full well
capacity and dynamic range for the 2 layer architecture are discussed.|
|**2023-06-08**|**Actively learning a Bayesian matrix fusion model with deep side information**|Yangyang Yu et.al.|[2306.05331v1](http://arxiv.org/abs/2306.05331v1)|null|High-dimensional deep neural network representations of images and concepts
can be aligned to predict human annotations of diverse stimuli. However, such
alignment requires the costly collection of behavioral responses, such that, in
practice, the deep-feature spaces are only ever sparsely sampled. Here, we
propose an active learning approach to adaptively sampling experimental stimuli
to efficiently learn a Bayesian matrix factorization model with deep side
information. We observe a significant efficiency gain over a passive baseline.
Furthermore, with a sequential batched sampling strategy, the algorithm is
applicable not only to small datasets collected from traditional laboratory
experiments but also to settings where large-scale crowdsourced data collection
is needed to accurately align the high-dimensional deep feature representations
derived from pre-trained networks.|
|**2023-06-08**|**A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments**|Guangyao Zheng et.al.|[2306.05310v1](http://arxiv.org/abs/2306.05310v1)|null|While Deep Reinforcement Learning has been widely researched in medical
imaging, the training and deployment of these models usually require powerful
GPUs. Since imaging environments evolve rapidly and can be generated by edge
devices, the algorithm is required to continually learn and adapt to changing
environments, and adjust to low-compute devices. To this end, we developed
three image coreset algorithms to compress and denoise medical images for
selective experience replayed-based lifelong reinforcement learning. We
implemented neighborhood averaging coreset, neighborhood sensitivity-based
sampling coreset, and maximum entropy coreset on full-body DIXON water and
DIXON fat MRI images. All three coresets produced 27x compression with
excellent performance in localizing five anatomical landmarks: left knee, right
trochanter, left kidney, spleen, and lung across both imaging environments.
Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$
average distance error, compared to the conventional lifelong learning
framework's $19.24\pm 50.77$.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis**|Gongshu Wang et.al.|[2306.05297v1](http://arxiv.org/abs/2306.05297v1)|null|Structural magnetic resonance imaging (sMRI) has shown great clinical value
and has been widely used in deep learning (DL) based computer-aided brain
disease diagnosis. Previous approaches focused on local shapes and textures in
sMRI that may be significant only within a particular domain. The learned
representations are likely to contain spurious information and have a poor
generalization ability in other diseases and datasets. To facilitate capturing
meaningful and robust features, it is necessary to first comprehensively
understand the intrinsic pattern of the brain that is not restricted within a
single data/task domain. Considering that the brain is a complex connectome of
interlinked neurons, the connectional properties in the brain have strong
biological significance, which is shared across multiple domains and covers
most pathological information. In this work, we propose a connectional style
contextual representation learning model (CS-CRL) to capture the intrinsic
pattern of the brain, used for multiple brain disease diagnosis. Specifically,
it has a vision transformer (ViT) encoder and leverages mask reconstruction as
the proxy task and Gram matrices to guide the representation of connectional
information. It facilitates the capture of global context and the aggregation
of features with biological plausibility. The results indicate that CS-CRL
achieves superior accuracy in multiple brain disease diagnosis tasks across six
datasets and three diseases and outperforms state-of-the-art models.
Furthermore, we demonstrate that CS-CRL captures more brain-network-like
properties, better aggregates features, is easier to optimize and is more
robust to noise, which explains its superiority in theory. Our source code will
be released soon.|
|**2023-06-08**|**The Star-forming and Ionizing Properties of Dwarf z~6-9 Galaxies in JADES: Insights on Bursty Star Formation and Ionized Bubble Growth**|Ryan Endsley et.al.|[2306.05295v1](http://arxiv.org/abs/2306.05295v1)|null|Reionization is thought to be driven by faint star-forming galaxies, but
characterizing this population in detail has long remained very challenging.
Here we utilize deep nine-band NIRCam imaging from JADES to study the
star-forming and ionizing properties of 756 $z\sim6-9$ galaxies, including
hundreds of very UV-faint objects ($M_\mathrm{UV}>-18$). The faintest
($m\sim30$) galaxies in our sample typically have stellar masses of
$M_\ast\sim(1-3)\times10^7$ $M_\odot$ and young light-weighted ages ($\sim$50
Myr), though some show strong Balmer breaks implying much older ages ($\sim$500
Myr). We find no evidence for extremely massive galaxies ($>3\times10^{10}$
$M_\odot$) in our sample. We infer a strong (factor $>$2) decline in the
typical [OIII]$+$H$\beta$ EWs towards very faint $z\sim6-9$ galaxies, yet a
weak UV luminosity dependence on the H$\alpha$ EWs at $z\sim6$. We demonstrate
that these EW trends can be explained if fainter galaxies have systematically
lower metallicities as well as more recently-declining star formation histories
relative to the most UV-luminous galaxies in our sample. Our data provide
evidence that the brightest galaxies are frequently experiencing a recent
strong upturn in SFR. We also discuss how the EW trends may be influenced by a
strong correlation between $M_\mathrm{UV}$ and Lyman continuum escape fraction.
This alternative explanation has dramatically different implications for the
contribution of galaxies along the luminosity function to cosmic reionization,
highlighting the need for deep spectroscopic follow-up. Finally, we quantify
the photometric overdensities around two $z>7$ strong Ly$\alpha$ emitters in
the JADES footprint. One Ly$\alpha$ emitter lies close to a strong photometric
overdensity while the other shows no significant nearby overdensity, perhaps
implying that not all strong $z>7$ Ly$\alpha$ emitters reside in large ionized
bubbles.|
|**2023-06-08**|**Weakly Lensed Gravitational Waves: Probing Cosmic Structures with Wave-Optics Features**|Stefano Savastano et.al.|[2306.05282v1](http://arxiv.org/abs/2306.05282v1)|null|Every signal propagating through the universe is at least weakly lensed by
the intervening gravitational field. In some situations, wave-optics phenomena
(diffraction, interference) can be observed as frequency-dependent modulations
of the waveform of gravitational waves (GWs). We will denote these signatures
as Wave-Optics Features (WOFs) and analyze them in detail. Our framework can
efficiently and accurately compute WOF in the single-image regime, of which
weak lensing is a limit. The phenomenology of WOF is rich and offers valuable
information: the dense cusps of individual halos appear as peaks in Green's
function for lensing. If resolved, these features probe the number, effective
masses, spatial distribution and inner profiles of substructures. High
signal-to-noise GW signals reveal WOFs well beyond the Einstein radius, leading
to a fair probability of observation by upcoming detectors such as LISA.
Potential applications of WOF include reconstruction of the lens' projected
density, delensing standard sirens and inferring large-scale structure
morphology and the halo mass function. Because WOF are sourced by light halos
with negligible baryonic content, their detection (or lack thereof) holds
promise to test dark matter scenarios.|
|**2023-06-08**|**Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training**|Haode Zhang et.al.|[2306.05278v1](http://arxiv.org/abs/2306.05278v1)|null|We consider the task of few-shot intent detection, which involves training a
deep learning model to classify utterances based on their underlying intents
using only a small amount of labeled data. The current approach to address this
problem is through continual pre-training, i.e., fine-tuning pre-trained
language models (PLMs) on external resources (e.g., conversational corpora,
public intent detection datasets, or natural language understanding datasets)
before using them as utterance encoders for training an intent classifier. In
this paper, we show that continual pre-training may not be essential, since the
overfitting problem of PLMs on this task may not be as serious as expected.
Specifically, we find that directly fine-tuning PLMs on only a handful of
labeled examples already yields decent results compared to methods that employ
continual pre-training, and the performance gap diminishes rapidly as the
number of labeled data increases. To maximize the utilization of the limited
available data, we propose a context augmentation method and leverage
sequential self-distillation to boost performance. Comprehensive experiments on
real-world benchmarks show that given only two or more labeled samples per
class, direct fine-tuning outperforms many strong baselines that utilize
external data sources for continual pre-training. The code can be found at
https://github.com/hdzhang-code/DFTPlus.|
|**2023-06-08**|**Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models**|Tianzhe Chu et.al.|[2306.05272v1](http://arxiv.org/abs/2306.05272v1)|[link](https://github.com/leslietrue/cpp)|The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.|
|**2023-06-08**|**Factorized Contrastive Learning: Going Beyond Multi-view Redundancy**|Paul Pu Liang et.al.|[2306.05268v1](http://arxiv.org/abs/2306.05268v1)|[link](https://github.com/pliang279/factorcl)|In a wide range of multimodal tasks, contrastive learning has become a
particularly appealing approach since it can successfully learn representations
from abundant unlabeled data with only pairing information (e.g., image-caption
or video-audio pairs). Underpinning these approaches is the assumption of
multi-view redundancy - that shared information between modalities is necessary
and sufficient for downstream tasks. However, in many real-world settings,
task-relevant information is also contained in modality-unique regions:
information that is only present in one modality but still relevant to the
task. How can we learn self-supervised multimodal representations to capture
both shared and unique information relevant to downstream tasks? This paper
proposes FactorCL, a new multimodal representation learning method to go beyond
multi-view redundancy. FactorCL is built from three new contributions: (1)
factorizing task-relevant information into shared and unique representations,
(2) capturing task-relevant information via maximizing MI lower bounds and
removing task-irrelevant information via minimizing MI upper bounds, and (3)
multimodal data augmentations to approximate task relevance without labels. On
large-scale real-world datasets, FactorCL captures both shared and unique
information and achieves state-of-the-art results on six benchmarks.|

## Camera

### Camera
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |

## Transformer

### Transformer
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories**|Shizhe Diao et.al.|[2306.05406v1](http://arxiv.org/abs/2306.05406v1)|[link](https://github.com/amano-aki/mixture-of-domain-adapters)|Pre-trained language models (PLMs) demonstrate excellent abilities to
understand texts in the generic domain while struggling in a specific domain.
Although continued pre-training on a large domain-specific corpus is effective,
it is costly to tune all the parameters on the domain. In this paper, we
investigate whether we can adapt PLMs both effectively and efficiently by only
tuning a few parameters. Specifically, we decouple the feed-forward networks
(FFNs) of the Transformer architecture into two parts: the original pre-trained
FFNs to maintain the old-domain knowledge and our novel domain-specific
adapters to inject domain-specific knowledge in parallel. Then we adopt a
mixture-of-adapters gate to fuse the knowledge from different domain adapters
dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a
two-stage adapter-tuning strategy that leverages both unlabeled data and
labeled data to help the domain adaptation: i) domain-specific adapter on
unlabeled data; followed by ii) the task-specific adapter on labeled data.
MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and
our experiments demonstrate that MixDA achieves superior performance on
in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and
knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,
scalability, and efficiency of our method. The code is available at
https://github.com/Amano-Aki/Mixture-of-Domain-Adapters.|
|**2023-06-08**|**Rate Forecaster based Energy Aware Band Assignment in Multiband Networks**|Brijesh Soni et.al.|[2306.05369v1](http://arxiv.org/abs/2306.05369v1)|null|The high frequency communication bands (mmWave and sub-THz) promise
tremendous data rates, however, they also have very high power consumption
which is particularly significant for battery-power-limited user-equipment
(UE). In this context, we design an energy aware band assignment system which
reduces the power consumption while also achieving a target sum rate of M in T
time-slots. We do this by using 1) Rate forecaster(s); 2) Channel forecaster(s)
which forecasts T direct multistep ahead using a stacked (long short term
memory) LSTM architecture. We propose an iterative rate updating algorithm
which updates the target rate based on current rate and future predicted rates
in a frame. The proposed approach is validated on the publicly available
`DeepMIMO' dataset. Research findings shows that the rate forecaster based
approach performs better than the channel forecaster. Furthermore, LSTM based
predictions outperforms well celebrated Transformer predictions in terms of
NRMSE and NMAE. Research findings reveals that the power consumption with this
approach is ~ 300 mW lower compared to a greedy band assignment at a 1.5Gb/s
target rate.|
|**2023-06-08**|**Ordinal Potential-based Player Rating**|Nelson Vadori et.al.|[2306.05366v1](http://arxiv.org/abs/2306.05366v1)|null|A two-player symmetric zero-sum game is transitive if for any pure strategies
$x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$
is better than $z$. It was recently observed that the Elo rating fails at
preserving transitive relations among strategies and therefore cannot correctly
extract the transitive component of a game. Our first contribution is to show
that the Elo rating actually does preserve transitivity when computed in the
right space. Precisely, using a suitable invertible mapping $\varphi$, we first
apply $\varphi$ to the game, then compute Elo ratings, then go back to the
original space by applying $\varphi^{-1}$. We provide a characterization of
transitive games as a weak variant of ordinal potential games with additively
separable potential functions. Leveraging this insight, we introduce the
concept of transitivity order, the minimum number of invertible mappings
required to transform the payoff of a transitive game into (differences of) its
potential function. The transitivity order is a tool to classify transitive
games, with Elo games being an example of transitive games of order one. Most
real-world games have both transitive and non-transitive (cyclic) components,
and we use our analysis of transitivity to extract the transitive (potential)
component of an arbitrary game. We link transitivity to the known concept of
sign-rank: transitive games have sign-rank two; arbitrary games may have higher
sign-rank. Using a neural network-based architecture, we learn a decomposition
of an arbitrary game into transitive and cyclic components that prioritises
capturing the sign pattern of the game. In particular, a transitive game always
has just one component in its decomposition, the potential component. We
provide a comprehensive evaluation of our methodology using both toy examples
and empirical data from real-world games.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**Mode-locked laser in nanophotonic lithium niobate**|Qiushi Guo et.al.|[2306.05314v1](http://arxiv.org/abs/2306.05314v1)|null|Mode-locked lasers (MLLs) have enabled ultrafast sciences and technologies by
generating ultrashort pulses with peak powers substantially exceeding their
average powers. Recently, tremendous efforts have been focused on realizing
integrated MLLs not only to address the challenges associated with their size
and power demand, but also to enable transforming the ultrafast technologies
into nanophotonic chips, and ultimately to unlock their potential for a
plethora of applications. However, till now the prospect of integrated MLLs
driving ultrafast nanophotonic circuits has remained elusive because of their
typically low peak powers, lack of controllability, and challenges with
integration with appropriate nanophotonic platforms. Here, we overcome these
limitations by demonstrating an electrically-pumped actively MLL in
nanophotonic lithium niobate based on its hybrid integration with a III-V
semiconductor optical amplifier. Our MLL generates $\sim$4.8 ps optical pulses
around 1065 nm at a repetition rate of $\sim$10 GHz, with pulse energy
exceeding 2.6 pJ and a high peak power beyond 0.5 W. We show that both the
repetition rate and the carrier-envelope-offset of the resulting frequency comb
can be flexibly controlled in a wide range using the RF driving frequency and
the pump current, paving the way for fully-stabilized on-chip frequency combs
in nanophotonics. Our work marks an important step toward fully-integrated
nonlinear and ultrafast photonic systems in nanophotonic lithium niobate.|
|**2023-06-08**|**Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis**|Gongshu Wang et.al.|[2306.05297v1](http://arxiv.org/abs/2306.05297v1)|null|Structural magnetic resonance imaging (sMRI) has shown great clinical value
and has been widely used in deep learning (DL) based computer-aided brain
disease diagnosis. Previous approaches focused on local shapes and textures in
sMRI that may be significant only within a particular domain. The learned
representations are likely to contain spurious information and have a poor
generalization ability in other diseases and datasets. To facilitate capturing
meaningful and robust features, it is necessary to first comprehensively
understand the intrinsic pattern of the brain that is not restricted within a
single data/task domain. Considering that the brain is a complex connectome of
interlinked neurons, the connectional properties in the brain have strong
biological significance, which is shared across multiple domains and covers
most pathological information. In this work, we propose a connectional style
contextual representation learning model (CS-CRL) to capture the intrinsic
pattern of the brain, used for multiple brain disease diagnosis. Specifically,
it has a vision transformer (ViT) encoder and leverages mask reconstruction as
the proxy task and Gram matrices to guide the representation of connectional
information. It facilitates the capture of global context and the aggregation
of features with biological plausibility. The results indicate that CS-CRL
achieves superior accuracy in multiple brain disease diagnosis tasks across six
datasets and three diseases and outperforms state-of-the-art models.
Furthermore, we demonstrate that CS-CRL captures more brain-network-like
properties, better aggregates features, is easier to optimize and is more
robust to noise, which explains its superiority in theory. Our source code will
be released soon.|
|**2023-06-08**|**Simple and Controllable Music Generation**|Jade Copet et.al.|[2306.05284v1](http://arxiv.org/abs/2306.05284v1)|null|We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, while being conditioned on textual
description or melodic features, allowing better controls over the generated
output. We conduct extensive empirical evaluation, considering both automatic
and human studies, showing the proposed approach is superior to the evaluated
baselines on a standard text-to-music benchmark. Through ablation studies, we
shed light over the importance of each of the components comprising MusicGen.
Music samples, code, and models are available at
https://github.com/facebookresearch/audiocraft.|
|**2023-06-08**|**Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction**|Simone Scaboro et.al.|[2306.05276v1](http://arxiv.org/abs/2306.05276v1)|[link](https://github.com/ailabudinegit/ade-detection-survey)|Adverse Event (ADE) extraction is one of the core tasks in digital
pharmacovigilance, especially when applied to informal texts. This task has
been addressed by the Natural Language Processing community using large
pre-trained language models, such as BERT. Despite the great number of
Transformer-based architectures used in the literature, it is unclear which of
them has better performances and why. Therefore, in this paper we perform an
extensive evaluation and analysis of 19 Transformer-based models for ADE
extraction on informal texts. We compare the performance of all the considered
models on two datasets with increasing levels of informality (forums posts and
tweets). We also combine the purely Transformer-based models with two
commonly-used additional processing layers (CRF and LSTM), and analyze their
effect on the models performance. Furthermore, we use a well-established
feature importance technique (SHAP) to correlate the performance of the models
with a set of features that describe them: model category (AutoEncoding,
AutoRegressive, Text-to-Text), pretraining domain, training from scratch, and
model size in number of parameters. At the end of our analyses, we identify a
list of take-home messages that can be derived from the experimental data.|
|**2023-06-08**|**Unscented Autoencoder**|Faris Janjo et.al.|[2306.05256v1](http://arxiv.org/abs/2306.05256v1)|null|The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.|
|**2023-06-08**|**Efficient Multi-Task Scene Analysis with RGB-D Transformers**|Shnke Benedikt Fischedick et.al.|[2306.05242v1](http://arxiv.org/abs/2306.05242v1)|[link](https://github.com/tui-nicr/nicr-scene-analysis-datasets)|Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.|
|**2023-06-08**|**Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition**|Bo Jiang et.al.|[2306.05239v1](http://arxiv.org/abs/2306.05239v1)|null|Considering the balance of performance and efficiency, sampled point and
voxel methods are usually employed to down-sample dense events into sparse
ones. After that, one popular way is to leverage a graph model which treats the
sparse points/voxels as nodes and adopts graph neural networks (GNNs) to learn
the representation for event data. Although good performance can be obtained,
however, their results are still limited mainly due to two issues. (1) Existing
event GNNs generally adopt the additional max (or mean) pooling layer to
summarize all node embeddings into a single graph-level representation for the
whole event data representation. However, this approach fails to capture the
importance of graph nodes and also fails to be fully aware of the node
representations. (2) Existing methods generally employ either a sparse point or
voxel graph representation model which thus lacks consideration of the
complementary between these two types of representation models. To address
these issues, in this paper, we propose a novel dual point-voxel absorbing
graph representation learning for event stream data representation. To be
specific, given the input event stream, we first transform it into the sparse
event cloud and voxel grids and build dual absorbing graph models for them
respectively. Then, we design a novel absorbing graph convolutional network
(AGCN) for our dual absorbing graph representation and learning. The key aspect
of the proposed AGCN is its ability to effectively capture the importance of
nodes and thus be fully aware of node representations in summarizing all node
representations through the introduced absorbing nodes. Finally, the event
representations of dual learning branches are concatenated together to extract
the complementary information of two cues. The output is then fed into a linear
layer for event data classification.|
|**2023-06-08**|**Iterative Signal Processing for Integrated Sensing and Communication Systems**|Zhiqing Wei et.al.|[2306.05235v1](http://arxiv.org/abs/2306.05235v1)|null|Integrated sensing and communication (ISAC), with sensing and communication
sharing the same wireless resources and hardware, has the advantages of high
spectrum efficiency and low hardware cost, which is regarded as one of the key
technologies of the fifth generation advanced (5G-A) and sixth generation (6G)
mobile communication systems. ISAC has the potential to be applied in the
intelligent applications requiring both communication and high accurate sensing
capabilities. The fundamental challenges of ISAC system are the ISAC signal
design and ISAC signal processing. However, the existing ISAC signal has low
anti-noise capability. And the existing ISAC signal processing algorithms have
the disadvantages of quantization errors and high complexity, resulting in
large energy consumption. In this paper, phase coding is applied in ISAC signal
design to improve the anti-noise performance of ISAC signal. Then, the effect
of phase coding method on improving the sensing accuracy is analyzed. In order
to improve the sensing accuracy with low-complexity algorithm, the iterative
ISAC signal processing methods are proposed. The proposed methods improve the
sensing accuracy with low computational complexity, realizing energy efficient
ISAC signal processing. Taking the scenarios of short distance and long
distance sensing into account, the iterative two-dimensional (2D) fast Fourier
transform (FFT) and iterative cyclic cross-correlation (CC) methods are
proposed, respectively, realizing high sensing accuracy and low computational
complexity. Finally, the feasibility of the proposed ISAC signal processing
methods are verified by simulation results.|
|**2023-06-08**|**Autoencoding for the 'Good Dictionary' of eigen pairs of the Koopman Operator**|Neranjaka Jayarathne et.al.|[2306.05224v1](http://arxiv.org/abs/2306.05224v1)|null|Reduced order modelling relies on representing complex dynamical systems
using simplified modes, which can be achieved through Koopman operator
analysis. However, computing Koopman eigen pairs for high-dimensional
observable data can be inefficient. This paper proposes using deep
autoencoders, a type of deep learning technique, to perform non-linear
geometric transformations on raw data before computing Koopman eigen vectors.
The encoded data produced by the deep autoencoder is diffeomorphic to a
manifold of the dynamical system, and has a significantly lower dimension than
the raw data. To handle high-dimensional time series data, Takens's time delay
embedding is presented as a pre-processing technique. The paper concludes by
presenting examples of these techniques in action.|
|**2023-06-08**|**Boosting-based Construction of BDDs for Linear Threshold Functions and Its Application to Verification of Neural Networks**|Yiping Tang et.al.|[2306.05211v1](http://arxiv.org/abs/2306.05211v1)|null|Understanding the characteristics of neural networks is important but
difficult due to their complex structures and behaviors. Some previous work
proposes to transform neural networks into equivalent Boolean expressions and
apply verification techniques for characteristics of interest. This approach is
promising since rich results of verification techniques for circuits and other
Boolean expressions can be readily applied. The bottleneck is the time
complexity of the transformation. More precisely, (i) each neuron of the
network, i.e., a linear threshold function, is converted to a Binary Decision
Diagram (BDD), and (ii) they are further combined into some final form, such as
Boolean circuits. For a linear threshold function with $n$ variables, an
existing method takes $O(n2^{\frac{n}{2}})$ time to construct an ordered BDD of
size $O(2^{\frac{n}{2}})$ consistent with some variable ordering. However, it
is non-trivial to choose a variable ordering producing a small BDD among $n!$
candidates.
  We propose a method to convert a linear threshold function to a specific form
of a BDD based on the boosting approach in the machine learning literature. Our
method takes $O(2^n \text{poly}(1/\rho))$ time and outputs BDD of size
$O(\frac{n^2}{\rho^4}\ln{\frac{1}{\rho}})$, where $\rho$ is the margin of some
consistent linear threshold function. Our method does not need to search for
good variable orderings and produces a smaller expression when the margin of
the linear threshold function is large. More precisely, our method is based on
our new boosting algorithm, which is of independent interest. We also propose a
method to combine them into the final Boolean expression representing the
neural network.|
|**2023-06-08**|**Bayesian Inference for Multivariate Monotone Densities**|Kang Wang et.al.|[2306.05202v1](http://arxiv.org/abs/2306.05202v1)|null|We consider a nonparametric Bayesian approach to estimation and testing for a
multivariate monotone density. Instead of following the conventional Bayesian
route of putting a prior distribution complying with the monotonicity
restriction, we put a prior on the step heights through binning and a Dirichlet
distribution. An arbitrary piece-wise constant probability density is converted
to a monotone one by a projection map, taking its $\mathbb{L}_1$-projection
onto the space of monotone functions, which is subsequently normalized to
integrate to one. We construct consistent Bayesian tests to test multivariate
monotonicity of a probability density based on the $\mathbb{L}_1$-distance to
the class of monotone functions. The test is shown to have a size going to zero
and high power against alternatives sufficiently separated from the null
hypothesis. To obtain a Bayesian credible interval for the value of the density
function at an interior point with guaranteed asymptotic frequentist coverage,
we consider a posterior quantile interval of an induced map transforming the
function value to its value optimized over certain blocks. The limiting
coverage is explicitly calculated and is seen to be higher than the credibility
level used in the construction. By exploring the asymptotic relationship
between the coverage and the credibility, we show that a desired asymptomatic
coverage can be obtained exactly by starting with an appropriate credibility
level.|
|**2023-06-08**|**Topological structures of energy flow: Poynting vector skyrmions**|Sicong Wang et.al.|[2306.05191v1](http://arxiv.org/abs/2306.05191v1)|null|Topological properties of energy flow of light are fundamentally interesting
and have rich practical applications in optical manipulations. Here,
skyrmion-like structures formed by Poynting vectors are unveiled in the focal
region of a pair of counter-propagating cylindrical vector vortex beams in free
space. A N\'eel-Bloch-N\'eel skyrmion type transformation of Poynting vectors
is observed along the light propagating direction within a volume with
subwavelength feature sizes. The corresponding skyrmion type can be determined
by the phase singularities of the individual components of the coherently
superposed electromagnetic field in the focal region. This work reveals a new
family member of optical skyrmions and may introduce novel physical phenomena
associated with light scattering and optical force.|
|**2023-06-08**|**RRWKV: Capturing Long-range Dependencies in RWKV**|Leilei Wang et.al.|[2306.05176v1](http://arxiv.org/abs/2306.05176v1)|null|Owing to the impressive dot-product attention, the Transformers have been the
dominant architectures in various natural language processing (NLP) tasks.
Recently, the Receptance Weighted Key Value (RWKV) architecture follows a
non-transformer architecture to eliminate the drawbacks of dot-product
attention, where memory and computational complexity exhibits quadratic scaling
with sequence length. Although RWKV has exploited a linearly tensor-product
attention mechanism and achieved parallelized computations by deploying the
time-sequential mode, it fails to capture long-range dependencies because of
its limitation on looking back at previous information, compared with full
information obtained by direct interactions in the standard transformer.
Therefore, the paper devises the Retrospected Receptance Weighted Key Value
(RRWKV) architecture via incorporating the retrospecting ability into the RWKV
to effectively absorb information, which maintains memory and computational
efficiency as well.|
|**2023-06-08**|**Large-scale Dataset Pruning with Dynamic Uncertainty**|Muyang He et.al.|[2306.05175v1](http://arxiv.org/abs/2306.05175v1)|null|The state of the art of many learning tasks, e.g., image classification, is
advanced by collecting larger datasets and then training larger models on them.
As the outcome, the increasing computational cost is becoming unaffordable. In
this paper, we investigate how to prune the large-scale datasets, and thus
produce an informative subset for training sophisticated deep models with
negligible performance drop. We propose a simple yet effective dataset pruning
method by exploring both the prediction uncertainty and training dynamics. To
our knowledge, this is the first work to study dataset pruning on large-scale
datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin
Transformer and ConvNeXt. Extensive experimental results indicate that our
method outperforms the state of the art and achieves 75% lossless compression
ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are
available at https://github.com/BAAI-DCAI/Dataset-Pruning.|
|**2023-06-08**|**Achieving higher photoabsorption than group III-V semiconductors in silicon using photon-trapping surface structures**|Wayesh Qarony et.al.|[2306.05170v1](http://arxiv.org/abs/2306.05170v1)|null|The photosensitivity of silicon is inherently very low in the visible
electromagnetic spectrum, and it drops rapidly beyond 800 nm in near-infrared
wavelengths. Herein, we have experimentally demonstrated a technique utilizing
photon-trapping surface structures to show a prodigious improvement of
photoabsorption in one-micrometer-thin silicon, surpassing the inherent
absorption efficiency of gallium arsenide for a broad spectrum. The
photon-trapping structures allow the bending of normally incident light by
almost ninety degrees to transform into laterally propagating modes along the
silicon plane. Consequently, the propagation length of light increases,
contributing to more than an order of magnitude improvement in absorption
efficiency in photodetectors. This high absorption phenomenon is explained by
FDTD analysis, where we show an enhanced photon density of states while
substantially reducing the optical group velocity of light compared to silicon
without photon-trapping structures, leading to significantly enhanced
light-matter interactions. Our simulations also predict an enhanced absorption
efficiency of photodetectors designed using 30 and 100-nanometer silicon thin
films that are compatible with CMOS electronics. Despite a very thin absorption
layer, such photon-trapping structures can enable high-efficiency and
high-speed photodetectors needed in ultra-fast computer networks, data
communication, and imaging systems with the potential to revolutionize on-chip
logic and optoelectronic integration.|
|**2023-06-08**|**Decision S4: Efficient Sequence-Based RL via State Spaces Layers**|Shmuel Bar-David et.al.|[2306.05167v1](http://arxiv.org/abs/2306.05167v1)|null|Recently, sequence learning methods have been applied to the problem of
off-policy Reinforcement Learning, including the seminal work on Decision
Transformers, which employs transformers for this task. Since transformers are
parameter-heavy, cannot benefit from history longer than a fixed window size,
and are not computed using recurrence, we set out to investigate the
suitability of the S4 family of models, which are based on state-space layers
and have been shown to outperform transformers, especially in modeling
long-range dependencies. In this work we present two main algorithms: (i) an
off-policy training procedure that works with trajectories, while still
maintaining the training efficiency of the S4 model. (ii) An on-policy training
procedure that is trained in a recurrent manner, benefits from long-range
dependencies, and is based on a novel stable actor-critic mechanism. Our
results indicate that our method outperforms multiple variants of decision
transformers, as well as the other baseline methods on most tasks, while
reducing the latency, number of parameters, and training time by several orders
of magnitude, making our approach more suitable for real-world RL.|
|**2023-06-08**|**DFT-Based Channel Estimation for Holographic MIMO**|Antonio Alberto D'Amico et.al.|[2306.05156v1](http://arxiv.org/abs/2306.05156v1)|null|Holographic MIMO (hMIMO) systems with a massive number of individually
controlled antennas N make minimum mean square error (MMSE) channel estimation
particularly challenging, due to its computational complexity that scales as
$N^3$ . This paper investigates uniform linear arrays and proposes a
low-complexity method based on the discrete Fourier transform (DFT)
approximation, which follows from replacing the covariance matrix by a suitable
circulant matrix. Numerical results show that, already for arrays with moderate
size (in the order of tens of wavelengths), it achieves the same performance of
the optimal MMSE, but with a significant lower computational load that scales
as $N \log N$. Interestingly, the proposed method provides also increased
robustness in case of imperfect knowledge of the covariance matrix.|
|**2023-06-08**|**Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose**|Wiktor Mucha et.al.|[2306.05147v1](http://arxiv.org/abs/2306.05147v1)|null|Egocentric action recognition is essential for healthcare and assistive
technology that relies on egocentric cameras because it allows for the
automatic and continuous monitoring of activities of daily living (ADLs)
without requiring any conscious effort from the user. This study explores the
feasibility of using 2D hand and object pose information for egocentric action
recognition. While current literature focuses on 3D hand pose information, our
work shows that using 2D skeleton data is a promising approach for hand-based
action classification, might offer privacy enhancement, and could be less
computationally demanding. The study uses a state-of-the-art transformer-based
method to classify sequences and achieves validation results of 94%,
outperforming other existing solutions. The accuracy of the test subset drops
to 76%, indicating the need for further generalization improvement. This
research highlights the potential of 2D hand and object pose information for
action recognition tasks and offers a promising alternative to 3D-based
methods.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer**|Zehui Li et.al.|[2306.05143v1](http://arxiv.org/abs/2306.05143v1)|null|Given the increasing volume and quality of genomics data, extracting new
insights requires interpretable machine-learning models. This work presents
Genomic Interpreter: a novel architecture for genomic assay prediction. This
model outperforms the state-of-the-art models for genomic assay prediction
tasks. Our model can identify hierarchical dependencies in genomic sites. This
is achieved through the integration of 1D-Swin, a novel Transformer-based block
designed by us for modelling long-range hierarchical data. Evaluated on a
dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter
demonstrates superior performance in chromatin accessibility and gene
expression prediction and unmasks the underlying `syntax' of gene regulation.|
|**2023-06-08**|**Transition to elasto-capillary thinning dynamics in viscoelastic jets**|Konstantinos Zinelis et.al.|[2306.05137v1](http://arxiv.org/abs/2306.05137v1)|null|We perform simulations of an impulsively-started, axisymmetric viscoelastic
jet exiting a nozzle and entering a stagnant gas phase using the open-source
code Basilisk. This code allows for efficient computations through an
adaptively-refined volume-of-fluid technique that can accurately capture the
deformation of the liquid-gas interface. We use the FENE-P constitutive
equation to describe the viscoelasticity of the liquid and employ the
log-conformation transformation, which provides stable solutions for the
evolution of the conformation tensor as the jet thins down under the action of
interfacial tension. For the first time, the entire jetting and breakup process
of a viscoelastic fluid is simulated, including the pre-shearing flow through
the nozzle, which results in an inhomogeneous initial radial stress
distribution in the fluid thread that affects the subsequent breakup dynamics.
The evolution of the velocity field and the elastic stresses in the nozzle are
validated against analytical solutions where possible, and the early-stage
dynamics of the jet evolution are compared favourably to the predictions of
linear stability theory. We study the effect of the flow inside the nozzle on
the thinning dynamics of the viscoelastic jet (which develops distinctive
"beads-on-a-string" structures) and on the spatio-temporal evolution of the
polymeric stresses in order to systematically explore the dependence of the
filament thinning and breakup characteristics on the initial axial momentum of
the jet and the extensibility of the dissolved polymer chains.|
|**2023-06-08**|**Can AI Moderate Online Communities?**|Henrik Axelsen et.al.|[2306.05122v1](http://arxiv.org/abs/2306.05122v1)|null|The task of cultivating healthy communication in online communities becomes
increasingly urgent, as gaming and social media experiences become
progressively more immersive and life-like. We approach the challenge of
moderating online communities by training student models using a large language
model (LLM). We use zero-shot learning models to distill and expand datasets
followed by a few-shot learning and a fine-tuning approach, leveraging
open-access generative pre-trained transformer models (GPT) from OpenAI. Our
preliminary findings suggest, that when properly trained, LLMs can excel in
identifying actor intentions, moderating toxic comments, and rewarding positive
contributions. The student models perform above-expectation in non-contextual
assignments such as identifying classically toxic behavior and perform
sufficiently on contextual assignments such as identifying positive
contributions to online discourse. Further, using open-access models like
OpenAI's GPT we experience a step-change in the development process for what
has historically been a complex modeling task. We contribute to the information
system (IS) discourse with a rapid development framework on the application of
generative AI in content online moderation and management of culture in
decentralized, pseudonymous communities by providing a sample model suite of
industrial-ready generative AI models based on open-access LLMs.|
|**2023-06-08**|**TransTIC: Transferring Transformer-based Image Compression from Human Visualization to Machine Perception**|Yi-Hsin Chen et.al.|[2306.05085v1](http://arxiv.org/abs/2306.05085v1)|null|This work aims for transferring a Transformer-based image compression codec
from human vision to machine perception without fine-tuning the codec. We
propose a transferable Transformer-based image compression framework, termed
TransTIC. Inspired by visual prompt tuning, we propose an instance-specific
prompt generator to inject instance-specific prompts to the encoder and
task-specific prompts to the decoder. Extensive experiments show that our
proposed method is capable of transferring the codec to various machine tasks
and outshining the competing methods significantly. To our best knowledge, this
work is the first attempt to utilize prompting on the low-level image
compression task.|
|**2023-06-08**|**Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS**|Cheng-Han Chiang et.al.|[2306.05083v1](http://arxiv.org/abs/2306.05083v1)|null|Existing sentence textual similarity benchmark datasets only use a single
number to summarize how similar the sentence encoder's decision is to humans'.
However, it is unclear what kind of sentence pairs a sentence encoder (SE)
would consider similar. Moreover, existing SE benchmarks mainly consider
sentence pairs with low lexical overlap, so it is unclear how the SEs behave
when two sentences have high lexical overlap. We introduce a high-quality SE
diagnostic dataset, HEROS. HEROS is constructed by transforming an original
sentence into a new sentence based on certain rules to form a \textit{minimal
pair}, and the minimal pair has high lexical overlaps. The rules include
replacing a word with a synonym, an antonym, a typo, a random word, and
converting the original sentence into its negation. Different rules yield
different subsets of HEROS. By systematically comparing the performance of over
60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised
sentence encoders are insensitive to negation. We find the datasets used to
train the SE are the main determinants of what kind of sentence pairs an SE
considers similar. We also show that even if two SEs have similar performance
on STS benchmarks, they can have very different behavior on HEROS. Our result
reveals the blind spot of traditional STS benchmarks when evaluating SEs.|
|**2023-06-08**|**LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification**|Konstantin Chernyshev et.al.|[2306.05075v1](http://arxiv.org/abs/2306.05075v1)|[link](https://github.com/lct-rug-2022/edos-2023)|Misogyny and sexism are growing problems in social media. Advances have been
made in online sexism detection but the systems are often uninterpretable.
SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at
increasing explainability of the sexism detection, and our team participated in
all the proposed subtasks. Our system is based on further domain-adaptive
pre-training (Gururangan et al., 2020). Building on the Transformer-based
models with the domain adaptation, we compare fine-tuning with multi-task
learning and show that each subtask requires a different system configuration.
In our experiments, multi-task learning performs on par with standard
fine-tuning for sexism detection and noticeably better for coarse-grained
sexism classification, while fine-tuning is preferable for fine-grained
classification.|

### Vision Transformer
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Grounded Text-to-Image Synthesis with Attention Refocusing**|Quynh Phung et.al.|[2306.05427v1](http://arxiv.org/abs/2306.05427v1)|null|Driven by scalable diffusion models trained on large-scale paired text-image
datasets, text-to-image synthesis methods have shown compelling results.
However, these models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are involved in the
prompt. In this paper, we identify the potential reasons in both the
cross-attention and self-attention layers of the diffusion model. We propose
two novel losses to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive experiments on the
DrawBench and HRS benchmarks using layouts synthesized by Large Language
Models, showing that our proposed losses can be integrated easily and
effectively into existing text-to-image methods and consistently improve their
alignment between the generated images and the text prompts.|
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**|Muhammad Maaz et.al.|[2306.05424v1](http://arxiv.org/abs/2306.05424v1)|[link](https://github.com/mbzuai-oryx/video-chatgpt)|Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**Stochastic Multi-Person 3D Motion Forecasting**|Sirui Xu et.al.|[2306.05421v1](http://arxiv.org/abs/2306.05421v1)|null|This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.|
|**2023-06-08**|**Scaling Spherical CNNs**|Carlos Esteves et.al.|[2306.05420v1](http://arxiv.org/abs/2306.05420v1)|[link](https://github.com/google-research/spherical-cnn)|Spherical CNNs generalize CNNs to functions on the sphere, by using spherical
convolutions as the main linear operation. The most accurate and efficient way
to compute spherical convolutions is in the spectral domain (via the
convolution theorem), which is still costlier than the usual planar
convolutions. For this reason, applications of spherical CNNs have so far been
limited to small problems that can be approached with low model capacity. In
this work, we show how spherical CNNs can be scaled for much larger problems.
To achieve this, we make critical improvements including novel variants of
common model components, an implementation of core operations to exploit
hardware accelerator characteristics, and application-specific input
representations that exploit the properties of our model. Experiments show our
larger spherical CNNs reach state-of-the-art on several targets of the QM9
molecular benchmark, which was previously dominated by equivariant graph neural
networks, and achieve competitive performance on multiple weather forecasting
tasks. Our code is available at
https://github.com/google-research/spherical-cnn.|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**Improving Negative-Prompt Inversion via Proximal Guidance**|Ligong Han et.al.|[2306.05414v1](http://arxiv.org/abs/2306.05414v1)|[link](https://github.com/phymhan/prompt-to-prompt)|DDIM inversion has revealed the remarkable potential of real image editing
within diffusion-based methods. However, the accuracy of DDIM reconstruction
degrades as larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align
the reconstruction and inversion trajectories with larger CFG scales, enabling
real image editing with cross-attention control. Negative-prompt inversion
(NPI) further offers a training-free closed-form solution of NTI. However, it
may introduce artifacts and is still constrained by DDIM reconstruction
quality. To overcome these limitations, we propose Proximal Negative-Prompt
Inversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI with
a regularization term and reconstruction guidance, which reduces artifacts
while capitalizing on its training-free nature. Our method provides an
efficient and straightforward approach, effectively addressing real image
editing tasks with minimal computational overhead.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories**|Shizhe Diao et.al.|[2306.05406v1](http://arxiv.org/abs/2306.05406v1)|[link](https://github.com/amano-aki/mixture-of-domain-adapters)|Pre-trained language models (PLMs) demonstrate excellent abilities to
understand texts in the generic domain while struggling in a specific domain.
Although continued pre-training on a large domain-specific corpus is effective,
it is costly to tune all the parameters on the domain. In this paper, we
investigate whether we can adapt PLMs both effectively and efficiently by only
tuning a few parameters. Specifically, we decouple the feed-forward networks
(FFNs) of the Transformer architecture into two parts: the original pre-trained
FFNs to maintain the old-domain knowledge and our novel domain-specific
adapters to inject domain-specific knowledge in parallel. Then we adopt a
mixture-of-adapters gate to fuse the knowledge from different domain adapters
dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a
two-stage adapter-tuning strategy that leverages both unlabeled data and
labeled data to help the domain adaptation: i) domain-specific adapter on
unlabeled data; followed by ii) the task-specific adapter on labeled data.
MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and
our experiments demonstrate that MixDA achieves superior performance on
in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and
knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,
scalability, and efficiency of our method. The code is available at
https://github.com/Amano-Aki/Mixture-of-Domain-Adapters.|
|**2023-06-08**|**RDumb: A simple approach that questions our progress in continual test-time adaptation**|Ori Press et.al.|[2306.05401v1](http://arxiv.org/abs/2306.05401v1)|[link](https://github.com/oripress/ccc)|Test-Time Adaptation (TTA) allows to update pretrained models to changing
data distributions at deployment time. While early work tested these algorithms
for individual fixed distribution shifts, recent work proposed and applied
methods for continual adaptation over long timescales. To examine the reported
progress in the field, we propose the Continuously Changing Corruptions (CCC)
benchmark to measure asymptotic performance of TTA techniques. We find that
eventually all but one state-of-the-art methods collapse and perform worse than
a non-adapting model, including models specifically proposed to be robust to
performance collapse. In addition, we introduce a simple baseline, "RDumb",
that periodically resets the model to its pretrained state. RDumb performs
better or on par with the previously proposed state-of-the-art in all
considered benchmarks. Our results show that previous TTA approaches are
neither effective at regularizing adaptation to avoid collapse nor able to
outperform a simplistic resetting strategy.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Rate Forecaster based Energy Aware Band Assignment in Multiband Networks**|Brijesh Soni et.al.|[2306.05369v1](http://arxiv.org/abs/2306.05369v1)|null|The high frequency communication bands (mmWave and sub-THz) promise
tremendous data rates, however, they also have very high power consumption
which is particularly significant for battery-power-limited user-equipment
(UE). In this context, we design an energy aware band assignment system which
reduces the power consumption while also achieving a target sum rate of M in T
time-slots. We do this by using 1) Rate forecaster(s); 2) Channel forecaster(s)
which forecasts T direct multistep ahead using a stacked (long short term
memory) LSTM architecture. We propose an iterative rate updating algorithm
which updates the target rate based on current rate and future predicted rates
in a frame. The proposed approach is validated on the publicly available
`DeepMIMO' dataset. Research findings shows that the rate forecaster based
approach performs better than the channel forecaster. Furthermore, LSTM based
predictions outperforms well celebrated Transformer predictions in terms of
NRMSE and NMAE. Research findings reveals that the power consumption with this
approach is ~ 300 mW lower compared to a greedy band assignment at a 1.5Gb/s
target rate.|
|**2023-06-08**|**Ordinal Potential-based Player Rating**|Nelson Vadori et.al.|[2306.05366v1](http://arxiv.org/abs/2306.05366v1)|null|A two-player symmetric zero-sum game is transitive if for any pure strategies
$x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$
is better than $z$. It was recently observed that the Elo rating fails at
preserving transitive relations among strategies and therefore cannot correctly
extract the transitive component of a game. Our first contribution is to show
that the Elo rating actually does preserve transitivity when computed in the
right space. Precisely, using a suitable invertible mapping $\varphi$, we first
apply $\varphi$ to the game, then compute Elo ratings, then go back to the
original space by applying $\varphi^{-1}$. We provide a characterization of
transitive games as a weak variant of ordinal potential games with additively
separable potential functions. Leveraging this insight, we introduce the
concept of transitivity order, the minimum number of invertible mappings
required to transform the payoff of a transitive game into (differences of) its
potential function. The transitivity order is a tool to classify transitive
games, with Elo games being an example of transitive games of order one. Most
real-world games have both transitive and non-transitive (cyclic) components,
and we use our analysis of transitivity to extract the transitive (potential)
component of an arbitrary game. We link transitivity to the known concept of
sign-rank: transitive games have sign-rank two; arbitrary games may have higher
sign-rank. Using a neural network-based architecture, we learn a decomposition
of an arbitrary game into transitive and cyclic components that prioritises
capturing the sign pattern of the game. In particular, a transitive game always
has just one component in its decomposition, the potential component. We
provide a comprehensive evaluation of our methodology using both toy examples
and empirical data from real-world games.|
|**2023-06-08**|**Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models**|Nan Liu et.al.|[2306.05357v1](http://arxiv.org/abs/2306.05357v1)|null|Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.|
|**2023-06-08**|**ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**|Ge Yuan et.al.|[2306.05356v1](http://arxiv.org/abs/2306.05356v1)|[link](https://github.com/ygtxr1997/reliableswap)|Almost all advanced face swapping approaches use reconstruction as the proxy
task, i.e., supervision only exists when the target and source belong to the
same person. Otherwise, lacking pixel-level supervision, these methods struggle
for source identity preservation. This paper proposes to construct reliable
supervision, dubbed cycle triplets, which serves as the image-level guidance
when the source identity differs from the target one during training.
Specifically, we use face reenactment and blending techniques to synthesize the
swapped face from real images in advance, where the synthetic face preserves
source identity and target attributes. However, there may be some artifacts in
such a synthetic face. To avoid the potential artifacts and drive the
distribution of the network output close to the natural one, we reversely take
synthetic images as input while the real face as reliable supervision during
the training stage of face swapping. Besides, we empirically find that the
existing methods tend to lose lower-face details like face shape and mouth from
the source. This paper additionally designs a FixerNet, providing
discriminative embeddings of lower faces as an enhancement. Our face swapping
framework, named ReliableSwap, can boost the performance of any existing face
swapping network with negligible overhead. Extensive experiments demonstrate
the efficacy of our ReliableSwap, especially in identity preservation. The
project page is https://reliable-swap.github.io/.|
|**2023-06-08**|**Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features**|Wenwen Li et.al.|[2306.05341v1](http://arxiv.org/abs/2306.05341v1)|null|This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.|
|**2023-06-08**|**Categorical centers and Yetter--Drinfel`d-modules as 2-categorical (bi)lax structures**|Bojana Femi et.al.|[2306.05337v1](http://arxiv.org/abs/2306.05337v1)|null|The bicategorical point of view provides a natural setting for many concepts
in the representation theory of monoidal categories. We show that centers of
twisted bimodule categories correspond to categories of 2-dimensional natural
transformations and modifications between the deloopings of the twisting
functors. We also show that dualities lift to centers of twisted bimodule
categories. Inspired by the notion of (pre)bimonoidal functors due to McCurdy
and Street and by bilax functors of Aguiar and Mahajan, we study 2-dimensional
functors which are simultaneously lax and colax with a compatibility condition.
Our approach uses a sort of 2-categorical Yang-Baxter operators, but the idea
could equally be carried out using a kind of 2-categorical braidings. We show
how this concept, which we call bilax functors, generalize many known notions
from the theory of Hopf algebras. We propose a 2-category of bilax functors
whose 1-cells generalize the notions of Yetter-Drinfel`d modules in ordinary
categories, and a type of bimonads and mixed distributive laws in 2-categories.
We show that the 2-category of bilax functors from the trivial 2-category is
isomorphic to the 2-category of bimonads, and that there is a faithful
2-functor from the latter to the 2-category of mixed distributive laws of Power
and Watanabe.|
|**2023-06-08**|**Mode-locked laser in nanophotonic lithium niobate**|Qiushi Guo et.al.|[2306.05314v1](http://arxiv.org/abs/2306.05314v1)|null|Mode-locked lasers (MLLs) have enabled ultrafast sciences and technologies by
generating ultrashort pulses with peak powers substantially exceeding their
average powers. Recently, tremendous efforts have been focused on realizing
integrated MLLs not only to address the challenges associated with their size
and power demand, but also to enable transforming the ultrafast technologies
into nanophotonic chips, and ultimately to unlock their potential for a
plethora of applications. However, till now the prospect of integrated MLLs
driving ultrafast nanophotonic circuits has remained elusive because of their
typically low peak powers, lack of controllability, and challenges with
integration with appropriate nanophotonic platforms. Here, we overcome these
limitations by demonstrating an electrically-pumped actively MLL in
nanophotonic lithium niobate based on its hybrid integration with a III-V
semiconductor optical amplifier. Our MLL generates $\sim$4.8 ps optical pulses
around 1065 nm at a repetition rate of $\sim$10 GHz, with pulse energy
exceeding 2.6 pJ and a high peak power beyond 0.5 W. We show that both the
repetition rate and the carrier-envelope-offset of the resulting frequency comb
can be flexibly controlled in a wide range using the RF driving frequency and
the pump current, paving the way for fully-stabilized on-chip frequency combs
in nanophotonics. Our work marks an important step toward fully-integrated
nonlinear and ultrafast photonic systems in nanophotonic lithium niobate.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields**|Qianqiu Tan et.al.|[2306.05303v1](http://arxiv.org/abs/2306.05303v1)|null|The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF|
|**2023-06-08**|**Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis**|Gongshu Wang et.al.|[2306.05297v1](http://arxiv.org/abs/2306.05297v1)|null|Structural magnetic resonance imaging (sMRI) has shown great clinical value
and has been widely used in deep learning (DL) based computer-aided brain
disease diagnosis. Previous approaches focused on local shapes and textures in
sMRI that may be significant only within a particular domain. The learned
representations are likely to contain spurious information and have a poor
generalization ability in other diseases and datasets. To facilitate capturing
meaningful and robust features, it is necessary to first comprehensively
understand the intrinsic pattern of the brain that is not restricted within a
single data/task domain. Considering that the brain is a complex connectome of
interlinked neurons, the connectional properties in the brain have strong
biological significance, which is shared across multiple domains and covers
most pathological information. In this work, we propose a connectional style
contextual representation learning model (CS-CRL) to capture the intrinsic
pattern of the brain, used for multiple brain disease diagnosis. Specifically,
it has a vision transformer (ViT) encoder and leverages mask reconstruction as
the proxy task and Gram matrices to guide the representation of connectional
information. It facilitates the capture of global context and the aggregation
of features with biological plausibility. The results indicate that CS-CRL
achieves superior accuracy in multiple brain disease diagnosis tasks across six
datasets and three diseases and outperforms state-of-the-art models.
Furthermore, we demonstrate that CS-CRL captures more brain-network-like
properties, better aggregates features, is easier to optimize and is more
robust to noise, which explains its superiority in theory. Our source code will
be released soon.|

## Robotics

### Robotics
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**A Data-Driven Approach to Positioning Grab Bars in the Sagittal Plane for Elderly Persons**|Roberto Bolli Jr. et.al.|[2306.05343v1](http://arxiv.org/abs/2306.05343v1)|null|The placement of grab bars for elderly users is based largely on ADA building
codes and does not reflect the large differences in height, mobility, and
muscle power between individual persons. The goal of this study is to see if
there are any correlations between an elderly user's preferred handlebar pose
and various demographic indicators, self-rated mobility for tasks requiring
postural change, and biomechanical markers. For simplicity, we consider only
the case where the handlebar is positioned directly in front of the user, as
this confines the relevant body kinematics to a 2D sagittal plane. Previous
eldercare devices have been constructed to position a handlebar in various
poses in space. Our work augments these devices and adds to the body of
knowledge by assessing how the handlebar should be positioned based on data on
actual elderly people instead of simulations.|
|**2023-06-08**|**Research Impact of Solar Panel Cleaning Robot on Photovoltaic Panel's Deflection**|Trung Dat Phan et.al.|[2306.05340v1](http://arxiv.org/abs/2306.05340v1)|null|In the last few decades, solar panel cleaning robots (SPCR) have been widely
used for sanitizing photovoltaic (PV) panels as an effective solution for
ensuring PV efficiency. However, the dynamic load generated by the SPCR during
operation might have a negative impact on PV panels. To reduce these effects,
this paper presents the utilization of ANSYS software to simulate multiple
scenarios involving the impact of SPCR on PV panels. The simulation scenarios
provided in the paper are derived from the typical movements of SPCR observed
during practical operations. The simulation results show the deformation
process of PV panels, and a second-order polynomial is established to describe
the deformed amplitude along the centerline of PV panels. This second-order
polynomial contributes to the design process of a damper system for SPCR aiming
to reduce the influence of SPCR on PV panels. Moreover, the experiments are
conducted to examine the correlation between the results of the simulation and
the experiment.|
|**2023-06-08**|**Movement Optimization of Robotic Arms for Energy and Time Reduction using Evolutionary Algorithms**|Abolfazl Akbari et.al.|[2306.05329v1](http://arxiv.org/abs/2306.05329v1)|null|Trajectory optimization of a robot manipulator consists of both optimization
of the robot movement as well as optimization of the robot end-effector path.
This paper aims to find optimum movement parameters including movement type,
speed, and acceleration to minimize robot energy. Trajectory optimization by
minimizing the energy would increase the longevity of robotic manipulators. We
utilized the particle swarm optimization method to find the movement parameters
leading to minimum energy consumption. The effectiveness of the proposed method
is demonstrated on different trajectories. Experimental results show that 49%
efficiency was obtained using a UR5 robotic arm.|
|**2023-06-08**|**Perching by hugging: an initial feasibility study**|William Stewart et.al.|[2306.05324v1](http://arxiv.org/abs/2306.05324v1)|null|Current UAVs capable of perching require added structure and mechanisms to
accomplish this. These take the form of hooks, claws, needles, etc which add
weight and usually drag. We propose in this paper the dual use of structures
already on the vehicle to enable perching, thus reducing the weight and drag
cost associated with perching UAVs. We propose a wing design capable of
passively wrapping around a vertical pole to perch. We experimentally
investigate the feasibility of the design, presenting results on minimum
required perching speeds as well as the effect of weight distribution on the
success rate of the wing wrapping. Finally, we comment on design requirements
for holding onto the pole based on our findings.|
|**2023-06-08**|**SMUG Planner: A Safe Multi-Goal Planner for Mobile Robots in Challenging Environments**|Changan Chen et.al.|[2306.05309v1](http://arxiv.org/abs/2306.05309v1)|null|Robotic exploration or monitoring missions require mobile robots to
autonomously and safely navigate between multiple target locations in
potentially challenging environments. Currently, this type of multi-goal
mission often relies on humans designing a set of actions for the robot to
follow in the form of a path or waypoints. In this work, we consider the
multi-goal problem of visiting a set of pre-defined targets, each of which
could be visited from multiple potential locations. To increase autonomy in
these missions, we propose a safe multi-goal (SMUG) planner that generates an
optimal motion path to visit those targets. To increase safety and efficiency,
we propose a hierarchical state validity checking scheme, which leverages
robot-specific traversability learned in simulation. We use LazyPRM* with an
informed sampler to accelerate collision-free path generation. Our iterative
dynamic programming algorithm enables the planner to generate a path visiting
more than ten targets within seconds. Moreover, the proposed hierarchical state
validity checking scheme reduces the planning time by 30% compared to pure
volumetric collision checking and increases safety by avoiding high-risk
regions. We deploy the SMUG planner on the quadruped robot ANYmal and show its
capability to guide the robot in multi-goal missions fully autonomously on
rough terrain.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Efficient Multi-Task Scene Analysis with RGB-D Transformers**|Shnke Benedikt Fischedick et.al.|[2306.05242v1](http://arxiv.org/abs/2306.05242v1)|[link](https://github.com/tui-nicr/nicr-scene-analysis-datasets)|Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.|
|**2023-06-08**|**A cognitive process approach to modeling gap acceptance in overtaking**|Samir H. A. Mohammad et.al.|[2306.05203v1](http://arxiv.org/abs/2306.05203v1)|null|Driving automation holds significant potential for enhancing traffic safety.
However, effectively handling interactions with human drivers in mixed traffic
remains a challenging task. Several models exist that attempt to capture human
behavior in traffic interactions, often focusing on gap acceptance. However, it
is not clear how models of an individual driver's gap acceptance can be
translated to dynamic human-AV interactions in the context of high-speed
scenarios like overtaking. In this study, we address this issue by employing a
cognitive process approach to describe the dynamic interactions by the oncoming
vehicle during overtaking maneuvers. Our findings reveal that by incorporating
an initial decision-making bias dependent on the initial velocity into existing
drift-diffusion models, we can accurately describe the qualitative patterns of
overtaking gap acceptance observed previously. Our results demonstrate the
potential of the cognitive process approach in modeling human overtaking
behavior when the oncoming vehicle is an AV. To this end, this study
contributes to the development of effective strategies for ensuring safe and
efficient overtaking interactions between human drivers and AVs.|
|**2023-06-08**|**Time-Optimal Path Tracking with ISO Safety Guarantees**|Shohei Fujii et.al.|[2306.05197v1](http://arxiv.org/abs/2306.05197v1)|null|One way of ensuring operator's safety during human-robot collaboration is
through Speed and Separation Monitoring (SSM), as defined in ISO standard
ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions:
consider for instance the case when the robot does not move at all, a human
operator can still collide with it by hitting it of her own voluntary motion.
In the SSM framework, it is possible however to minimize harm by requiring
this: \emph{if} a collision ever occurs, then the robot must be in a
\emph{stationary state} (all links have zero velocity) at the time instant of
the collision. In this paper, we propose a time-optimal control policy based on
Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior.
Specifically, we show that: for any robot motion that is strictly faster than
the motion recommended by our policy, there exists a human motion that results
in a collision with the robot in a non-stationary state. Correlatively, we
show, in simulation, that our policy is strictly less conservative than
state-of-the-art safe robot control methods. Additionally, we propose a
parallelization method to reduce the computation time of our pre-computation
phase (down to 0.5 sec, practically), which enables the whole pipeline
(including the pre-computation) to be executed at runtime, nearly in real-time.
Finally, we demonstrate the application of our method in a scenario:
time-optimal, safe control of a 6-dof industrial robot.|
|**2023-06-08**|**Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures**|Yue Zhen et.al.|[2306.05171v1](http://arxiv.org/abs/2306.05171v1)|[link](https://github.com/nomizy/think_net_prompt)|Traditional robot task planning methods face challenges when dealing with
highly unstructured environments and complex tasks. We propose a task planning
method that combines human expertise with an LLM and have designed an LLM
prompt template, Think_Net_Prompt, with stronger expressive power to represent
structured professional knowledge. We further propose a method to progressively
decompose tasks and generate a task tree to reduce the planning volume for each
task, and we have designed a strategy to decouple robot task planning. By
dividing different planning entities and separating the task from the actual
machine binding process, the task planning process becomes more flexible.
Research results show that our method performs well in handling specified code
formats, understanding the relationship between tasks and subtasks, and
extracting parameters from text descriptions. However, there are also problems
such as limited complexity of task logic handling, ambiguity in the quantity of
parts and the precise location of assembly. Improving the precision of task
description and cognitive structure can bring certain improvements.
https://github.com/NOMIzy/Think_Net_Prompt|
|**2023-06-08**|**AutoCharge: Autonomous Charging for Perpetual Quadrotor Missions**|Alessandro Saviolo et.al.|[2306.05111v1](http://arxiv.org/abs/2306.05111v1)|null|Battery endurance represents a key challenge for long-term autonomy and
long-range operations, especially in the case of aerial robots. In this paper,
we propose AutoCharge, an autonomous charging solution for quadrotors that
combines a portable ground station with a flexible, lightweight charging tether
and is capable of universal, highly efficient, and robust charging. We design
and manufacture a pair of circular magnetic connectors to ensure a precise
orientation-agnostic electrical connection between the ground station and the
charging tether. Moreover, we supply the ground station with an electromagnet
that largely increases the tolerance to localization and control errors during
the docking maneuver, while still guaranteeing smooth un-docking once the
charging process is completed. We demonstrate AutoCharge on a perpetual 10
hours quadrotor flight experiment and show that the docking and un-docking
performance is solidly repeatable, enabling perpetual quadrotor flight
missions.|
|**2023-06-08**|**Motion Planning for Aerial Pick-and-Place based on Geometric Feasibility Constraints**|Huazi Cao et.al.|[2306.04970v1](http://arxiv.org/abs/2306.04970v1)|null|This paper studies the motion planning problem of the pick-and-place of an
aerial manipulator that consists of a quadcopter flying base and a Delta arm.
We propose a novel partially decoupled motion planning framework to solve this
problem. Compared to the state-of-the-art approaches, the proposed one has two
novel features. First, it does not suffer from increased computation in
high-dimensional configuration spaces. That is because it calculates the
trajectories of the quadcopter base and the end-effector separately in the
Cartesian space based on proposed geometric feasibility constraints. The
geometric feasibility constraints can ensure the resulting trajectories satisfy
the aerial manipulator's geometry. Second, collision avoidance for the Delta
arm is achieved through an iterative approach based on a pinhole mapping
method, so that the feasible trajectory can be found in an efficient manner.
The proposed approach is verified by three experiments on a real aerial
manipulation platform. The experimental results show the effectiveness of the
proposed method for the aerial pick-and-place task.|
|**2023-06-08**|**UAP-BEV: Uncertainty Aware Planning using Bird's Eye View generated from Surround Monocular Images**|Vikrant Dewangan et.al.|[2306.04939v1](http://arxiv.org/abs/2306.04939v1)|null|Autonomous driving requires accurate reasoning of the location of objects
from raw sensor data. Recent end-to-end learning methods go from raw sensor
data to a trajectory output via Bird's Eye View(BEV) segmentation as an
interpretable intermediate representation. Motion planning over cost maps
generated via Birds Eye View (BEV) segmentation has emerged as a prominent
approach in autonomous driving. However, the current approaches have two
critical gaps. First, the optimization process is simplistic and involves just
evaluating a fixed set of trajectories over the cost map. The trajectory
samples are not adapted based on their associated cost values. Second, the
existing cost maps do not account for the uncertainty in the cost maps that can
arise due to noise in RGB images, and BEV annotations. As a result, these
approaches can struggle in challenging scenarios where there is abrupt cut-in,
stopping, overtaking, merging, etc from the neighboring vehicles.
  In this paper, we propose UAP-BEV: A novel approach that models the noise in
Spatio-Temporal BEV predictions to create an uncertainty-aware occupancy grid
map. Using queries of the distance to the closest occupied cell, we obtain a
sample estimate of the collision probability of the ego-vehicle. Subsequently,
our approach uses gradient-free sampling-based optimization to compute low-cost
trajectories over the cost map. Importantly, the sampling distribution is
adapted based on the optimal cost values of the sampled trajectories. By
explicitly modeling probabilistic collision avoidance in the BEV space, our
approach is able to outperform the cost-map-based baselines in collision
avoidance, route completion, time to completion, and smoothness. To further
validate our method, we also show results on the real-world dataset NuScenes,
where we report improvements in collision avoidance and smoothness.|
|**2023-06-08**|**Combined Left and Right Temporal Robustness for Control under STL Specifications**|Alna Rodionova et.al.|[2306.04936v1](http://arxiv.org/abs/2306.04936v1)|null|Many modern autonomous systems, particularly multi-agent systems, are
time-critical and need to be robust against timing uncertainties. Previous
works have studied left and right time robustness of signal temporal logic
specifications by considering time shifts in the predicates that are either
only to the left or only to the right. We propose a combined notion of temporal
robustness which simultaneously considers left and right time shifts. For
instance, in a scenario where a robot plans a trajectory around a pedestrian,
this combined notion can now capture uncertainty of the pedestrian arriving
earlier or later than anticipated. We first derive desirable properties of this
new notion with respect to left and right time shifts and then design control
laws for linear systems that maximize temporal robustness using mixed-integer
linear programming. Finally, we present two case studies to illustrate how the
proposed temporal robustness accounts for timing uncertainties.|
|**2023-06-08**|**Jigsaw-based Benchmarking for Learning Robotic Manipulation**|Xiaobo Liu et.al.|[2306.04932v1](http://arxiv.org/abs/2306.04932v1)|null|Benchmarking provides experimental evidence of the scientific baseline to
enhance the progression of fundamental research, which is also applicable to
robotics. In this paper, we propose a method to benchmark metrics of robotic
manipulation, which addresses the spatial-temporal reasoning skills for robot
learning with the jigsaw game. In particular, our approach exploits a simple
set of jigsaw pieces by designing a structured protocol, which can be highly
customizable according to a wide range of task specifications. Researchers can
selectively adopt the proposed protocol to benchmark their research outputs, on
a comparable scale in the functional, task, and system-level of details. The
purpose is to provide a potential look-up table for learning-based robot
manipulation, commonly available in other engineering disciplines, to
facilitate the adoption of robotics through calculated, empirical, and
systematic experimental evidence.|
|**2023-06-08**|**Underwater Intention Recognition using Head Motion and Throat Vibration for Supernumerary Robotic Assistance**|Yuqin Guo et.al.|[2306.04928v1](http://arxiv.org/abs/2306.04928v1)|null|This study presents a multi-modal mechanism for recognizing human intentions
while diving underwater, aiming to achieve natural human-robot interactions
through an underwater superlimb for diving assistance. The underwater
environment severely limits the divers' capabilities in intention expression,
which becomes more challenging when they intend to operate tools while keeping
control of body postures in 3D with the various diving suits and gears. The
current literature is limited in underwater intention recognition, impeding the
development of intelligent wearable systems for human-robot interactions
underwater. Here, we present a novel solution to simultaneously detect head
motion and throat vibrations under the water in a compact, wearable design.
Experiment results show that using machine learning algorithms, we achieved
high performance in integrating these two modalities to translate human
intentions to robot control commands for an underwater superlimb system. This
study's results paved the way for future development in underwater intention
recognition and underwater human-robot interactions with supernumerary support.|
|**2023-06-08**|**Local Map-Based DQN Navigation and a Transferability Metric Using Scene Similarity**|Shiwei Lian et.al.|[2306.04910v1](http://arxiv.org/abs/2306.04910v1)|null|Autonomous navigation in unknown environments without a global map is a
long-standing challenge for mobile robots. While deep reinforcement learning
(DRL) has attracted a rapidly growing interest in solving such an autonomous
navigation problem for its generalization capability, DRL typically leads to a
mediocre navigation performance in practice due to the gap between the training
scene and the actual test scene. Most existing work focuses on tuning the
algorithm to enhance its transferability, whereas few investigates how to
quantify or measure the gap therebetween. This letter presents a local
map-based deep Q-network (DQN) navigation algorithm, which uses local maps
converted from 2D LiDAR data as observations without a global map. More
importantly, this letter proposes a new transferability metric -- the scene
similarity calculated from an improved image template matching algorithm to
measure the similarity between the training and test scenes. With a wheeled
robot as the case study platform, both simulation and real-world experiments
are conducted in a total of 20 different scenes. The case study results
successfully validate the local map-based navigation algorithm as well as the
similarity metric in predicting the transferability or success rate of the
algorithm.|
|**2023-06-08**|**Spectrum Sharing between High Altitude Platform Network and Terrestrial Network: Modeling and Performance Analysis**|Zhiqing Wei et.al.|[2306.04906v1](http://arxiv.org/abs/2306.04906v1)|null|Achieving seamless global coverage is one of the ultimate goals of
space-air-ground integrated network, as a part of which High Altitude Platform
(HAP) network can provide wide-area coverage. However, deploying a large number
of HAPs will lead to severe congestion of existing frequency bands. Spectrum
sharing improves spectrum utilization. The coverage performance improvement and
interference caused by spectrum sharing need to be investigated. To this end,
this paper analyzes the performance of spectrum sharing between HAP network and
terrestrial network. We firstly generalize the Poisson Point Process (PPP) to
curves, surfaces and manifolds to model the distribution of terrestrial Base
Stations (BSs) and HAPs. Then, the closed-form expressions for coverage
probability of HAP network and terrestrial network are derived based on
differential geometry and stochastic geometry. We verify the accuracy of
closed-form expressions by Monte Carlo simulation. The results show that HAP
network has less interference to terrestrial network. Low height and suitable
deployment density can improve the coverage probability and transmission
capacity of HAP network.|
|**2023-06-08**|**The Hybrid Extended Bicycle: A Simple Model for High Dynamic Vehicle Trajectory Planning**|Agapius Bou Ghosn et.al.|[2306.04857v1](http://arxiv.org/abs/2306.04857v1)|null|While highly automated driving relies most of the time on a smooth driving
assumption, the possibility of a vehicle performing harsh maneuvers with high
dynamic driving to face unexpected events is very likely. The modeling of the
behavior of the vehicle in these events is crucial to proper planning and
controlling; the used model should present accurate and computationally
efficient properties. In this article, we propose an LSTM-based hybrid extended
bicycle model able to present an accurate description of the state of the
vehicle for both normal and aggressive situations. The introduced model is used
in an MPPI framework for planning trajectories in high-dynamic scenarios where
other simple models fail.|
|**2023-06-08**|**ExtPerFC: An Efficient 2D and 3D Perception Hardware-Software Framework for Mobile Cobot**|Tuan Dang et.al.|[2306.04853v1](http://arxiv.org/abs/2306.04853v1)|null|As the reliability of the robot's perception correlates with the number of
integrated sensing modalities to tackle uncertainty, a practical solution to
manage these sensors from different computers, operate them simultaneously, and
maintain their real-time performance on the existing robotic system with
minimal effort is needed. In this work, we present an end-to-end
software-hardware framework, namely ExtPerFC, that supports both conventional
hardware and software components and integrates machine learning object
detectors without requiring an additional dedicated graphic processor unit
(GPU). We first design our framework to achieve real-time performance on the
existing robotic system, guarantee configuration optimization, and concentrate
on code reusability. We then mathematically model and utilize our transfer
learning strategies for 2D object detection and fuse them into depth images for
3D depth estimation. Lastly, we systematically test the proposed framework on
the Baxter robot with two 7-DOF arms, a four-wheel mobility base, and an Intel
RealSense D435i RGB-D camera. The results show that the robot achieves
real-time performance while executing other tasks (e.g., map building,
localization, navigation, object detection, arm moving, and grasping)
simultaneously with available hardware like Intel onboard CPUS/GPUs on
distributed computers. Also, to comprehensively control, program, and monitor
the robot system, we design and introduce an end-user application. The source
code is available at https://github.com/tuantdang/perception_framework.|
|**2023-06-07**|**A Semi-supervised Object Detection Algorithm for Underwater Imagery**|Suraj Bijjahalli et.al.|[2306.04834v1](http://arxiv.org/abs/2306.04834v1)|null|Detection of artificial objects from underwater imagery gathered by
Autonomous Underwater Vehicles (AUVs) is a key requirement for many subsea
applications. Real-world AUV image datasets tend to be very large and
unlabelled. Furthermore, such datasets are typically imbalanced, containing few
instances of objects of interest, particularly when searching for unusual
objects in a scene. It is therefore, difficult to fit models capable of
reliably detecting these objects. Given these factors, we propose to treat
artificial objects as anomalies and detect them through a semi-supervised
framework based on Variational Autoencoders (VAEs). We develop a method which
clusters image data in a learned low-dimensional latent space and extracts
images that are likely to contain anomalous features. We also devise an anomaly
score based on extracting poorly reconstructed regions of an image. We
demonstrate that by applying both methods on large image datasets, human
operators can be shown candidate anomalous samples with a low false positive
rate to identify objects of interest. We apply our approach to real seafloor
imagery gathered by an AUV and evaluate its sensitivity to the dimensionality
of the latent representation used by the VAE. We evaluate the precision-recall
tradeoff and demonstrate that by choosing an appropriate latent dimensionality
and threshold, we are able to achieve an average precision of 0.64 on
unlabelled datasets.|
|**2023-06-07**|**A Framework for Designing Anthropomorphic Soft Hands through Interaction**|Pragna Mannam et.al.|[2306.04784v1](http://arxiv.org/abs/2306.04784v1)|null|Modeling and simulating soft robot hands can aid in design iteration for
complex and high degree-of-freedom (DoF) morphologies. This can be further
supplemented by iterating on the design based on its performance in real world
manipulation tasks. However, this requires a framework that allows us to
iterate quickly at low costs. In this paper, we present a framework that
leverages rapid prototyping of the hand using 3D-printing, and utilizes
teleoperation to evaluate the hand in real world manipulation tasks. Using this
framework, we design a 3D-printed 16-DoF dexterous anthropomorphic soft hand
(DASH) and iteratively improve its design over three iterations. Rapid
prototyping techniques such as 3D-printing allow us to directly evaluate the
fabricated hand without modeling it in simulation. We show that the design is
improved at each iteration through the hand's performance in 30 real-world
teleoperated manipulation tasks. Testing over 600 demonstrations shows that our
final version of DASH can solve 16 of the 30 tasks compared to Allegro, a
popular rigid hand in the market, which can only solve 7 tasks. We open-source
our CAD models as well as the teleoperated dataset for further study and are
available on our website (https://dash-through-interaction.github.io.)|
|**2023-06-07**|**Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach**|Diego Patio et.al.|[2306.04781v1](http://arxiv.org/abs/2306.04781v1)|null|Aerial operation in turbulent environments is a challenging problem due to
the chaotic behavior of the flow. This problem is made even more complex when a
team of aerial robots is trying to achieve coordinated motion in turbulent wind
conditions. In this paper, we present a novel multi-robot controller to
navigate in turbulent flows, decoupling the trajectory-tracking control from
the turbulence compensation via a nested control architecture. Unlike previous
works, our method does not learn to compensate for the air-flow at a specific
time and space. Instead, our method learns to compensate for the flow based on
its effect on the team. This is made possible via a deep reinforcement learning
approach, implemented via a Graph Convolutional Neural Network (GCNN)-based
architecture, which enables robots to achieve better wind compensation by
processing the spatial-temporal correlation of wind flows across the team. Our
approach scales well to large robot teams -- as each robot only uses
information from its nearest neighbors -- , and generalizes well to robot teams
larger than seen in training. Simulated experiments demonstrate how information
sharing improves turbulence compensation in a team of aerial robots and
demonstrate the flexibility of our method over different team configurations.|
|**2023-06-07**|**Online Multi-Contact Receding Horizon Planning via Value Function Approximation**|Jiayi Wang et.al.|[2306.04732v1](http://arxiv.org/abs/2306.04732v1)|null|Planning multi-contact motions in a receding horizon fashion requires a value
function to guide the planning with respect to the future, e.g., building
momentum to traverse large obstacles. Traditionally, the value function is
approximated by computing trajectories in a prediction horizon (never executed)
that foresees the future beyond the execution horizon. However, given the
non-convex dynamics of multi-contact motions, this approach is computationally
expensive. To enable online Receding Horizon Planning (RHP) of multi-contact
motions, we find efficient approximations of the value function. Specifically,
we propose a trajectory-based and a learning-based approach. In the former,
namely RHP with Multiple Levels of Model Fidelity, we approximate the value
function by computing the prediction horizon with a convex relaxed model. In
the latter, namely Locally-Guided RHP, we learn an oracle to predict local
objectives for locomotion tasks, and we use these local objectives to construct
local value functions for guiding a short-horizon RHP. We evaluate both
approaches in simulation by planning centroidal trajectories of a humanoid
robot walking on moderate slopes, and on large slopes where the robot cannot
maintain static balance. Our results show that locally-guided RHP achieves the
best computation efficiency (95\%-98.6\% cycles converge online). This
computation advantage enables us to demonstrate online receding horizon
planning of our real-world humanoid robot Talos walking in dynamic environments
that change on-the-fly.|
|**2023-06-07**|**Generalization Across Observation Shifts in Reinforcement Learning**|Anuj Mahajan et.al.|[2306.04595v1](http://arxiv.org/abs/2306.04595v1)|null|Learning policies which are robust to changes in the environment are critical
for real world deployment of Reinforcement Learning agents. They are also
necessary for achieving good generalization across environment shifts. We focus
on bisimulation metrics, which provide a powerful means for abstracting task
relevant components of the observation and learning a succinct representation
space for training the agent using reinforcement learning. In this work, we
extend the bisimulation framework to also account for context dependent
observation shifts. Specifically, we focus on the simulator based learning
setting and use alternate observations to learn a representation space which is
invariant to observation shifts using a novel bisimulation based objective.
This allows us to deploy the agent to varying observation settings during test
time and generalize to unseen scenarios. We further provide novel theoretical
bounds for simulator fidelity and performance transfer guarantees for using a
learnt policy to unseen shifts. Empirical analysis on the high-dimensional
image based control domains demonstrates the efficacy of our method.|
|**2023-06-07**|**Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking**|Ofer Dagan et.al.|[2306.04570v1](http://arxiv.org/abs/2306.04570v1)|null|In many robotics problems, there is a significant gain in collaborative
information sharing between multiple robots, for exploration, search and
rescue, tracking multiple targets, or mapping large environments. One of the
key implicit assumptions when solving cooperative multi-robot problems is that
all robots use the same (homogeneous) underlying algorithm. However, in
practice, we want to allow collaboration between robots possessing different
capabilities and that therefore must rely on heterogeneous algorithms. We
present a system architecture and the supporting theory, to enable
collaboration in a decentralized network of robots, where each robot relies on
different estimation algorithms. To develop our approach, we focus on
multi-robot simultaneous localization and mapping (SLAM) with multi-target
tracking. Our theoretical framework builds on our idea of exploiting the
conditional independence structure inherent to many robotics applications to
separate between each robot's local inference (estimation) tasks and fuse only
relevant parts of their non-equal, but overlapping probability density function
(pdfs). We present a new decentralized graph-based approach to the multi-robot
SLAM and tracking problem. We leverage factor graphs to split between different
parts of the problem for efficient data sharing between robots in the network
while enabling robots to use different local sparse
landmark/dense/metric-semantic SLAM algorithms.|
|**2023-06-07**|**PhenoBench -- A Large Dataset and Benchmarks for Semantic Image Interpretation in the Agricultural Domain**|Jan Weyler et.al.|[2306.04557v1](http://arxiv.org/abs/2306.04557v1)|null|The production of food, feed, fiber, and fuel is a key task of agriculture.
Especially crop production has to cope with a multitude of challenges in the
upcoming decades caused by a growing world population, climate change, the need
for sustainable production, lack of skilled workers, and generally the limited
availability of arable land. Vision systems could help cope with these
challenges by offering tools to make better and more sustainable field
management decisions and support the breeding of new varieties of crops by
allowing temporally dense and reproducible measurements. Recently, tackling
perception tasks in the agricultural domain got increasing interest in the
computer vision and robotics community since agricultural robotics are one
promising solution for coping with the lack of workers and enable a more
sustainable agricultural production at the same time. While large datasets and
benchmarks in other domains are readily available and have enabled significant
progress toward more reliable vision systems, agricultural datasets and
benchmarks are comparably rare. In this paper, we present a large dataset and
benchmarks for the semantic interpretation of images of real agricultural
fields. Our dataset recorded with a UAV provides high-quality, dense
annotations of crops and weeds, but also fine-grained labels of crop leaves at
the same time, which enable the development of novel algorithms for visual
perception in the agricultural domain. Together with the labeled data, we
provide novel benchmarks for evaluating different visual perception tasks on a
hidden test set comprised of different fields: known fields covered by the
training data and a completely unseen field. The tasks cover semantic
segmentation, panoptic segmentation of plants, leaf instance segmentation,
detection of plants and leaves, and hierarchical panoptic segmentation for
jointly identifying plants and leaves.|
|**2023-06-07**|**RotorPy: A Python-based Multirotor Simulator with Aerodynamics for Education and Research**|Spencer Folk et.al.|[2306.04485v1](http://arxiv.org/abs/2306.04485v1)|null|Simulators play a critical role in aerial robotics both in and out of the
classroom. We present RotorPy, a simulation environment written entirely in
Python intentionally designed to be a lightweight and accessible tool for
robotics students and researchers alike to probe concepts in estimation,
planning, and control for aerial robots. RotorPy simulates the 6-DoF dynamics
of a multirotor robot including aerodynamic wrenches, obstacles, actuator
dynamics and saturation, realistic sensors, and wind models. This work
describes the modeling choices for RotorPy, benchmark testing against real
data, and a case study using the simulator to design and evaluate a model-based
wind estimator.|
|**2023-06-07**|**STEPS: A Benchmark for Order Reasoning in Sequential Tasks**|Weizhi Wang et.al.|[2306.04441v1](http://arxiv.org/abs/2306.04441v1)|null|Various human activities can be abstracted into a sequence of actions in
natural text, i.e. cooking, repairing, manufacturing, etc. Such action
sequences heavily depend on the executing order, while disorder in action
sequences leads to failure of further task execution by robots or AI agents.
Therefore, to verify the order reasoning capability of current neural models in
sequential tasks, we propose a challenging benchmark , named STEPS. STEPS
involves two subtask settings, focusing on determining the rationality of given
next step in recipes and selecting the reasonable step from the multi-choice
question, respectively. We describe the data construction and task
formulations, and benchmark most of significant Large Language Models (LLMs).
The experimental results demonstrate 1) The commonsense reasoning of action
orders in sequential tasks are challenging to resolve via zero-shot prompting
or few-shot in-context learning for LLMs; 2) Prompting method still
significantly lags behind tuning-based method on STEPS.|

### SLAM
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-07**|**Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking**|Ofer Dagan et.al.|[2306.04570v1](http://arxiv.org/abs/2306.04570v1)|null|In many robotics problems, there is a significant gain in collaborative
information sharing between multiple robots, for exploration, search and
rescue, tracking multiple targets, or mapping large environments. One of the
key implicit assumptions when solving cooperative multi-robot problems is that
all robots use the same (homogeneous) underlying algorithm. However, in
practice, we want to allow collaboration between robots possessing different
capabilities and that therefore must rely on heterogeneous algorithms. We
present a system architecture and the supporting theory, to enable
collaboration in a decentralized network of robots, where each robot relies on
different estimation algorithms. To develop our approach, we focus on
multi-robot simultaneous localization and mapping (SLAM) with multi-target
tracking. Our theoretical framework builds on our idea of exploiting the
conditional independence structure inherent to many robotics applications to
separate between each robot's local inference (estimation) tasks and fuse only
relevant parts of their non-equal, but overlapping probability density function
(pdfs). We present a new decentralized graph-based approach to the multi-robot
SLAM and tracking problem. We leverage factor graphs to split between different
parts of the problem for efficient data sharing between robots in the network
while enabling robots to use different local sparse
landmark/dense/metric-semantic SLAM algorithms.|
|**2023-06-06**|**Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping**|Manon Kok et.al.|[2306.03953v1](http://arxiv.org/abs/2306.03953v1)|null|Simultaneous localization and mapping (SLAM) is the task of building a map
representation of an unknown environment while it at the same time is used for
positioning. A probabilistic interpretation of the SLAM task allows for
incorporating prior knowledge and for operation under uncertainty. Contrary to
the common practice of computing point estimates of the system states, we
capture the full posterior density through approximate Bayesian inference. This
dynamic learning task falls under state estimation, where the state-of-the-art
is in sequential Monte Carlo methods that tackle the forward filtering problem.
In this paper, we introduce a framework for probabilistic SLAM using particle
smoothing that does not only incorporate observed data in current state
estimates, but it also back-tracks the updated knowledge to correct for past
drift and ambiguities in both the map and in the states. Our solution can
efficiently handle both dense and sparse map representations by
Rao-Blackwellization of conditionally linear and conditionally linearized
models. We show through simulations and real-world experiments how the
principles apply to radio (BLE/Wi-Fi), magnetic field, and visual SLAM. The
proposed solution is general, efficient, and works well under confounding
noise.|
|**2023-06-06**|**PQM: A Point Quality Evaluation Metric for Dense Maps**|Yash Turkar et.al.|[2306.03660v1](http://arxiv.org/abs/2306.03660v1)|null|LiDAR-based mapping/reconstruction are important for various applications,
but evaluating the quality of the dense maps they produce is challenging. The
current methods have limitations, including the inability to capture
completeness, structural information, and local variations in error. In this
paper, we propose a novel point quality evaluation metric (PQM) that consists
of four sub-metrics to provide a more comprehensive evaluation of point cloud
quality. The completeness sub-metric evaluates the proportion of missing data,
the artifact score sub-metric recognizes and characterizes artifacts, the
accuracy sub-metric measures registration accuracy, and the resolution
sub-metric quantifies point cloud density. Through an ablation study using a
prototype dataset, we demonstrate the effectiveness of each of the sub-metrics
and compare them to popular point cloud distance measures. Using three LiDAR
SLAM systems to generate maps, we evaluate their output map quality and
demonstrate the metrics robustness to noise and artifacts. Our implementation
of PQM, datasets and detailed documentation on how to integrate with your
custom dense mapping pipeline can be found at github.com/droneslab/pqm|
|**2023-06-04**|**NICE-SLAM with Adaptive Feature Grids**|Ganlin Zhang et.al.|[2306.02395v1](http://arxiv.org/abs/2306.02395v1)|[link](https://github.com/zhangganlin/nice-slam-with-adaptive-feature-grids)|NICE-SLAM is a dense visual SLAM system that combines the advantages of
neural implicit representations and hierarchical grid-based scene
representation. However, the hierarchical grid features are densely stored,
leading to memory explosion problems when adapting the framework to large
scenes. In our project, we present sparse NICE-SLAM, a sparse SLAM system
incorporating the idea of Voxel Hashing into NICE-SLAM framework. Instead of
initializing feature grids in the whole space, voxel features near the surface
are adaptively added and optimized. Experiments demonstrated that compared to
NICE-SLAM algorithm, our approach takes much less memory and achieves
comparable reconstruction quality on the same datasets. Our implementation is
available at
https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.|
|**2023-06-02**|**DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System**|Abanob Soliman et.al.|[2306.01891v1](http://arxiv.org/abs/2306.01891v1)|null|This paper presents a robust approach for a visual parallel tracking and
mapping (PTAM) system that excels in challenging environments. Our proposed
method combines the strengths of heterogeneous multi-modal visual sensors,
including stereo event-based and frame-based sensors, in a unified reference
frame through a novel spatio-temporal synchronization of stereo visual frames
and stereo event streams. We employ deep learning-based feature extraction and
description for estimation to enhance robustness further. We also introduce an
end-to-end parallel tracking and mapping optimization layer complemented by a
simple loop-closure algorithm for efficient SLAM behavior. Through
comprehensive experiments on both small-scale and large-scale real-world
sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM)
demonstrates superior performance compared to state-of-the-art methods in terms
of robustness and accuracy in adverse conditions. Our implementation's
research-based Python API is publicly available on GitHub for further research
and development: https://github.com/AbanobSoliman/DH-PTAM.|
|**2023-06-01**|**BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms**|Wei Zhang et.al.|[2306.01173v1](http://arxiv.org/abs/2306.01173v1)|null|In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial
SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms
(RFT) to achieve accurate and robust state estimation in challenging scenarios.
First, our system directly operates on raw fisheye images, enabling us to fully
exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome
the low-texture challenge, we explore the tightly-coupled integration of
multi-camera inputs and complementary inertial measurements via a unified
factor graph and jointly optimize the poses and dense depth maps. Third, for
global consistency, the wide FoV of the fisheye camera allows the system to
find more potential loop closures, and powered by the broad convergence basin
of RFT, our system can perform very wide baseline loop closing with little
overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the
expensive full global BA. By combining relative pose factors with loop closure
factors, the global states can be adjusted efficiently with modest memory
footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford
and Newer College datasets show the superior performance of the proposed system
over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves
second place. In a subsequent submission, our complete system, including the
global BA backend, outperforms the winning approach.|
|**2023-06-01**|**FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM**|Tongyan Hua et.al.|[2306.00579v1](http://arxiv.org/abs/2306.00579v1)|null|In this paper, we introduce FMapping, an efficient neural field mapping
framework that facilitates the continuous estimation of a colorized point cloud
map in real-time dense RGB SLAM. To achieve this challenging goal without
depth, a hurdle is how to improve efficiency and reduce the mapping uncertainty
of the RGB SLAM system. To this end, we first build up a theoretical analysis
by decomposing the SLAM system into tracking and mapping parts, and the mapping
uncertainty is explicitly defined within the frame of neural representations.
Based on the analysis, we then propose an effective factorization scheme for
scene representation and introduce a sliding window strategy to reduce the
uncertainty for scene reconstruction. Specifically, we leverage the factorized
neural field to decompose uncertainty into a lower-dimensional space, which
enhances robustness to noise and improves training efficiency. We then propose
the sliding window sampler to reduce uncertainty by incorporating coherent
geometric cues from observed frames during map initialization to enhance
convergence. Our factorized neural mapping approach enjoys some advantages,
such as low memory consumption, more efficient computation, and fast
convergence during map initialization. Experiments on two benchmark datasets
show that our method can update the map of high-fidelity colorized point clouds
around 2 seconds in real time while requiring no customized CUDA kernels.
Additionally, it utilizes x20 fewer parameters than the most concise neural
implicit mapping of prior methods for SLAM, e.g., iMAP [ 31] and around x1000
fewer parameters than the state-of-the-art approach, e.g., NICE-SLAM [ 42]. For
more details, please refer to our project homepage:
https://vlis2022.github.io/fmap/.|
|**2023-05-31**|**On the relevance of fatigue in the risk of failure of marine structures exposed to bottom wave slamming**|Romain Hascot et.al.|[2305.19899v1](http://arxiv.org/abs/2305.19899v1)|null|The aim of the present study is to investigate whether fatigue damage induced
by bottom wave slamming is a failure channel, important to consider when sizing
a marine structure. The shape and structural arrangement of the body exposed to
water-wave slamming are considered to be such that the rise time of wave-impact
hydrodynamic loads, $t_{\rm on}$, is short compared to the vibratory response
timescale, $t_{\rm vib}$, of the structure, $t_{\rm on} \ll t_{\rm vib}$.
Without further specifying the details of the body, two asymptotic structural
response regimes are considered: (i) a first regime where the typical wave
impact duration, $t_{\rm imp}$, is much smaller than the vibratory response
timescale, $t_{\rm imp} \ll t_{\rm vib}$; (ii) a second opposite regime where
$t_{\rm imp} \gg t_{\rm vib}$. In the first regime, fatigue is found to be the
dominant failure mechanism, and accounting for the risk of failure due to
fatigue damage yields design constraints which are significantly more
conservative than the constraints obtained from the risk of ultimate strength
exceedance. A detailed modeling of the risk of failure in the second response
regime poses various challenges; in this regime, our results suggest that the
risk of yield stress exceedance may be used as an alternative design criterion.|
|**2023-05-28**|**OSPC: Online Sequential Photometric Calibration**|Jawad Haidar et.al.|[2305.17673v1](http://arxiv.org/abs/2305.17673v1)|null|Photometric calibration is essential to many computer vision applications.
One of its key benefits is enhancing the performance of Visual SLAM, especially
when it depends on a direct method for tracking, such as the standard KLT
algorithm. Another advantage could be in retrieving the sensor irradiance
values from measured intensities, as a pre-processing step for some vision
algorithms, such as shape-from-shading. Current photometric calibration systems
rely on a joint optimization problem and encounter an ambiguity in the
estimates, which can only be resolved using ground truth information. We
propose a novel method that solves for photometric parameters using a
sequential estimation approach. Our proposed method achieves high accuracy in
estimating all parameters; furthermore, the formulations are linear and convex,
which makes the solution fast and suitable for online applications. Experiments
on a Visual Odometry system validate the proposed method and demonstrate its
advantages.|
|**2023-05-24**|**UAV Trajectory Optimization and Tracking for User Localization in Wireless Networks**|Omid Esrafilian et.al.|[2305.14959v1](http://arxiv.org/abs/2305.14959v1)|null|In this paper, we investigate the problem of UAV-aided user localization in
wireless networks. Unlike the existing works, we do not assume perfect
knowledge of the UAV location, hence we not only need to localize the users but
also to track the UAV location. To do so, we utilize the time-of-arrival along
with received signal strength radio measurements collected from users using a
UAV. A simultaneous localization and mapping (SLAM) framework building on the
Expectation-Maximization-based least-squares method is proposed to classify
measurements into line-of-sight or non-line-of-sight categories and learn the
radio channel, and at the same, localize the users and track the UAV. This
framework also allows us to exploit other types of measurements such as the
rough estimate of the UAV location available from GPS, and the UAV velocity
measured by an inertial measurement unit (IMU) on-board, to achieve better
localization accuracy. Moreover, the trajectory of the UAV is optimized which
brings considerable improvement to the localization performance. The
simulations show the out-performance of the developed algorithm when compared
to other approaches.|
|**2023-05-24**|**Towards View-invariant and Accurate Loop Detection Based on Scene Graph**|Chuhao Liu et.al.|[2305.14885v1](http://arxiv.org/abs/2305.14885v1)|null|Loop detection plays a key role in visual Simultaneous Localization and
Mapping (SLAM) by correcting the accumulated pose drift. In indoor scenarios,
the richly distributed semantic landmarks are view-point invariant and hold
strong descriptive power in loop detection. The current semantic-aided loop
detection embeds the topology between semantic instances to search a loop.
However, current semantic-aided loop detection methods face challenges in
dealing with ambiguous semantic instances and drastic viewpoint differences,
which are not fully addressed in the literature. This paper introduces a novel
loop detection method based on an incrementally created scene graph, targeting
the visual SLAM at indoor scenes. It jointly considers the macro-view topology,
micro-view topology, and occupancy of semantic instances to find correct
correspondences. Experiments using handheld RGB-D sequence show our method is
able to accurately detect loops in drastically changed viewpoints. It maintains
a high precision in observing objects with similar topology and appearance. Our
method also demonstrates that it is robust in changed indoor scenes.|
|**2023-05-24**|**Robust Imaging Sonar-based Place Recognition and Localization in Underwater Environments**|Hogyun Kim et.al.|[2305.14773v1](http://arxiv.org/abs/2305.14773v1)|[link](https://github.com/sparolab/sonar_context)|Place recognition using SOund Navigation and Ranging (SONAR) images is an
important task for simultaneous localization and mapping(SLAM) in underwater
environments. This paper proposes a robust and efficient imaging SONAR based
place recognition, SONAR context, and loop closure method. Unlike previous
methods, our approach encodes geometric information based on the
characteristics of raw SONAR measurements without prior knowledge or training.
We also design a hierarchical searching procedure for fast retrieval of
candidate SONAR frames and apply adaptive shifting and padding to achieve
robust matching on rotation and translation changes. In addition, we can derive
the initial pose through adaptive shifting and apply it to the iterative
closest point (ICP) based loop closure factor. We evaluate the performance of
SONAR context in the various underwater sequences such as simulated open water,
real water tank, and real underwater environments. The proposed approach shows
the robustness and improvements of place recognition on various datasets and
evaluation metrics. Supplementary materials are available at
https://github.com/sparolab/sonar_context.git.|
|**2023-05-23**|**Exploiting Radio Fingerprints for Simultaneous Localization and Mapping**|Ran Liu et.al.|[2305.13635v1](http://arxiv.org/abs/2305.13635v1)|null|Simultaneous localization and mapping (SLAM) is paramount for unmanned
systems to achieve self-localization and navigation. It is challenging to
perform SLAM in large environments, due to sensor limitations, complexity of
the environment, and computational resources. We propose a novel approach for
localization and mapping of autonomous vehicles using radio fingerprints, for
example WiFi (Wireless Fidelity) or LTE (Long Term Evolution) radio features,
which are widely available in the existing infrastructure. In particular, we
present two solutions to exploit the radio fingerprints for SLAM. In the first
solution-namely Radio SLAM, the output is a radio fingerprint map generated
using SLAM technique. In the second solution-namely Radio+LiDAR SLAM, we use
radio fingerprint to assist conventional LiDAR-based SLAM to improve accuracy
and speed, while generating the occupancy map. We demonstrate the effectiveness
of our system in three different environments, namely outdoor, indoor building,
and semi-indoor environment.|
|**2023-05-22**|**FEDORA: Flying Event Dataset fOr Reactive behAvior**|Amogh Joshi et.al.|[2305.14392v1](http://arxiv.org/abs/2305.14392v1)|null|The ability of living organisms to perform complex high speed manoeuvers in
flight with a very small number of neurons and an incredibly low failure rate
highlights the efficacy of these resource-constrained biological systems.
Event-driven hardware has emerged, in recent years, as a promising avenue for
implementing complex vision tasks in resource-constrained environments.
Vision-based autonomous navigation and obstacle avoidance consists of several
independent but related tasks such as optical flow estimation, depth
estimation, Simultaneous Localization and Mapping (SLAM), object detection, and
recognition. To ensure coherence between these tasks, it is imperative that
they be trained on a single dataset. However, most existing datasets provide
only a selected subset of the required data. This makes inter-network coherence
difficult to achieve. Another limitation of existing datasets is the limited
temporal resolution they provide. To address these limitations, we present
FEDORA, a first-of-its-kind fully synthetic dataset for vision-based tasks,
with ground truths for depth, pose, ego-motion, and optical flow. FEDORA is the
first dataset to provide optical flow at three different frequencies - 10Hz,
25Hz, and 50Hz|
|**2023-05-22**|**WiROS: WiFi sensing toolbox for robotics**|William Hunter et.al.|[2305.13418v1](http://arxiv.org/abs/2305.13418v1)|[link](https://github.com/ucsdwcsng/wiros)|Many recent works have explored using WiFi-based sensing to improve SLAM,
robot manipulation, or exploration. Moreover, widespread availability makes
WiFi the most advantageous RF signal to leverage. But WiFi sensors lack an
accurate, tractable, and versatile toolbox, which hinders their widespread
adoption with robot's sensor stacks.
  We develop WiROS to address this immediate need, furnishing many WiFi-related
measurements as easy-to-consume ROS topics. Specifically, WiROS is a
plug-and-play WiFi sensing toolbox providing access to coarse-grained WiFi
signal strength (RSSI), fine-grained WiFi channel state information (CSI), and
other MAC-layer information (device address, packet id's or frequency-channel
information). Additionally, WiROS open-sources state-of-art algorithms to
calibrate and process WiFi measurements to furnish accurate bearing information
for received WiFi signals. The open-sourced repository is:
https://github.com/ucsdwcsng/WiROS|
|**2023-05-22**|**PALoc: Robust Prior-assisted Trajectory Generation for Benchmarking**|Xiangcheng Hu et.al.|[2305.13147v1](http://arxiv.org/abs/2305.13147v1)|null|Evaluating simultaneous localization and mapping (SLAM) algorithms
necessitates high-precision and dense ground truth (GT) trajectories. But
obtaining desirable GT trajectories is sometimes challenging without GT
tracking sensors. As an alternative, in this paper, we propose a novel
prior-assisted SLAM system to generate a full six-degree-of-freedom ($6$-DOF)
trajectory at around $10$Hz for benchmarking under the framework of the factor
graph. Our degeneracy-aware map factor utilizes a prior point cloud map and
LiDAR frame for point-to-plane optimization, simultaneously detecting
degeneration cases to reduce drift and enhancing the consistency of pose
estimation. Our system is seamlessly integrated with cutting-edge odometry via
a loosely coupled scheme to generate high-rate and precise trajectories.
Moreover, we propose a norm-constrained gravity factor for stationary cases,
optimizing pose and gravity to boost performance. Extensive evaluations
demonstrate our algorithm's superiority over existing SLAM or map-based methods
in diverse scenarios in terms of precision, smoothness, and robustness. Our
approach substantially advances reliable and accurate SLAM evaluation methods,
fostering progress in robotics research.|
|**2023-05-22**|**Angle-based SLAM on 5G mmWave Systems: Design, Implementation, and Measurement**|Jie Yang et.al.|[2305.12669v1](http://arxiv.org/abs/2305.12669v1)|null|Simultaneous localization and mapping (SLAM) is a key technology that
provides user equipment (UE) tracking and environment mapping services,
enabling the deep integration of sensing and communication. The millimeter-wave
(mmWave) communication, with its larger bandwidths and antenna arrays,
inherently facilitates more accurate delay and angle measurements than sub-6
GHz communication, thereby providing opportunities for SLAM. However, none of
the existing works have realized the SLAM function under the 5G New Radio (NR)
standard due to specification and hardware constraints. In this study, we
investigate how 5G mmWave communication systems can achieve situational
awareness without changing the transceiver architecture and 5G NR standard. We
implement 28 GHz mmWave transceivers that deploy OFDM-based 5G NR waveform with
160 MHz channel bandwidth, and we realize beam management following the 5G NR.
Furthermore, we develop an efficient successive cancellation-based angle
extraction approach to obtain angles of arrival and departure from the
reference signal received power measurements. On the basis of angle
measurements, we propose an angle-only SLAM algorithm to track UE and map
features in the radio environment. Thorough experiments and ray tracing-based
computer simulations verify that the proposed angle-based SLAM can achieve
sub-meter level localization and mapping accuracy with a single base station
and without the requirement of strict time synchronization. Our experiments
also reveal many propagation properties critical to the success of SLAM in 5G
mmWave communication systems.|
|**2023-05-17**|**TextSLAM: Visual SLAM with Semantic Planar Text Features**|Boying Li et.al.|[2305.10029v1](http://arxiv.org/abs/2305.10029v1)|[link](https://github.com/sjtu-visys/textslam)|We propose a novel visual SLAM method that integrates text objects tightly by
treating them as semantic features via fully exploring their geometric and
semantic prior. The text object is modeled as a texture-rich planar patch whose
semantic meaning is extracted and updated on the fly for better data
association. With the full exploration of locally planar characteristics and
semantic meaning of text objects, the SLAM system becomes more accurate and
robust even under challenging conditions such as image blurring, large
viewpoint changes, and significant illumination variations (day and night). We
tested our method in various scenes with the ground truth data. The results
show that integrating texture features leads to a more superior SLAM system
that can match images across day and night. The reconstructed semantic 3D text
map could be useful for navigation and scene understanding in robotic and mixed
reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .|
|**2023-05-16**|**Graph-based Global Robot Simultaneous Localization and Mapping using Architectural Plans**|Muhammad Shaheer et.al.|[2305.09295v1](http://arxiv.org/abs/2305.09295v1)|null|In this paper, we propose a solution for graph-based global robot
simultaneous localization and mapping (SLAM) using architectural plans. Before
the start of the robot operation, the previously available architectural plan
of the building is converted into our proposed architectural graph (A-Graph).
When the robot starts its operation, it uses its onboard LIDAR and odometry to
carry out an online SLAM relying on our situational graph (S-Graph), which
includes both, a representation of the environment with multiple levels of
abstractions, such as walls or rooms, and their relationships, as well as the
robot poses with their associated keyframes. Our novel graph-to-graph matching
method is used to relate the aforementioned S-Graph and A-Graph, which are
aligned and merged, resulting in our novel informed Situational Graph
(iS-Graph). Our iS-Graph not only provides graph-based global robot
localization, but it extends the graph-based SLAM capabilities of the S-Graph
by incorporating into it the prior knowledge of the environment existing in the
architectural plan|
|**2023-05-12**|**An Object SLAM Framework for Association, Mapping, and High-Level Tasks**|Yanmin Wu et.al.|[2305.07299v1](http://arxiv.org/abs/2305.07299v1)|null|Object SLAM is considered increasingly significant for robot high-level
perception and decision-making. Existing studies fall short in terms of data
association, object representation, and semantic mapping and frequently rely on
additional assumptions, limiting their performance. In this paper, we present a
comprehensive object SLAM framework that focuses on object-based perception and
object-oriented robot tasks. First, we propose an ensemble data association
approach for associating objects in complicated conditions by incorporating
parametric and nonparametric statistic testing. In addition, we suggest an
outlier-robust centroid and scale estimation algorithm for modeling objects
based on the iForest and line alignment. Then a lightweight and object-oriented
map is represented by estimated general object models. Taking into
consideration the semantic invariance of objects, we convert the object map to
a topological map to provide semantic descriptors to enable multi-map matching.
Finally, we suggest an object-driven active exploration strategy to achieve
autonomous mapping in the grasping scenario. A range of public datasets and
real-world results in mapping, augmented reality, scene matching,
relocalization, and robotic manipulation have been used to evaluate the
proposed object SLAM framework for its efficient performance.|
|**2023-05-11**|**Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems**|Nathan Hughes et.al.|[2305.07154v1](http://arxiv.org/abs/2305.07154v1)|[link](https://github.com/mit-spark/hydra)|3D spatial perception is the problem of building and maintaining an
actionable and persistent representation of the environment in real-time using
sensor data and prior knowledge. Despite the fast-paced progress in robot
perception, most existing methods either build purely geometric maps (as in
traditional SLAM) or flat metric-semantic maps that do not scale to large
environments or large dictionaries of semantic labels. The first part of this
paper is concerned with representations: we show that scalable representations
for spatial perception need to be hierarchical in nature. Hierarchical
representations are efficient to store, and lead to layered graphs with small
treewidth, which enable provably efficient inference. We then introduce an
example of hierarchical representation for indoor environments, namely a 3D
scene graph, and discuss its structure and properties. The second part of the
paper focuses on algorithms to incrementally construct a 3D scene graph as the
robot explores the environment. Our algorithms combine 3D geometry, topology
(to cluster the places into rooms), and geometric deep learning (e.g., to
classify the type of rooms the robot is moving across). The third part of the
paper focuses on algorithms to maintain and correct 3D scene graphs during
long-term operation. We propose hierarchical descriptors for loop closure
detection and describe how to correct a scene graph in response to loop
closures, by solving a 3D scene graph optimization problem. We conclude the
paper by combining the proposed perception algorithms into Hydra, a real-time
spatial perception system that builds a 3D scene graph from visual-inertial
data in real-time. We showcase Hydra's performance in photo-realistic
simulations and real data collected by a Clearpath Jackal robots and a Unitree
A1 robot. We release an open-source implementation of Hydra at
https://github.com/MIT-SPARK/Hydra.|
|**2023-05-09**|**Understanding why SLAM algorithms fail in modern indoor environments**|Nwankwo Linus et.al.|[2305.05313v1](http://arxiv.org/abs/2305.05313v1)|null|Simultaneous localization and mapping (SLAM) algorithms are essential for the
autonomous navigation of mobile robots. With the increasing demand for
autonomous systems, it is crucial to evaluate and compare the performance of
these algorithms in real-world environments. In this paper, we provide an
evaluation strategy and real-world datasets to test and evaluate SLAM
algorithms in complex and challenging indoor environments. Further, we analysed
state-of-the-art (SOTA) SLAM algorithms based on various metrics such as
absolute trajectory error, scale drift, and map accuracy and consistency. Our
results demonstrate that SOTA SLAM algorithms often fail in challenging
environments, with dynamic objects, transparent and reflecting surfaces. We
also found that successful loop closures had a significant impact on the
algorithm's performance. These findings highlight the need for further research
to improve the robustness of the algorithms in real-world scenarios.|
|**2023-05-07**|**Simulation of Dynamic Environments for SLAM**|Elia Bonetto et.al.|[2305.04286v2](http://arxiv.org/abs/2305.04286v2)|null|Simulation engines are widely adopted in robotics. However, they lack either
full simulation control, ROS integration, realistic physics, or photorealism.
Recently, synthetic data generation and realistic rendering has advanced tasks
like target tracking and human pose estimation. However, when focusing on
vision applications, there is usually a lack of information like sensor
measurements or time continuity. On the other hand, simulations for most
robotics tasks are performed in (semi)static environments, with specific
sensors and low visual fidelity. To solve this, we introduced in our previous
work a fully customizable framework for generating realistic animated dynamic
environments (GRADE) [1]. We use GRADE to generate an indoor dynamic
environment dataset and then compare multiple SLAM algorithms on different
sequences. By doing that, we show how current research over-relies on known
benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN
models provide further evidence that additional research in dynamic SLAM is
necessary. The code, results, and generated data are provided as open-source at
https://eliabntt.github.io/grade-rrSimulation of Dynamic Environments for SLAM|
|**2023-05-06**|**Robust optimization of control parameters for WEC arrays using stochastic methods**|Marco Gambarini et.al.|[2305.04130v1](http://arxiv.org/abs/2305.04130v1)|null|This work presents a new computational optimization framework for the robust
control of parks of Wave Energy Converters (WEC) in irregular waves. The power
of WEC parks is maximized with respect to the individual control damping and
stiffness coefficients of each device. The results are robust with respect to
the incident wave direction, which is treated as a random variable.
Hydrodynamic properties are computed using the linear potential model, and the
dynamics of the system is computed in the frequency domain. A slamming
constraint is enforced to ensure that the results are physically realistic. We
show that the stochastic optimization problem is well posed. Two optimization
approaches for dealing with stochasticity are then considered: stochastic
approximation and sample average approximation. The outcomes of the above
mentioned methods in terms of accuracy and computational time are presented.
The results of the optimization for complex and realistic array configurations
of possible engineering interest are then discussed. Results of extensive
numerical experiments demonstrate the efficiency of the proposed computational
framework.|
|**2023-05-05**|**Multi S-graphs: A Collaborative Semantic SLAM architecture**|Miguel Fernandez-Cortizas et.al.|[2305.03441v1](http://arxiv.org/abs/2305.03441v1)|null|Collaborative Simultaneous Localization and Mapping (CSLAM) is a critical
capability for enabling multiple robots to operate in complex environments.
Most CSLAM techniques rely on the transmission of low-level features for visual
and LiDAR-based approaches, which are used for pose graph optimization.
However, these low-level features can lead to incorrect loop closures,
negatively impacting map generation.Recent approaches have proposed the use of
high-level semantic information in the form of Hierarchical Semantic Graphs to
improve the loop closure procedures and overall precision of SLAM algorithms.
In this work, we present Multi S-Graphs, an S-graphs [1] based distributed
CSLAM algorithm that utilizes high-level semantic information for cooperative
map generation while minimizing the amount of information exchanged between
robots. Experimental results demonstrate the promising performance of the
proposed algorithm in map generation tasks.|
|**2023-05-05**|**Set-Type Belief Propagation with Applications to Mapping, MTT, SLAM, and SLAT**|Hyowon Kim et.al.|[2305.04797v1](http://arxiv.org/abs/2305.04797v1)|null|Belief propagation (BP) is a useful probabilistic inference algorithm for
efficiently computing approximate marginal probability densities of random
variables. However, in its standard form, BP is applicable to only the
vector-type random variables, while certain applications rely on set-type
random variables with an unknown number of vector elements. In this paper, we
first develop BP rules for set-type random variables and demonstrate that
vector-type BP is a special case of set-type BP. We further propose factor
graphs with set-factor and set-variable nodes by devising the set-factor nodes
that can address the set-variables with random elements and cardinality, while
the number of vector elements in vector-type is known. To demonstrate the
validity of developed set-type BP, we apply it to the Poisson multi-Bernoulli
(PMB) filter for simultaneous localization and mapping (SLAM), which naturally
leads to a new set-type BP-SLAM filter. Finally, we reveal connections between
the vector-type BP-SLAM filter and the proposed set-type BP-SLAM filter and
show a performance gain of the proposed set-type BP-SLAM filter in comparison
with the vector-type BP-SLAM filter.|
|**2023-05-04**|**Edge-aware Consistent Stereo Video Depth Estimation**|Elena Kosheleva et.al.|[2305.02645v1](http://arxiv.org/abs/2305.02645v1)|null|Video depth estimation is crucial in various applications, such as scene
reconstruction and augmented reality. In contrast to the naive method of
estimating depths from images, a more sophisticated approach uses temporal
information, thereby eliminating flickering and geometrical inconsistencies. We
propose a consistent method for dense video depth estimation; however, unlike
the existing monocular methods, ours relates to stereo videos. This technique
overcomes the limitations arising from the monocular input. As a benefit of
using stereo inputs, a left-right consistency loss is introduced to improve the
performance. Besides, we use SLAM-based camera pose estimation in the process.
To address the problem of depth blurriness during test-time training (TTT), we
present an edge-preserving loss function that improves the visibility of fine
details while preserving geometrical consistency. We show that our edge-aware
stereo video model can accurately estimate the dense depth maps.|
|**2023-05-03**|**Direct LiDAR-Inertial Odometry and Mapping: Perceptive and Connective SLAM**|Kenny Chen et.al.|[2305.01843v1](http://arxiv.org/abs/2305.01843v1)|null|This paper presents Direct LiDAR-Inertial Odometry and Mapping (DLIOM), a
robust SLAM algorithm with an explicit focus on computational efficiency,
operational reliability, and real-world efficacy. DLIOM contains several key
algorithmic innovations in both the front-end and back-end subsystems to design
a resilient LiDAR-inertial architecture that is perceptive to the environment
and produces accurate localization and high-fidelity 3D mapping for autonomous
robotic platforms. Our ideas spawned after a deep investigation into modern
LiDAR SLAM systems and their inabilities to generalize across different
operating environments, in which we address several common algorithmic failure
points by means of proactive safe-guards to provide long-term operational
reliability in the unstructured real world. We detail several important
innovations to localization accuracy and mapping resiliency distributed
throughout a typical LiDAR SLAM pipeline to comprehensively increase
algorithmic speed, accuracy, and robustness. In addition, we discuss insights
gained from our ground-up approach while implementing such a complex system for
real-time state estimation on resource-constrained systems, and we
experimentally show the increased performance of our method as compared to the
current state-of-the-art on both public benchmark and self-collected datasets.|
|**2023-05-02**|**EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors**|Xinyu Yi et.al.|[2305.01599v1](http://arxiv.org/abs/2305.01599v1)|null|Human and environment sensing are two important topics in Computer Vision and
Graphics. Human motion is often captured by inertial sensors, while the
environment is mostly reconstructed using cameras. We integrate the two
techniques together in EgoLocate, a system that simultaneously performs human
motion capture (mocap), localization, and mapping in real time from sparse
body-mounted sensors, including 6 inertial measurement units (IMUs) and a
monocular phone camera. On one hand, inertial mocap suffers from large
translation drift due to the lack of the global positioning signal. EgoLocate
leverages image-based simultaneous localization and mapping (SLAM) techniques
to locate the human in the reconstructed scene. On the other hand, SLAM often
fails when the visual feature is poor. EgoLocate involves inertial mocap to
provide a strong prior for the camera motion. Experiments show that
localization, a key challenge for both two fields, is largely improved by our
technique, compared with the state of the art of the two fields. Our codes are
available for research at https://xinyu-yi.github.io/EgoLocate/.|
|**2023-04-30**|**LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking**|Zhongyang Zhu et.al.|[2305.00406v1](http://arxiv.org/abs/2305.00406v1)|null|Simultaneous localization and mapping (SLAM) is critical to the
implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms
assume a static environment, leading to unreliable localization in dynamic
environments. Furthermore, accurate tracking of moving objects is of great
significance for the control and planning of autonomous vehicle operation. This
study proposes LIMOT, a tightly-coupled multi-object tracking and
LiDAR-inertial SLAM system capable of accurately estimating the poses of both
ego-vehicle and objects. First, we use 3D bounding boxes generated by an object
detector to represent all movable objects and perform LiDAR odometry using
inertial measurement unit (IMU) pre-integration result. Based on the historical
trajectories of tracked objects in a sliding window, we perform robust object
association. We propose a trajectory-based dynamic feature filtering method,
which filters out features belonging to moving objects by leveraging tracking
results. Factor graph-based optimization is then conducted to optimize the bias
of the IMU and the poses of both the ego-vehicle and surrounding objects in a
sliding window. Experiments conducted on KITTI datasets show that our method
achieves better pose and tracking accuracy than our previous work DL-SLOT and
other SLAM and multi-object tracking baseline methods.|

### SFM
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Background Prompting for Improved Object Depth**|Manel Baradad et.al.|[2306.05428v1](http://arxiv.org/abs/2306.05428v1)|null|Estimating the depth of objects from a single image is a valuable task for
many vision, robotics, and graphics applications. However, current methods
often fail to produce accurate depth for objects in diverse scenes. In this
work, we propose a simple yet effective Background Prompting strategy that
adapts the input object image with a learned background. We learn the
background prompts only using small-scale synthetic object datasets. To infer
object depth on a real image, we place the segmented object into the learned
background prompt and run off-the-shelf depth networks. Background Prompting
helps the depth networks focus on the foreground object, as they are made
invariant to background variations. Moreover, Background Prompting minimizes
the domain gap between synthetic and real object images, leading to better
sim2real generalization than simple finetuning. Results on multiple synthetic
and real datasets demonstrate consistent improvements in real object depths for
a variety of existing depth networks. Code and optimized background prompts can
be found at: https://mbaradad.github.io/depth_prompt.|
|**2023-06-08**|**SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking**|Chris Cundy et.al.|[2306.05426v1](http://arxiv.org/abs/2306.05426v1)|null|In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.|
|**2023-06-08**|**MIMIC-IT: Multi-Modal In-Context Instruction Tuning**|Bo Li et.al.|[2306.05425v1](http://arxiv.org/abs/2306.05425v1)|[link](https://github.com/luodian/otter)|High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.|
|**2023-06-08**|**ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process**|Changyao Tian et.al.|[2306.05423v1](http://arxiv.org/abs/2306.05423v1)|null|Image recognition and generation have long been developed independently of
each other. With the recent trend towards general-purpose representation
learning, the development of general representations for both recognition and
generation tasks is also promoted. However, preliminary attempts mainly focus
on generation performance, but are still inferior on recognition tasks. These
methods are modeled in the vector-quantized (VQ) space, whereas leading
recognition methods use pixels as inputs. Our key insights are twofold: (1)
pixels as inputs are crucial for recognition tasks; (2) VQ tokens as
reconstruction targets are beneficial for generation tasks. These observations
motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that
integrates these two spaces within a single representation learning framework.
In each denoising step, our method first decodes pixels from previous VQ
tokens, then generates new VQ tokens from the decoded pixels. The diffusion
process gradually masks out a portion of VQ tokens to construct the training
samples. The learned representations can be used to generate diverse
high-fidelity images and also demonstrate excellent transfer performance on
recognition tasks. Extensive experiments show that our method achieves
competitive performance on unconditional generation, ImageNet classification,
COCO detection, and ADE20k segmentation. Importantly, our method represents the
first successful development of general representations applicable to both
generation and dense recognition tasks. Code shall be released.|
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**Stochastic Multi-Person 3D Motion Forecasting**|Sirui Xu et.al.|[2306.05421v1](http://arxiv.org/abs/2306.05421v1)|null|This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.|
|**2023-06-08**|**2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction**|Jiawei He et.al.|[2306.05418v1](http://arxiv.org/abs/2306.05418v1)|null|With the advent of the big model era, the demand for data has become more
important. Especially in monocular 3D object detection, expensive manual
annotations potentially limit further developments. Existing works have
investigated weakly supervised algorithms with the help of LiDAR modality to
generate 3D pseudo labels, which cannot be applied to ordinary videos. In this
paper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea
of global-to-local 3D reconstruction for 2D supervised monocular 3D object
detection. Specifically, we recover 3D structures from monocular videos by
scene-level global reconstruction with global bundle adjustment (BA) and obtain
object clusters by the DoubleClustering algorithm. Learning from completely
reconstructed objects in global BA, GBA-Learner predicts pseudo labels for
occluded objects. Finally, we train an LBA-Learner with object-centric local BA
to generalize the generated 3D pseudo labels to moving objects. Experiments on
the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is
on par with the fully-supervised BA-Det trained with 10% videos and even
outperforms some pioneer fully-supervised methods. We also show the great
potential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.
The code will be made available. Project page: https://ba2det.site .|
|**2023-06-08**|**The sum of all width-one tensors**|William Q. Erickson et.al.|[2306.05417v1](http://arxiv.org/abs/2306.05417v1)|null|This paper generalizes a recent result by the authors concerning the sum of
width-one matrices; in the present work, we consider width-one tensors of
arbitrary dimensions. A tensor is said to have width 1 if, when visualized as
an array, its nonzero entries lie along a path consisting of steps in the
directions of the standard coordinate vectors. We prove two different formulas
to compute the sum of all width-one tensors with fixed dimensions and fixed sum
of (nonnegative integer) components. The first formula is obtained by
converting width-one tensors into tuples of one-row semistandard Young
tableaux; the second formula, which extracts coefficients from products of
multiset Eulerian polynomials, is derived via Stanley-Reisner theory, making
use of the EL-shelling of the order complex on the standard basis of tensors.|
|**2023-06-08**|**Tracking Objects with 3D Representation from Videos**|Jiawei He et.al.|[2306.05416v1](http://arxiv.org/abs/2306.05416v1)|null|Data association is a knotty problem for 2D Multiple Object Tracking due to
the object occlusion. However, in 3D space, data association is not so hard.
Only with a 3D Kalman Filter, the online object tracker can associate the
detections from LiDAR. In this paper, we rethink the data association in 2D MOT
and utilize the 3D object representation to separate each object in the feature
space. Unlike the existing depth-based MOT methods, the 3D object
representation can be jointly learned with the object association module.
Besides, the object's 3D representation is learned from the video and
supervised by the 2D tracking labels without additional manual annotations from
LiDAR or pretrained depth estimator. With 3D object representation learning
from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT
paradigm, called P3DTrack. Extensive experiments show the effectiveness of our
method. We achieve new state-of-the-art performance on the large-scale Waymo
Open Dataset.|
|**2023-06-08**|**Causal normalizing flows: from theory to practice**|Adrin Javaloy et.al.|[2306.05415v1](http://arxiv.org/abs/2306.05415v1)|null|In this work, we deepen on the use of normalizing flows for causal reasoning.
Specifically, we first leverage recent results on non-linear ICA to show that
causal models are identifiable from observational data given a causal ordering,
and thus can be recovered using autoregressive normalizing flows (NFs). Second,
we analyze different design and learning choices for causal normalizing flows
to capture the underlying causal data-generating process. Third, we describe
how to implement the do-operator in causal NFs, and thus, how to answer
interventional and counterfactual questions. Finally, in our experiments, we
validate our design and training choices through a comprehensive ablation
study; compare causal NFs to other approaches for approximating causal models;
and empirically demonstrate that causal NFs can be used to address real-world
problems, where the presence of mixed discrete-continuous data and partial
knowledge on the causal graph is the norm. The code for this work can be found
at https://github.com/psanch21/causal-flows.|
|**2023-06-08**|**R-MAE: Regions Meet Masked Autoencoders**|Duy-Kien Nguyen et.al.|[2306.05411v1](http://arxiv.org/abs/2306.05411v1)|null|Vision-specific concepts such as "region" have played a key role in extending
general machine learning frameworks to tasks like object detection. Given the
success of region-based detectors for supervised learning and the progress of
intra-image methods for contrastive learning, we explore the use of regions for
reconstructive pre-training. Starting from Masked Autoencoding (MAE) both as a
baseline and an inspiration, we propose a parallel pre-text task tailored to
address the one-to-many mapping between images and regions. Since such regions
can be generated in an unsupervised way, our approach (R-MAE) inherits the wide
applicability from MAE, while being more "region-aware". We conduct thorough
analyses during the development of R-MAE, and converge on a variant that is
both effective and efficient (1.3% overhead over MAE). Moreover, it shows
consistent quantitative improvements when generalized to various pre-training
data and downstream detection and segmentation benchmarks. Finally, we provide
extensive qualitative visualizations to enhance the understanding of R-MAE's
behaviour and potential. Code will be made available at
https://github.com/facebookresearch/r-mae.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**Quantum symmetries in 2+1 dimensions: Carroll, (a)dS-Carroll, Galilei and (a)dS-Galilei**|Tomasz Trzeniewski et.al.|[2306.05409v1](http://arxiv.org/abs/2306.05409v1)|null|There is a surge of research devoted to the formalism and physical
manifestations of non-Lorentzian kinematical symmetries, which focuses
especially on the ones associated with the Galilei and Carroll relativistic
limits (the speed of light taken to infinity or to zero, respectively). The
investigations has also been extended to quantum deformations of the Carrollian
and Galilean symmetries, in the sense of (quantum) Hopf algebras. The case of
2+1 dimensions is particularly worth to study due to both the mathematical
nature of the corresponding (classical) theory of gravity, and the recently
finalized classification of all quantum-deformed algebras of spacetime
isometries. Consequently, the list of all quantum deformations of (anti-)de
Sitter-Carroll algebra is immediately provided by its well-known isomorphism
with either Poincar\'{e} or Euclidean algebra. Quantum contractions from the
(anti-)de Sitter to (anti-)de Sitter-Carroll classification allow to almost
completely recover the latter. One may therefore conjecture that the analogous
contractions from the (anti-)de Sitter to (anti-)de Sitter-Galilei $r$-matrices
provide (almost) all coboundary deformations of (anti-)de Sitter-Galilei
algebra. This scheme is complemented by deriving (Carrollian and Galilean)
quantum contractions of deformations of Poincar\'{e} algebra, leading to
coboundary deformations of Carroll and Galilei algebras.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories**|Shizhe Diao et.al.|[2306.05406v1](http://arxiv.org/abs/2306.05406v1)|[link](https://github.com/amano-aki/mixture-of-domain-adapters)|Pre-trained language models (PLMs) demonstrate excellent abilities to
understand texts in the generic domain while struggling in a specific domain.
Although continued pre-training on a large domain-specific corpus is effective,
it is costly to tune all the parameters on the domain. In this paper, we
investigate whether we can adapt PLMs both effectively and efficiently by only
tuning a few parameters. Specifically, we decouple the feed-forward networks
(FFNs) of the Transformer architecture into two parts: the original pre-trained
FFNs to maintain the old-domain knowledge and our novel domain-specific
adapters to inject domain-specific knowledge in parallel. Then we adopt a
mixture-of-adapters gate to fuse the knowledge from different domain adapters
dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a
two-stage adapter-tuning strategy that leverages both unlabeled data and
labeled data to help the domain adaptation: i) domain-specific adapter on
unlabeled data; followed by ii) the task-specific adapter on labeled data.
MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and
our experiments demonstrate that MixDA achieves superior performance on
in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and
knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,
scalability, and efficiency of our method. The code is available at
https://github.com/Amano-Aki/Mixture-of-Domain-Adapters.|
|**2023-06-08**|**The FRB20190520B Sightline Intersects Foreground Galaxy Clusters**|Khee-Gan Lee et.al.|[2306.05403v1](http://arxiv.org/abs/2306.05403v1)|null|The repeating fast radio burst FRB20190520B is an anomaly of the FRB
population thanks to its high dispersion measure (DM$=1205\,pc\,cm^{-3}$)
despite its low redshift of $z_\mathrm{frb}=0.241$. This excess has been
attributed to a host contribution of ${DM_{host}} \approx
900\,\mathrm{pc\,cm^{-3}}$, far larger than any other known FRB. In this paper,
we describe spectroscopic observations of the FRB20190520B field obtained as
part of the FLIMFLAM survey on the 2dF/AAOmega facility, which yielded 701
galaxies redshifts in a field of $\approx 3\,\mathrm{deg}^2$. Applying a
friends-of-friends group finder reveals multiple galaxy groups and clusters,
for which we then estimated halo masses by comparing their richness with
forward-modeled mocks from numerical simulations. We discover two separate
$M_\mathrm{halo} >10^{14}\,M_\odot$ galaxy clusters, at $z=0.1867$ and
$z=0.2170$, respectively, that are directly intersected by the FRB sightline
within their characteristic radius $r_{200}$. Subtracting off their estimated
DM contributions as well that of the diffuse intergalactic medium, we estimate
a host contribution of $DM_{host}=467^{+140}_{-230}\,\mathrm{pc\,cm^{-3}}$ or
${DM_{host}} = 339^{+122}_{-174}\,\mathrm{pc\,cm^{-3}}$ (observed frame)
depending on whether we assume the halo gas extends to $r_{200}$ or $2\times
r_{200}$. This significantly smaller $DM_{host}$ -- no longer the largest known
value -- is now consistent with H$\alpha$ emission measure estimates of the
host galaxy without having to invoke unusually high gas temperatures. We also
re-estimate the turbulent fluctuation and geometric amplification factor of the
scattering layer to be $FG \approx 3.9 - 7.5\,(\mathrm{pc^2\;km})^{-1/3}$. This
result illustrates the importance of incorporating foreground data for FRB
analyses, both for understanding the nature of FRBs and to realize their
potential as a cosmological probe.|
|**2023-06-08**|**Fully Robust Federated Submodel Learning in a Distributed Storage System**|Zhusheng Wang et.al.|[2306.05402v1](http://arxiv.org/abs/2306.05402v1)|null|We consider the federated submodel learning (FSL) problem in a distributed
storage system. In the FSL framework, the full learning model at the server
side is divided into multiple submodels such that each selected client needs to
download only the required submodel(s) and upload the corresponding update(s)
in accordance with its local training data. The server comprises multiple
independent databases and the full model is stored across these databases. An
eavesdropper passively observes all the storage and listens to all the
communicated data, of its controlled databases, to gain knowledge about the
remote client data and the submodel information. In addition, a subset of
databases may fail, negatively affecting the FSL process, as FSL process may
take a non-negligible amount of time for large models. To resolve these two
issues together (i.e., security and database repair), we propose a novel coding
mechanism coined ramp secure regenerating coding (RSRC), to store the full
model in a distributed manner. Using our new RSRC method, the eavesdropper is
permitted to learn a controllable amount of submodel information for the sake
of reducing the communication and storage costs. Further, during the database
repair process, in the construction of the replacement database, the submodels
to be updated are stored in the form of their latest version from updating
clients, while the remaining submodels are obtained from the previous version
in other databases through routing clients. Our new RSRC-based distributed FSL
approach is constructed on top of our earlier two-database FSL scheme which
uses private set union (PSU). A complete one-round FSL process consists of
FSL-PSU phase, FSL-write phase and additional auxiliary phases. Our proposed
FSL scheme is also robust against database drop-outs, client drop-outs, client
late-arrivals and an active adversary controlling databases.|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|[link](https://github.com/shi-labs/matting-anything)|In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.|
|**2023-06-08**|**Bayesian model calibration for diblock copolymer thin film self-assembly using power spectrum of microscopy data**|Lianghao Cao et.al.|[2306.05398v1](http://arxiv.org/abs/2306.05398v1)|null|Identifying parameters of computational models from experimental data, or
model calibration, is fundamental for assessing and improving the
predictability and reliability of computer simulations. In this work, we
propose a method for Bayesian calibration of models that predict morphological
patterns of diblock copolymer (Di-BCP) thin film self-assembly while accounting
for various sources of uncertainties in pattern formation and data acquisition.
This method extracts the azimuthally-averaged power spectrum (AAPS) of the
top-down microscopy characterization of Di-BCP thin film patterns as summary
statistics for Bayesian inference of model parameters via the pseudo-marginal
method. We derive the analytical and approximate form of a conditional
likelihood for the AAPS of image data. We demonstrate that AAPS-based image
data reduction retains the mutual information, particularly on important length
scales, between image data and model parameters while being relatively agnostic
to the aleatoric uncertainties associated with the random long-range disorder
of Di-BCP patterns. Additionally, we propose a phase-informed prior
distribution for Bayesian model calibration. Furthermore, reducing image data
to AAPS enables us to efficiently build surrogate models to accelerate the
proposed Bayesian model calibration procedure. We present the formulation and
training of two multi-layer perceptrons for approximating the
parameter-to-spectrum map, which enables fast integrated likelihood
evaluations. We validate the proposed Bayesian model calibration method through
numerical examples, for which the neural network surrogate delivers a fivefold
reduction of the number of model simulations performed for a single calibration
task.|
|**2023-06-08**|**Asymmetric periodic boundary conditions for molecular dynamics and coarse-grained simulations of nucleic acids**|Radek Erban et.al.|[2306.05396v1](http://arxiv.org/abs/2306.05396v1)|null|Periodic boundary conditions are commonly applied in molecular dynamics
simulations in the microcanonical (NVE), canonical (NVT) and
isothermal-isobaric (NpT) ensembles. In their simplest application, a
biological system of interest is placed in the middle of a solvation box, which
is chosen 'sufficiently large' to minimize any numerical artefacts associated
with the periodic boundary conditions. This practical approach brings
limitations to the size of biological systems that can be simulated. Here, we
study simulations of effectively infinitely-long nucleic acids, which are
solvated in the directions perpendicular to the polymer chain, while periodic
boundary conditions are also applied along the polymer chain. We study the
effects of these asymmetric periodic boundary conditions (APBC) on the
simulated results, including the mechanical properties of biopolymers and the
properties of the surrounding solvent. To get some further insights into the
advantages of using the APBC, a coarse-grained worm-like chain model is first
studied, illustrating how the persistence length can be extracted from local
properties of the polymer chain, which are less affected by the APBC than some
global averages. This is followed by all-atom molecular dynamics simulations of
DNA in ionic solutions, where we use the APBC to investigate sequence-dependent
properties of DNA molecules and properties of the surrounding solvent.|
|**2023-06-08**|**Series Solution for Interaction of Scalar Plane Wave with Spatially Decaying Gravitational Wave**|Jesse Elder et.al.|[2306.05394v1](http://arxiv.org/abs/2306.05394v1)|null|In this paper we present the power series solution of the Klein-Gordon
equation in the spacetime background of a gravitational wave with amplitude
that decays with distance from the source. The resulting solution describes the
interaction of a scalar plane wave travelling in an arbitrary direction
relative to the direction of propagation of the gravitational wave. This
solution has the unexpected property that as the scalar wave approaches
collinearity with the gravitational wave there is a rapid transition in the
form of the solution. The solution in the collinear limit exhibits a resonance
phenomenon which distinguishes these results from previous analyses involving
plane gravitational wave backgrounds. We discuss in detail the similarities and
differences between the solutions for plane gravitational waves and
gravitational waves with amplitude that decreases with distance from the
source. We give an argument that this solution of the Klein-Gordon equation
only describes the interaction of a gravitational wave with a scalar wave and
that the gravitational wave will not produce a scalar waveform in a vacuum. The
interaction between the gravitational and scalar waves lead to both sinusoidal
time-dependent fluctuations in, and time-independent enhancement of, the scalar
current in the direction of the gravitational wave. Finally, we discuss the
possibility of observable effects of this interaction.|
|**2023-06-08**|**HQ-50K: A Large-scale, High-quality Dataset for Image Restoration**|Qinhong Yang et.al.|[2306.05390v1](http://arxiv.org/abs/2306.05390v1)|[link](https://github.com/littleyaang/hq-50k)|This paper introduces a new large-scale image restoration dataset, called
HQ-50K, which contains 50,000 high-quality images with rich texture details and
semantic diversity. We analyze existing image restoration datasets from five
different perspectives, including data scale, resolution, compression rates,
texture details, and semantic coverage. However, we find that all of these
datasets are deficient in some aspects. In contrast, HQ-50K considers all of
these five aspects during the data curation process and meets all requirements.
We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, which
enables a single model to handle multiple corruption types and unknown levels.
Our extensive experiments demonstrate that HQ-50K consistently improves the
performance on various image restoration tasks, such as super-resolution,
denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained on
our \dataset, outperforms existing state-of-the-art unified models designed for
multiple restoration tasks and levels. The dataset and code are available at
\url{https://github.com/littleYaang/HQ-50K}.|
|**2023-06-08**|**Towards distinguishing Dirac from Majorana neutrino mass with gravitational waves**|Stephen F. King et.al.|[2306.05389v1](http://arxiv.org/abs/2306.05389v1)|null|We propose to distinguish the nature of neutrino masses, Dirac vs Majorana,
from the spectrum of gravitational waves generated. We study two simple models
of Majorana and Dirac mass genesis motivated by generating small neutrino
masses without assuming tiny Yukawa couplings. For Majorana neutrinos,
spontaneous breaking of the gauged $B-L$ symmetry gives a cosmic string induced
gravitational wave signal flat over a large range of frequencies, whereas for
Dirac neutrinos, spontaneous and soft-breaking of a $Z_2$ symmetry generate a
peaked gravitational wave spectrum from annihilation of domain walls. The
striking difference between the shape of the spectra in the two cases can help
differentiate between Dirac vs Majorana neutrino masses in the two class of
models considered, complementing results of neutrinoless double beta decay
experiments.|
|**2023-06-08**|**Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age**|Daniela Teodorescu et.al.|[2306.05387v1](http://arxiv.org/abs/2306.05387v1)|null|Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.|
|**2023-06-08**|**Particle-in-cell simulation of a 50~mTorr capacitively coupled argon discharge over a range of frequencies**|Saurabh Simha et.al.|[2306.05386v1](http://arxiv.org/abs/2306.05386v1)|null|The effect of driving frequency in the range of 13.56 MHz to 73 MHz on
electron energy distribution and electron heating modes in a 50 mTorr
capacitively coupled argon plasma discharge is studied using 1D-3V
particle-in-cell simulations. Calculated electron energy probability functions
exhibit three distinct ``temperatures'' for low-, mid-, and high-energy
electrons. When compared to published experimental data, the calculated
probability functions show a reasonable agreement for the energy range resolved
in the measurements (about 2 eV to 10 eV). Discrepancies outside this range
lead to differences between computational and experimental values of the
electron number density determined from the distribution functions, but the
predicted effective electron temperature is within 25\% of experimental values.
The impedance of the discharge is interpreted in terms of a homogeneous
equivalent circuit model and the driving frequency dependence of the inferred
combined sheath thickness is found to obey a known, theoretically-derived,
power law. The average power transferred from the field to the electrons
(electron heating) is computed, and a region of negative heating near the
sheath edge, particularly at higher driving frequencies, is identified.
Analysis of the electron momentum equation shows that electron inertia, which
would average to zero in a linear regime, is responsible for negative values of
power deposition near the sheath edge at high driving frequencies due to the
highly nonlinear behavior of the discharge.|
|**2023-06-08**|**Line-graph qubit routing: from kagome to heavy-hex and more**|Joris Kattemlle et.al.|[2306.05385v1](http://arxiv.org/abs/2306.05385v1)|null|Quantum computers have the potential to outperform classical computers, but
are currently limited in their capabilities. One such limitation is the
restricted connectivity between qubits, as captured by the hardware's coupling
graph. This limitation poses a challenge for running algorithms that require a
coupling graph different from what the hardware can provide. To overcome this
challenge and fully utilize the hardware, efficient qubit routing strategies
are necessary. In this paper, we introduce line-graph qubit routing, a general
method for routing qubits when the algorithm's coupling graph is a line graph
and the hardware coupling graph is a heavy graph. Line-graph qubit routing is
fast, deterministic, and effective; it requires a classical computational cost
that scales at most quadratically with the number of gates in the original
circuit, while producing a circuit with a SWAP overhead of at most two times
the number of two-qubit gates in the original circuit. We implement line-graph
qubit routing and demonstrate its effectiveness in mapping quantum circuits on
kagome, checkerboard, and shuriken lattices to hardware with heavy-hex,
heavy-square, and heavy-square-octagon coupling graphs, respectively.
Benchmarking shows the ability of line-graph qubit routing to outperform
established general-purpose methods, both in the required classical wall-clock
time and in the quality of the solution that is found. Line-graph qubit routing
has direct applications in the quantum simulation of lattice-based models and
aids the exploration of the capabilities of near-term quantum hardware.|
|**2023-06-08**|**A shape derivative approach to domain simplification**|Jochen Hinz et.al.|[2306.05384v1](http://arxiv.org/abs/2306.05384v1)|null|The objective of this study is to address the difficulty of simplifying the
geometric model in which a differential problem is formulated, also called
defeaturing, while simultaneously ensuring that the accuracy of the solution is
maintained under control. This enables faster and more efficient simulations,
without sacrificing accuracy. More precisely, we consider an isogeometric
discretisation of an elliptic model problem defined on a two-dimensional
hierarchical B-spline computational domain with a complex boundary. Starting
with an oversimplification of the geometry, we build a goal-oriented adaptive
strategy that adaptively reintroduces continuous geometrical features in
regions where the analysis suggests a large impact on the quantity of interest.
This strategy is driven by an a posteriori estimator of the defeaturing error
based on first-order shape sensitivity analysis, and it profits from the local
refinement properties of hierarchical B-splines. The adaptive algorithm is
described together with a procedure to generate (partially) simplified
hierarchical B-spline geometrical domains. Numerical experiments are presented
to illustrate the proposed strategy and its limitations.|
|**2023-06-08**|**Automatic Image Blending Algorithm Based on SAM and DINO**|Haochen Xue et.al.|[2306.05382v1](http://arxiv.org/abs/2306.05382v1)|null|The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.|
|**2023-06-08**|**Optimizing helical disc dynamo**|J. Priede et.al.|[2306.05379v1](http://arxiv.org/abs/2306.05379v1)|null|We present an optimized design of our recently realized helical disc dynamo.
Like the original set-up, the optimized dynamo consists of a flat multi-arm
spiral coil and a co-axially placed disc which is connected to the former by
sliding liquid metal contacts. In contrast to the original set-up, the disc and
the coil in the optimized design have different sizes. This allows the disc to
capture more of the high-density magnetic flux generated in the inner part of
the coil and to avoid the reverse flux in the outer part of the coil. By
optimizing the coil and dics radii, the critical magnetic Reynolds number can
be reduced from ${\mathit Rm}\approx34.6$ when the disc and coil have equal
inner and outer radii with the ratio $r_{i}/r_{o}\approx0.36$ to ${\mathit
Rm}\approx11.6.$ This lowest possible disc dynamo threshold is attained when
the disc and coil have relatively narrow widths. Using a slightly suboptimal
but more practical set-up with the inner and outer radii of the disc and coil
equal to to $(0.3,0.9)$ and $(0.74,1),$ respectively, self-excitation is
expected at ${\mathit Rm}\approx14.6.$|
|**2023-06-08**|**Numerical coupling of aerosol emissions, dry removal, and turbulent mixing in the E3SM Atmosphere Model version 1 (EAMv1), part I: dust budget analyses and the impacts of a revised coupling scheme**|Hui Wan et.al.|[2306.05377v1](http://arxiv.org/abs/2306.05377v1)|null|An earlier study evaluating the dust life cycle in EAMv1 has revealed that
the simulated global mean dust lifetime is substantially shorter when higher
vertical resolution is used, primarily due to significant strengthening of dust
dry removal in source regions. This paper demonstrates that the sequential
splitting of aerosol emissions, dry removal, and turbulent mixing in the
model's time integration loop, especially the calculation of dry removal after
surface emissions and before turbulent mixing, is the primary reason for the
vertical resolution sensitivity reported in that earlier study. Based on this
reasoning, we propose a simple revision to the numerical process coupling
scheme, which moves the application of the surface emissions to after dry
removal and before turbulent mixing. The revised scheme allows newly emitted
particles to be transported aloft by turbulence before being removed from the
atmosphere, and hence better resembles the dust life cycle in the real world.
Sensitivity experiments are conducted and analyzed to evaluate the impact of
the revised coupling on the simulated aerosol climatology in EAMv1.|

### Visual Localization
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**Tracking Everything Everywhere All at Once**|Qianqian Wang et.al.|[2306.05422v1](http://arxiv.org/abs/2306.05422v1)|null|We present a new test-time optimization method for estimating dense and
long-range motion from a video sequence. Prior optical flow or particle video
tracking algorithms typically operate within limited temporal windows,
struggling to track through occlusions and maintain global consistency of
estimated motion trajectories. We propose a complete and globally consistent
motion representation, dubbed OmniMotion, that allows for accurate, full-length
motion estimation of every pixel in a video. OmniMotion represents a video
using a quasi-3D canonical volume and performs pixel-wise tracking via
bijections between local and canonical space. This representation allows us to
ensure global consistency, track through occlusions, and model any combination
of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and
real-world footage show that our approach outperforms prior state-of-the-art
methods by a large margin both quantitatively and qualitatively. See our
project page for more results: http://omnimotion.github.io/|
|**2023-06-08**|**TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture**|M. Esat Kalfaoglu et.al.|[2306.05419v1](http://arxiv.org/abs/2306.05419v1)|null|Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.|
|**2023-06-08**|**LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs**|Zezhou Cheng et.al.|[2306.05410v1](http://arxiv.org/abs/2306.05410v1)|null|A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.|
|**2023-06-08**|**SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding**|Paul-Edouard Sarlin et.al.|[2306.05407v1](http://arxiv.org/abs/2306.05407v1)|null|Semantic 2D maps are commonly used by humans and machines for navigation
purposes, whether it's walking or driving. However, these maps have
limitations: they lack detail, often contain inaccuracies, and are difficult to
create and maintain, especially in an automated fashion. Can we use raw imagery
to automatically create better maps that can be easily interpreted by both
humans and machines? We introduce SNAP, a deep network that learns rich neural
2D maps from ground-level and overhead images. We train our model to align
neural maps estimated from different inputs, supervised only with camera poses
over tens of millions of StreetView images. SNAP can resolve the location of
challenging image queries beyond the reach of traditional methods,
outperforming the state of the art in localization by a large margin. Moreover,
our neural maps encode not only geometry and appearance but also high-level
semantics, discovered without explicit supervision. This enables effective
pre-training for data-efficient semantic scene understanding, with the
potential to unlock cost-efficient creation of more detailed maps.|
|**2023-06-08**|**Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings**|Ernest Pokropek et.al.|[2306.05311v1](http://arxiv.org/abs/2306.05311v1)|null|In this work, we present a pipeline to reconstruct the 3D pose of a horse
from 4 simultaneous surveillance camera recordings. Our environment poses
interesting challenges to tackle, such as limited field view of the cameras and
a relatively closed and small environment. The pipeline consists of training a
2D markerless pose estimation model to work on every viewpoint, then applying
it to the videos and performing triangulation. We present numerical evaluation
of the results (error analysis), as well as show the utility of the achieved
poses in downstream tasks of selected behavioral predictions. Our analysis of
the predictive model for equine behavior showed a bias towards pain-induced
horses, which aligns with our understanding of how behavior varies across
painful and healthy subjects.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in
predictable positions in limited environments. Thus, when the location of the
target object deviates severely from the expected location, a robot sometimes
responds in an unexpected way, especially when it operates with a human. For
safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a
robot hand camera that recognizes an object's absence during manipulation. The
robot decides whether to proceed by examining the tracker's bounding box output
containing the target object. We adopt an out-of-distribution classifier for
more accurate object recognition since trackers can mistrack a background as a
target object. To the best of our knowledge, our method is the first approach
of applying an out-of-distribution classification technique to a tracker
output. We evaluate our method on the first-person video benchmark dataset,
TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e
robot. Then we test our tracker on the UR5e robot in real-time with a
conveyor-belt sushi task, to examine the tracker's ability to track target
dishes and to determine the exit status. Our tracker shows 38% higher
exit-aware performance than a baseline method. The dataset and the code will be
released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose**|Wiktor Mucha et.al.|[2306.05147v1](http://arxiv.org/abs/2306.05147v1)|null|Egocentric action recognition is essential for healthcare and assistive
technology that relies on egocentric cameras because it allows for the
automatic and continuous monitoring of activities of daily living (ADLs)
without requiring any conscious effort from the user. This study explores the
feasibility of using 2D hand and object pose information for egocentric action
recognition. While current literature focuses on 3D hand pose information, our
work shows that using 2D skeleton data is a promising approach for hand-based
action classification, might offer privacy enhancement, and could be less
computationally demanding. The study uses a state-of-the-art transformer-based
method to classify sequences and achieves validation results of 94%,
outperforming other existing solutions. The accuracy of the test subset drops
to 76%, indicating the need for further generalization improvement. This
research highlights the potential of 2D hand and object pose information for
action recognition tasks and offers a promising alternative to 3D-based
methods.|
|**2023-06-08**|**Variable Radiance Field for Real-Life Category-Specifc Reconstruction from Single Image**|Kun Wang et.al.|[2306.05145v1](http://arxiv.org/abs/2306.05145v1)|null|Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.|
|**2023-06-08**|**Neuromorphic Sampling of Signals in Shift-Invariant Spaces**|Abijith Jagannath Kamath et.al.|[2306.05103v1](http://arxiv.org/abs/2306.05103v1)|null|Neuromorphic sampling is a paradigm shift in analog-to-digital conversion
where the acquisition strategy is opportunistic and measurements are recorded
only when there is a significant change in the signal. Neuromorphic sampling
has given rise to a new class of event-based sensors called dynamic vision
sensors or neuromorphic cameras. The neuromorphic sampling mechanism utilizes
low power and provides high-dynamic range sensing with low latency and high
temporal resolution. The measurements are sparse and have low redundancy making
it convenient for downstream tasks. In this paper, we present a
sampling-theoretic perspective to neuromorphic sensing of continuous-time
signals. We establish a close connection between neuromorphic sampling and
time-based sampling - where signals are encoded temporally. We analyse
neuromorphic sampling of signals in shift-invariant spaces, in particular,
bandlimited signals and polynomial splines. We present an iterative technique
for perfect reconstruction subject to the events satisfying a density
criterion. We also provide necessary and sufficient conditions for perfect
reconstruction. Owing to practical limitations in meeting the sufficient
conditions for perfect reconstruction, we extend the analysis to approximate
reconstruction from sparse events. In the latter setting, we pose signal
reconstruction as a continuous-domain linear inverse problem whose solution can
be obtained by solving an equivalent finite-dimensional convex optimization
program using a variable-splitting approach. We demonstrate the performance of
the proposed algorithm and validate our claims via experiments on synthetic
signals.|
|**2023-06-08**|**Real-Time Rendering of Glinty Appearances using Distributed Binomial Laws on Anisotropic Grids**|Deliot et.al.|[2306.05051v1](http://arxiv.org/abs/2306.05051v1)|null|In this work, we render in real-time glittery materials caused by discrete
flakes on the surface. To achieve this, one has to count the number of flakes
reflecting the light towards the camera within every texel covered by a given
pixel footprint. To do so, we derive a counting method for arbitrary footprints
that, unlike previous work, outputs the correct statistics. We combine this
counting method with an anisotropic parameterization of the texture space that
reduces the number of texels falling under a pixel footprint. This allows our
method to run with both stable performance and 1.5X to 5X faster than the
state-of-the-art.|
|**2023-06-08**|**StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views**|Jianfei Guo et.al.|[2306.04988v1](http://arxiv.org/abs/2306.04988v1)|null|We present a novel multi-view implicit surface reconstruction technique,
termed StreetSurf, that is readily applicable to street view images in
widely-used autonomous driving datasets, such as Waymo-perception sequences,
without necessarily requiring LiDAR data. As neural rendering research expands
rapidly, its integration into street views has started to draw interests.
Existing approaches on street views either mainly focus on novel view synthesis
with little exploration of the scene geometry, or rely heavily on dense LiDAR
data when investigating reconstruction. Neither of them investigates multi-view
implicit surface reconstruction, especially under settings without LiDAR data.
Our method extends prior object-centric neural surface reconstruction
techniques to address the unique challenges posed by the unbounded street views
that are captured with non-object-centric, long and narrow camera trajectories.
We delimit the unbounded space into three parts, close-range, distant-view and
sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids
along with road-surface initialization scheme for finer and disentangled
representation. To further address the geometric errors arising from
textureless regions and insufficient viewing angles, we adopt geometric priors
that are estimated using general purpose monocular models. Coupled with our
implementation of efficient and fine-grained multi-stage ray marching strategy,
we achieve state of the art reconstruction quality in both geometry and
appearance within only one to two hours of training time with a single RTX3090
GPU for each street view sequence. Furthermore, we demonstrate that the
reconstructed implicit surfaces have rich potential for various downstream
tasks, including ray tracing and LiDAR simulation.|
|**2023-06-08**|**Ultraviolet Photodetectors based on GaN and AlGaN/AlN Nanowire Ensembles: Effects of Planarization with Hydrogen Silsesquioxane and Nanowire Architecture**|E. Akar et.al.|[2306.04986v1](http://arxiv.org/abs/2306.04986v1)|null|The interest in nanowire photodetectors stems from their potential to improve
the performance of a variety of devices, including solar cells, cameras,
sensors, and communication systems. Implementing devices based on nanowire
ensembles requires a planarization process which must be conceived to preserve
the advantages of the nanowire geometry. This is particularly challenging in
the ultraviolet (UV) range, where spin coating with hydrogen silsesquioxane
(HSQ) appears as an interesting approach in terms of transmittance and
refractive index. Here, we report a comprehensive study on UV photodetectors
based on GaN or AlGaN/AlN nanowire ensembles encapsulated in HSQ. We show that
this material is efficient for passivating the nanowire surface, it introduces
a compressive strain in the nanowires and preserves their radiative efficiency.
We discuss the final performance of planarized UV photodetectors based on three
kinds of nanowire ensembles: (i) non-intentionally-doped (nid) GaN nanowires,
(ii) Ge-doped GaN nanowires, and (iii) nid GaN nanowires terminated with an
AlGaN/AlN superlattice. The incorporation of the superlattice allows tuning the
spectral response with bias, which can enhance the carrier collection from the
AlGaN/AlN superlattice or from the GaN stem. In all the cases, the performance
of the planarized devices remains determined by the nanowire nature, since
their characteristics in terms of linearity and spectral selectivity are closer
to those demonstrated in single nanowires than those of planar devices. Thus,
the visible rejection is several orders of magnitude and there is no indication
of persistent photocurrent, which makes all the samples suitable for
UV-selective photodetection applications.|
|**2023-06-08**|**Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization**|Jungwuk Park et.al.|[2306.04911v1](http://arxiv.org/abs/2306.04911v1)|null|In domain generalization (DG), the target domain is unknown when the model is
being trained, and the trained model should successfully work on an arbitrary
(and possibly unseen) target domain during inference. This is a difficult
problem, and despite active studies in recent years, it remains a great
challenge. In this paper, we take a simple yet effective approach to tackle
this issue. We propose test-time style shifting, which shifts the style of the
test sample (that has a large style gap with the source domains) to the nearest
source domain that the model is already familiar with, before making the
prediction. This strategy enables the model to handle any target domains with
arbitrary style statistics, without additional model update at test-time.
Additionally, we propose style balancing, which provides a great platform for
maximizing the advantage of test-time style shifting by handling the
DG-specific imbalance issues. The proposed ideas are easy to implement and
successfully work in conjunction with various other DG schemes. Experimental
results on different datasets show the effectiveness of our methods.|
|**2023-06-08**|**ViG-UNet: Vision Graph Neural Networks for Medical Image Segmentation**|Juntao Jiang et.al.|[2306.04905v1](http://arxiv.org/abs/2306.04905v1)|null|Deep neural networks have been widely used in medical image analysis and
medical image segmentation is one of the most important tasks. U-shaped neural
networks with encoder-decoder are prevailing and have succeeded greatly in
various segmentation tasks. While CNNs treat an image as a grid of pixels in
Euclidean space and Transformers recognize an image as a sequence of patches,
graph-based representation is more generalized and can construct connections
for each part of an image. In this paper, we propose a novel ViG-UNet, a graph
neural network-based U-shaped architecture with the encoder, the decoder, the
bottleneck, and skip connections. The downsampling and upsampling modules are
also carefully designed. The experimental results on ISIC 2016, ISIC 2017 and
Kvasir-SEG datasets demonstrate that our proposed architecture outperforms most
existing classic and state-of-the-art U-shaped networks.|
|**2023-06-08**|**ExtPerFC: An Efficient 2D and 3D Perception Hardware-Software Framework for Mobile Cobot**|Tuan Dang et.al.|[2306.04853v1](http://arxiv.org/abs/2306.04853v1)|null|As the reliability of the robot's perception correlates with the number of
integrated sensing modalities to tackle uncertainty, a practical solution to
manage these sensors from different computers, operate them simultaneously, and
maintain their real-time performance on the existing robotic system with
minimal effort is needed. In this work, we present an end-to-end
software-hardware framework, namely ExtPerFC, that supports both conventional
hardware and software components and integrates machine learning object
detectors without requiring an additional dedicated graphic processor unit
(GPU). We first design our framework to achieve real-time performance on the
existing robotic system, guarantee configuration optimization, and concentrate
on code reusability. We then mathematically model and utilize our transfer
learning strategies for 2D object detection and fuse them into depth images for
3D depth estimation. Lastly, we systematically test the proposed framework on
the Baxter robot with two 7-DOF arms, a four-wheel mobility base, and an Intel
RealSense D435i RGB-D camera. The results show that the robot achieves
real-time performance while executing other tasks (e.g., map building,
localization, navigation, object detection, arm moving, and grasping)
simultaneously with available hardware like Intel onboard CPUS/GPUs on
distributed computers. Also, to comprehensively control, program, and monitor
the robot system, we design and introduce an end-user application. The source
code is available at https://github.com/tuantdang/perception_framework.|
|**2023-06-07**|**BU-CVKit: Extendable Computer Vision Framework for Species Independent Tracking and Analysis**|Mahir Patel et.al.|[2306.04736v1](http://arxiv.org/abs/2306.04736v1)|null|A major bottleneck of interdisciplinary computer vision (CV) research is the
lack of a framework that eases the reuse and abstraction of state-of-the-art CV
models by CV and non-CV researchers alike. We present here BU-CVKit, a computer
vision framework that allows the creation of research pipelines with chainable
Processors. The community can create plugins of their work for the framework,
hence improving the re-usability, accessibility, and exposure of their work
with minimal overhead. Furthermore, we provide MuSeqPose Kit, a user interface
for the pose estimation package of BU-CVKit, which automatically scans for
installed plugins and programmatically generates an interface for them based on
the metadata provided by the user. It also provides software support for
standard pose estimation features such as annotations, 3D reconstruction,
reprojection, and camera calibration. Finally, we show examples of behavioral
neuroscience pipelines created through the sample plugins created for our
framework.|
|**2023-06-07**|**ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections**|Chun-Han Yao et.al.|[2306.04619v1](http://arxiv.org/abs/2306.04619v1)|null|Estimating 3D articulated shapes like animal bodies from monocular images is
inherently challenging due to the ambiguities of camera viewpoint, pose,
texture, lighting, etc. We propose ARTIC3D, a self-supervised framework to
reconstruct per-instance 3D shapes from a sparse image collection in-the-wild.
Specifically, ARTIC3D is built upon a skeleton-based surface representation and
is further guided by 2D diffusion priors from Stable Diffusion. First, we
enhance the input images with occlusions/truncation via 2D diffusion to obtain
cleaner mask estimates and semantic features. Second, we perform
diffusion-guided 3D optimization to estimate shape and texture that are of
high-fidelity and faithful to input images. We also propose a novel technique
to calculate more stable image-level gradients via diffusion models compared to
existing alternatives. Finally, we produce realistic animations by fine-tuning
the rendered shape and texture under rigid part transformations. Extensive
evaluations on multiple existing datasets as well as newly introduced noisy web
image collections with occlusions and truncation demonstrate that ARTIC3D
outputs are more robust to noisy images, higher quality in terms of shape and
texture details, and more realistic when animated. Project page:
https://chhankyao.github.io/artic3d/|
|**2023-06-07**|**Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt**|Kai Chen et.al.|[2306.04607v2](http://arxiv.org/abs/2306.04607v2)|null|Diffusion models have attracted significant attention due to their remarkable
ability to create content and generate data for tasks such as image
classification. However, the usage of diffusion models to generate high-quality
object detection data remains an underexplored area, where not only the
image-level perceptual quality but also geometric conditions such as bounding
boxes and camera views are essential. Previous studies have utilized either
copy-paste synthesis or layout-to-image (L2I) generation with specifically
designed modules to encode semantic layouts. In this paper, we propose
GeoDiffusion, a simple framework that can flexibly translate various geometric
conditions into text prompts and empower the pre-trained text-to-image (T2I)
diffusion models for high-quality detection data generation. Unlike previous
L2I methods, our GeoDiffusion is able to encode not only bounding boxes but
also extra geometric conditions such as camera views in self-driving scenes.
Extensive experiments demonstrate GeoDiffusion outperforms previous L2I methods
while maintaining 4x training time faster. To the best of our knowledge, this
is the first work to adopt diffusion models for layout-to-image generation with
geometric conditions and demonstrate that L2I-generated images can be
beneficial for improving the performance of object detectors.|
|**2023-06-07**|**The CYGNO experiment, a directional detector for direct Dark Matter searches**|F. D. Amaro et.al.|[2306.04568v1](http://arxiv.org/abs/2306.04568v1)|null|The CYGNO project aims at the development of a high precision optical readout
gaseous Tima Projection Chamber (TPC) for directional dark matter (DM)
searches, to be hosted at Laboratori Nazionali del Gran Sasso (LNGS). CYGNO
employs a He:CF$_4$ gas mixture at atmospheric pressure with a Gas Electron
Multiplier (GEM) based amplification structure coupled to an optical readout
comprised of sCMOS cameras and photomultiplier tubes (PMTs). This experimental
setup allows to achieve 3D tracking and background rejection down to O(1) keV
energy, to boost sensitivity to low WIMP masses. The characteristics of the
optical readout approach in terms of the light yield will be illustrated along
with the particle identification properties. The project timeline foresees, in
the next 2-3 years, the realisation and installation of a 0.4 m$^3$ TPC in the
underground laboratories at LNGS to act as a demonstrator. Finally, the studies
of the expected DM sensitivities of the CYGNO demonstrator will be presented.|
|**2023-06-07**|**Integrated Photonic Encoder for Terapixel Image Processing**|Xiao Wang et.al.|[2306.04554v1](http://arxiv.org/abs/2306.04554v1)|null|Modern lens designs are capable of resolving >10 gigapixels, while advances
in camera frame-rate and hyperspectral imaging have made Terapixel/s data
acquisition a real possibility. The main bottlenecks preventing such high
data-rate systems are power consumption and data storage. In this work, we show
that analog photonic encoders could address this challenge, enabling high-speed
image compression using orders-of-magnitude lower power than digital
electronics. Our approach relies on a silicon-photonics front-end to compress
raw image data, foregoing energy-intensive image conditioning and reducing data
storage requirements. The compression scheme uses a passive disordered photonic
structure to perform kernel-type random projections of the raw image data with
minimal power consumption and low latency. A back-end neural network can then
reconstruct the original images with structural similarity exceeding 90%. This
scheme has the potential to process Terapixel/s data streams using less than
100 fJ/pixel, providing a path to ultra-high-resolution data and image
acquisition systems.|
|**2023-06-07**|**Revising deep learning methods in parking lot occupancy detection**|Anastasia Martynova et.al.|[2306.04288v2](http://arxiv.org/abs/2306.04288v2)|[link](https://github.com/eighonet/parking-research)|Parking guidance systems have recently become a popular trend as a part of
the smart cities' paradigm of development. The crucial part of such systems is
the algorithm allowing drivers to search for available parking lots across
regions of interest. The classic approach to this task is based on the
application of neural network classifiers to camera records. However, existing
systems demonstrate a lack of generalization ability and appropriate testing
regarding specific visual conditions. In this study, we extensively evaluate
state-of-the-art parking lot occupancy detection algorithms, compare their
prediction quality with the recently emerged vision transformers, and propose a
new pipeline based on EfficientNet architecture. Performed computational
experiments have demonstrated the performance increase in the case of our
model, which was evaluated on 5 different datasets.|
|**2023-06-07**|**Learning Probabilistic Coordinate Fields for Robust Correspondences**|Weiyue Zhao et.al.|[2306.04231v1](http://arxiv.org/abs/2306.04231v1)|null|We introduce Probabilistic Coordinate Fields (PCFs), a novel
geometric-invariant coordinate representation for image correspondence
problems. In contrast to standard Cartesian coordinates, PCFs encode
coordinates in correspondence-specific barycentric coordinate systems (BCS)
with affine invariance. To know \textit{when and where to trust} the encoded
coordinates, we implement PCFs in a probabilistic network termed PCF-Net, which
parameterizes the distribution of coordinate fields as Gaussian mixture models.
By jointly optimizing coordinate fields and their confidence conditioned on
dense flows, PCF-Net can work with various feature descriptors when quantifying
the reliability of PCFs by confidence maps. An interesting observation of this
work is that the learned confidence map converges to geometrically coherent and
semantically consistent regions, which facilitates robust coordinate
representation. By delivering the confident coordinates to keypoint/feature
descriptors, we show that PCF-Net can be used as a plug-in to existing
correspondence-dependent approaches. Extensive experiments on both indoor and
outdoor datasets suggest that accurate geometric invariant coordinates help to
achieve the state of the art in several correspondence problems, such as sparse
feature matching, dense image registration, camera pose estimation, and
consistency filtering. Further, the interpretable confidence map predicted by
PCF-Net can also be leveraged to other novel applications from texture transfer
to multi-homography classification.|
|**2023-06-07**|**StructuredMesh: 3D Structured Optimization of Faade Components on Photogrammetric Mesh Models using Binary Integer Programming**|Libin Wang et.al.|[2306.04184v1](http://arxiv.org/abs/2306.04184v1)|null|The lack of fa\c{c}ade structures in photogrammetric mesh models renders them
inadequate for meeting the demands of intricate applications. Moreover, these
mesh models exhibit irregular surfaces with considerable geometric noise and
texture quality imperfections, making the restoration of structures
challenging. To address these shortcomings, we present StructuredMesh, a novel
approach for reconstructing fa\c{c}ade structures conforming to the regularity
of buildings within photogrammetric mesh models. Our method involves capturing
multi-view color and depth images of the building model using a virtual camera
and employing a deep learning object detection pipeline to semi-automatically
extract the bounding boxes of fa\c{c}ade components such as windows, doors, and
balconies from the color image. We then utilize the depth image to remap these
boxes into 3D space, generating an initial fa\c{c}ade layout. Leveraging
architectural knowledge, we apply binary integer programming (BIP) to optimize
the 3D layout's structure, encompassing the positions, orientations, and sizes
of all components. The refined layout subsequently informs fa\c{c}ade modeling
through instance replacement. We conducted experiments utilizing building mesh
models from three distinct datasets, demonstrating the adaptability,
robustness, and noise resistance of our proposed methodology. Furthermore, our
3D layout evaluation metrics reveal that the optimized layout enhances
precision, recall, and F-score by 6.5%, 4.5%, and 5.5%, respectively, in
comparison to the initial layout.|
|**2023-06-07**|**When to Read Documents or QA History: On Unified and Selective Open-domain QA**|Kyungjae Lee et.al.|[2306.04176v1](http://arxiv.org/abs/2306.04176v1)|null|This paper studies the problem of open-domain question answering, with the
aim of answering a diverse range of questions leveraging knowledge resources.
Two types of sources, QA-pair and document corpora, have been actively
leveraged with the following complementary strength. The former is highly
precise when the paraphrase of given question $q$ was seen and answered during
training, often posed as a retrieval problem, while the latter generalizes
better for unseen questions. A natural follow-up is thus leveraging both
models, while a naive pipelining or integration approaches have failed to bring
additional gains over either model alone. Our distinction is interpreting the
problem as calibration, which estimates the confidence of predicted answers as
an indicator to decide when to use a document or QA-pair corpus. The
effectiveness of our method was validated on widely adopted benchmarks such as
Natural Questions and TriviaQA.|
|**2023-06-07**|**BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives**|Sainan Liu et.al.|[2306.04166v1](http://arxiv.org/abs/2306.04166v1)|null|Implicit neural representation has emerged as a powerful method for
reconstructing 3D scenes from 2D images. Given a set of camera poses and
associated images, the models can be trained to synthesize novel, unseen views.
In order to expand the use cases for implicit neural representations, we need
to incorporate camera pose estimation capabilities as part of the
representation learning, as this is necessary for reconstructing scenes from
real-world video sequences where cameras are generally not being tracked.
Existing approaches like COLMAP and, most recently, bundle-adjusting neural
radiance field methods often suffer from lengthy processing times. These delays
ranging from hours to days, arise from laborious feature matching, hardware
limitations, dense point sampling, and long training times required by a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP). Our approach leverages accelerated
sampling and hash encoding to expedite both pose refinement/estimation and 3D
scene reconstruction. Experimental results demonstrate that our method achieves
a more than 10 to 20 $\times$ speed improvement in novel view synthesis
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation.|
|**2023-06-07**|**Effect of viscosity on the dynamics of a non-equilibrium bubble in free-field and near a free-surface**|Y. S. Kannan et.al.|[2306.04129v1](http://arxiv.org/abs/2306.04129v1)|null|The effect of viscosity on the behaviour of a non-equilibrium bubble is
investigated experimentally, in two scenarios; firstly, when the bubble is
generated in the bulk of the fluid (termed as ``free-field'' bubble) and
secondly when the bubble is generated near a free-surface (termed as
``free-surface'' bubble). The bubble is created using a low-voltage spark
circuit and its dynamics is captured using a high-speed camera with back-lit
illumination. The viscosity of the surrounding fluid is varied by using
different grades of silicone oil. For a ``free-field'' bubble, the bubble
oscillates radially and as the viscosity of the liquid increases, the number of
oscillations, as well as the time-period of each oscillation, are increased. At
high viscosities, the bubble also becomes stable and does not disintegrate into
smaller bubbles. For ``free-surface'' bubbles, two parameters, namely, the
initial distance of the bubble from the free-surface and the viscosity of the
surrounding fluid are varied. It is observed that beyond a certain initial
distance of the bubble from the free-surface, the bubble behaves as a
``free-field'' bubble with negligible influence of the free-surface on its
dynamics. This limiting initial distance decreases as the liquid viscosity is
increased and is not dependent on the bubble radius. For these bubbles,
different behaviours of the free-surface in each liquid are also presented as a
function of the two parameters.|
|**2023-06-07**|**Retrosynthesis Prediction with Local Template Retrieval**|Shufang Xie et.al.|[2306.04123v1](http://arxiv.org/abs/2306.04123v1)|null|Retrosynthesis, which predicts the reactants of a given target molecule, is
an essential task for drug discovery. In recent years, the machine learing
based retrosynthesis methods have achieved promising results. In this work, we
introduce RetroKNN, a local reaction template retrieval method to further boost
the performance of template-based systems with non-parametric retrieval. We
first build an atom-template store and a bond-template store that contain the
local templates in the training data, then retrieve from these templates with a
k-nearest-neighbor (KNN) search during inference. The retrieved templates are
combined with neural network predictions as the final output. Furthermore, we
propose a lightweight adapter to adjust the weights when combing neural network
and KNN predictions conditioned on the hidden representation and the retrieved
templates. We conduct comprehensive experiments on two widely used benchmarks,
the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy, we improved
7.1% on the USPTO-50K dataset and 12.0% on the USPTO-MIT dataset. These results
demonstrate the effectiveness of our method.|
|**2023-06-06**|**BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens Metadata Embedding**|Zhihao Yang et.al.|[2306.04032v1](http://arxiv.org/abs/2306.04032v1)|[link](https://github.com/indicator0/bokehornot)|Bokeh effect is an optical phenomenon that offers a pleasant visual
experience, typically generated by high-end cameras with wide aperture lenses.
The task of bokeh effect transformation aims to produce a desired effect in one
set of lenses and apertures based on another combination. Current models are
limited in their ability to render a specific set of bokeh effects, primarily
transformations from sharp to blur. In this paper, we propose a novel universal
method for embedding lens metadata into the model and introducing a loss
calculation method using alpha masks from the newly released Bokeh Effect
Transformation Dataset(BETD) [3]. Based on the above techniques, we propose the
BokehOrNot model, which is capable of producing both blur-to-sharp and
sharp-to-blur bokeh effect with various combinations of lenses and aperture
sizes. Our proposed model outperforms current leading bokeh rendering and image
restoration models and renders visually natural bokeh effects. Our code is
available at: https://github.com/indicator0/bokehornot.|
|**2023-06-06**|**Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification**|Christopher Neff et.al.|[2306.03993v1](http://arxiv.org/abs/2306.03993v1)|null|Following the popularity of Unsupervised Domain Adaptation (UDA) in person
re-identification, the recently proposed setting of Online Unsupervised Domain
Adaptation (OUDA) attempts to bridge the gap towards practical applications by
introducing a consideration of streaming data. However, this still falls short
of truly representing real-world applications. This paper defines the setting
of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for
Person Re-identification. The R$^2$OUDA setting sets the stage for true
real-world real-time OUDA, bringing to light four major limitations found in
real-world applications that are often neglected in current research: system
generated person images, subset distribution selection, time-based data stream
segmentation, and a segment-based time constraint. To address all aspects of
this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time
Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera system
for real-world person re-identification. Taking a popular person
re-identification dataset, R$^2$MMT was used to construct over 100 data subsets
and train more than 3000 models, exploring the breadth of the R$^2$OUDA setting
to understand the training time and accuracy trade-offs and limitations for
real-world applications. R$^2$MMT, a real-world system able to respect the
strict constraints of the proposed R$^2$OUDA setting, achieves accuracies
within 0.1% of comparable OUDA methods that cannot be applied directly to
real-world applications.|
|**2023-06-06**|**X-Align++: cross-modal cross-view alignment for Bird's-eye-view segmentation**|Shubhankar Borse et.al.|[2306.03810v1](http://arxiv.org/abs/2306.03810v1)|null|Bird's-eye-view (BEV) grid is a typical representation of the perception of
road components, e.g., drivable area, in autonomous driving. Most existing
approaches rely on cameras only to perform segmentation in BEV space, which is
fundamentally constrained by the absence of reliable depth information. The
latest works leverage both camera and LiDAR modalities but suboptimally fuse
their features using simple, concatenation-based mechanisms. In this paper, we
address these problems by enhancing the alignment of the unimodal features in
order to aid feature fusion, as well as enhancing the alignment between the
cameras' perspective view (PV) and BEV representations. We propose X-Align, a
novel end-to-end cross-modal and cross-view learning framework for BEV
segmentation consisting of the following components: (i) a novel Cross-Modal
Feature Alignment (X-FA) loss, (ii) an attention-based Cross-Modal Feature
Fusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an
auxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA)
losses to improve the PV-to-BEV transformation. We evaluate our proposed method
across two commonly used benchmark datasets, i.e., nuScenes and KITTI-360.
Notably, X-Align significantly outperforms the state-of-the-art by 3 absolute
mIoU points on nuScenes. We also provide extensive ablation studies to
demonstrate the effectiveness of the individual components.|
